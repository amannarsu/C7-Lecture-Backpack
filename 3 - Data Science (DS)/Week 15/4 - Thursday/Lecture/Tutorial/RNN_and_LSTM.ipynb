{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Foo8m7vM0GcA"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.datasets import PennTreebank\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","import math"]},{"cell_type":"markdown","metadata":{"id":"Kgb46NUD0GcB"},"source":["# Introduction to Sequence Modeling\n","\n","Sequence modeling is a crucial task in various domains, such as natural language processing, speech recognition, and time series analysis. It involves predicting or generating sequences of data based on historical information. In this notebook, we will explore Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, which are powerful architectures for handling sequential data.\n","\n","## Importance of Sequence Modeling\n","\n","Sequence modeling enables us to:\n","- Predict the next element in a sequence (e.g., predicting the next word in a sentence)\n","- Generate new sequences (e.g., generating text or music)\n","- Classify sequences (e.g., sentiment analysis of text)\n","- Translate sequences (e.g., machine translation between languages)\n","\n","Understanding and applying sequence modeling techniques is essential for building intelligent systems that can process and generate sequential data effectively.\n","\n","## Recurrent Neural Networks (RNNs)\n","\n","RNNs are a class of neural networks designed to handle sequential data. They maintain a hidden state that captures information from previous time steps, allowing them to capture dependencies and patterns in sequences.\n","\n","### RNN Architecture\n","\n","The basic architecture of an RNN consists of:\n","- Input layer: Receives the input at each time step\n","- Hidden layer: Maintains a hidden state that captures information from previous time steps\n","- Output layer: Produces the output at each time step\n","\n","The hidden state is updated at each time step based on the current input and the previous hidden state. This allows RNNs to maintain a memory of past information and use it to make predictions or generate outputs.\n","\n","### Challenges with RNNs\n","\n","Despite their ability to handle sequential data, RNNs suffer from the vanishing and exploding gradient problems. These problems arise when training RNNs on long sequences, as the gradients can become extremely small (vanishing) or large (exploding) during backpropagation. This makes it difficult for RNNs to capture long-term dependencies effectively.\n","\n","## Long Short-Term Memory (LSTM) Networks\n","\n","LSTMs are a type of RNN architecture designed to address the limitations of traditional RNNs. They introduce a memory cell and gating mechanisms to regulate the flow of information, enabling them to capture long-term dependencies more effectively.\n","\n","### LSTM Architecture\n","\n","The key components of an LSTM cell are:\n","- Input gate: Controls the flow of new information into the memory cell\n","- Forget gate: Determines what information to discard from the memory cell\n","- Output gate: Controls the output of the memory cell\n","- Memory cell: Stores the long-term information\n","\n","The gating mechanisms allow LSTMs to selectively update, forget, and output information, enabling them to capture complex patterns and dependencies in sequences.\n","\n","### Advantages of LSTMs\n","\n","LSTMs have several advantages over traditional RNNs:\n","- Ability to capture long-term dependencies\n","- Mitigation of the vanishing and exploding gradient problems\n","- Improved performance on tasks requiring long-range context\n","\n","LSTMs have been widely adopted and have shown remarkable success in various sequence modeling tasks, such as language modeling, sentiment analysis, and speech recognition.\n","\n","Now, let's dive into the implementation of RNNs and LSTMs using PyTorch and explore their application to a real-world dataset."]},{"cell_type":"markdown","metadata":{"id":"IDDFt9Ar0GcC"},"source":["# Preparing the Penn Treebank Dataset\n","\n","To demonstrate the application of RNNs and LSTMs, we will use the Penn Treebank dataset, which is a widely used dataset for language modeling tasks. The dataset consists of text from Wall Street Journal articles and is commonly used to evaluate the performance of language models.\n","\n","We will preprocess the dataset by tokenizing the text and building a vocabulary from the training data. This will allow us to convert the text into numerical representations that can be fed into our models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmoOJTV20GcD"},"outputs":[],"source":["# Tokenization\n","tokenizer = get_tokenizer('basic_english')\n","\n","# Load the Penn Treebank dataset\n","train_iter = PennTreebank(split='train')\n","\n","# Build the vocabulary\n","vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GU3VDU_r0GcD"},"outputs":[],"source":["# Preprocess the data\n","def data_process(raw_text_iter):\n","    \"\"\"\n","    Preprocesses the raw text data by tokenizing and converting to numerical representations.\n","    \"\"\"\n","    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n","    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwiH5-oZ0GcD"},"outputs":[],"source":["# Load and preprocess the train, validation, and test sets\n","train_iter, val_iter, test_iter = PennTreebank()\n","train_data = data_process(train_iter)\n","val_data = data_process(val_iter)\n","test_data = data_process(test_iter)"]},{"cell_type":"markdown","metadata":{"id":"pP9sSXq40GcD"},"source":["## Implementing the Language Model\n","\n","Now that we have preprocessed the dataset, let's implement a language model using an LSTM network. The language model will learn to predict the next word in a sequence based on the previous words.\n","\n","The model architecture consists of:\n","- Embedding layer: Converts the input words into dense vector representations\n","- LSTM layer: Processes the sequence of word embeddings and captures the long-term dependencies\n","- Linear layer: Transforms the LSTM outputs into probability distributions over the vocabulary\n","\n","We will define the model using PyTorch's `nn.Module` class and specify the forward pass of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfAxkg2s0GcD"},"outputs":[],"source":["class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(LanguageModel, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x, h):\n","        \"\"\"\n","        Forward pass of the language model.\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, sequence_length)\n","            h: Hidden state tensor of shape (num_layers, batch_size, hidden_size)\n","\n","        Returns:\n","            out: Output tensor of shape (batch_size * sequence_length, vocab_size)\n","            (h, c): Tuple of hidden state and cell state tensors of shape (num_layers, batch_size, hidden_size)\n","        \"\"\"\n","        x = self.embed(x)\n","        out, (h, c) = self.lstm(x, h)\n","        out = out.reshape(out.size(0) * out.size(1), out.size(2))\n","        out = self.linear(out)\n","        return out, (h, c)"]},{"cell_type":"markdown","metadata":{"id":"RO67lF770GcE"},"source":["## Training the Language Model\n","\n","With the language model implemented, let's define the training loop. We will use the cross-entropy loss as the criterion and the Adam optimizer to update the model parameters.\n","\n","The training loop involves:\n","1. Initializing the hidden and cell states\n","2. Iterating over the training data in batches\n","3. Performing forward pass to get the model outputs\n","4. Computing the loss between the predicted outputs and the target words\n","5. Backpropagating the gradients and updating the model parameters\n","6. Printing the loss for each epoch to monitor the training progress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se-dbRsU0GcE"},"outputs":[],"source":["def train_model(model, data, epochs, batch_size, seq_length, lr, clip):\n","    \"\"\"\n","    Trains the language model on the given data.\n","\n","    Args:\n","        model: Language model to be trained\n","        data: Training data tensor\n","        epochs: Number of training epochs\n","        batch_size: Batch size for training\n","        seq_length: Sequence length for truncated backpropagation through time\n","        lr: Learning rate for the optimizer\n","        clip: Gradient clipping threshold\n","    \"\"\"\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        states = (torch.zeros(num_layers, batch_size, hidden_size),\n","                  torch.zeros(num_layers, batch_size, hidden_size))\n","\n","        for i in range(0, data.size(0) - seq_length, seq_length * batch_size):\n","            batch_size_i = min(batch_size, (data.size(0) - i) // seq_length)\n","            inputs = data[i:i+seq_length * batch_size_i].view(batch_size_i, seq_length)\n","            targets = data[i+1:i+seq_length * batch_size_i+1].view(batch_size_i, seq_length)\n","\n","            states = detach(states)\n","            states = (states[0][:, :batch_size_i, :], states[1][:, :batch_size_i, :])\n","            outputs, states = model(inputs, states)\n","            loss = criterion(outputs, targets.reshape(-1))\n","\n","            model.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","        print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n","\n","def detach(states):\n","    return [state.detach() for state in states]"]},{"cell_type":"markdown","metadata":{"id":"Ucn4umuB0GcE"},"source":["## Evaluating the Language Model\n","\n","After training the language model, it's important to evaluate its performance on unseen data. We will evaluate the model on the training, validation, and test sets using the cross-entropy loss metric.\n","\n","The evaluation process involves:\n","1. Setting the model to evaluation mode\n","2. Initializing the hidden and cell states\n","3. Iterating over the evaluation data in batches\n","4. Performing forward pass to get the model outputs\n","5. Computing the loss between the predicted outputs and the target words\n","6. Accumulating the total loss\n","7. Returning the average loss over the evaluation data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv0SICaa0GcE"},"outputs":[],"source":["def evaluate_model(model, data, batch_size, seq_length):\n","    \"\"\"\n","    Evaluates the language model on the given data.\n","\n","    Args:\n","        model: Language model to be evaluated\n","        data: Evaluation data tensor\n","        batch_size: Batch size for evaluation\n","        seq_length: Sequence length for truncated backpropagation through time\n","\n","    Returns:\n","        Average loss over the evaluation data\n","        Perplexity: Measure of how well the model predicts the target words\n","    \"\"\"\n","    with torch.no_grad():\n","        states = (torch.zeros(num_layers, batch_size, hidden_size),\n","                  torch.zeros(num_layers, batch_size, hidden_size))\n","        total_loss = 0\n","        criterion = nn.CrossEntropyLoss()\n","\n","        for i in range(0, data.size(0) - seq_length, seq_length * batch_size):\n","            batch_size_i = min(batch_size, (data.size(0) - i) // seq_length)\n","            inputs = data[i:i+seq_length * batch_size_i].view(batch_size_i, seq_length)\n","            targets = data[i+1:i+seq_length * batch_size_i+1].view(batch_size_i, seq_length)\n","\n","            states = (states[0][:, :batch_size_i, :], states[1][:, :batch_size_i, :])\n","            outputs, states = model(inputs, states)\n","            loss = criterion(outputs, targets.reshape(-1))\n","            total_loss += loss.item()\n","\n","        average_loss = total_loss / (data.size(0) // seq_length)\n","        # Calculate perplexity\n","        perplexity = math.exp(average_loss)\n","\n","        return average_loss, perplexity"]},{"cell_type":"markdown","metadata":{"id":"NB_h3w4i0GcE"},"source":["## Putting It All Together\n","\n","Now that we have defined the language model, training loop, and evaluation function, let's put everything together and train our model on the Penn Treebank dataset.\n","\n","We will:\n","1. Set the hyperparameters for the model and training\n","2. Instantiate the language model\n","3. Train the model using the `train_model` function\n","4. Evaluate the trained model on the training, validation, and test sets using the `evaluate_model` function\n","5. Print the evaluation results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3QxG4GV0GcE"},"outputs":[],"source":["# Hyperparameters\n","vocab_size = len(vocab)\n","embed_size = 128\n","hidden_size = 256\n","num_layers = 2\n","epochs = 10\n","batch_size = 32\n","seq_length = 35\n","lr = 0.001\n","clip = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4_IZK390GcF"},"outputs":[],"source":["# Instantiate the language model\n","model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxMQGz0a0GcF","outputId":"7214e89b-2658-406f-9750-56875b77b64b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Loss: 5.5414\n","Epoch: 2, Loss: 5.1255\n","Epoch: 3, Loss: 4.8867\n","Epoch: 4, Loss: 4.6955\n","Epoch: 5, Loss: 4.5191\n","Epoch: 6, Loss: 4.3592\n","Epoch: 7, Loss: 4.2185\n","Epoch: 8, Loss: 4.0856\n","Epoch: 9, Loss: 3.9541\n","Epoch: 10, Loss: 3.8305\n"]}],"source":["# Train the model\n","train_model(model, train_data, epochs, batch_size, seq_length, lr, clip)"]},{"cell_type":"markdown","metadata":{"id":"4BYr6aei0GcF"},"source":["Results:\n","* Epoch: 1, Loss: 5.5389\n","* Epoch: 2, Loss: 5.1305\n","* Epoch: 3, Loss: 4.8541\n","* Epoch: 4, Loss: 4.6449\n","* Epoch: 5, Loss: 4.4670\n","* Epoch: 6, Loss: 4.3003\n","* Epoch: 7, Loss: 4.1513\n","* Epoch: 8, Loss: 4.0137\n","* Epoch: 9, Loss: 3.8938\n","* Epoch: 10, Loss: 3.7792"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZxYzn-M0GcF"},"outputs":[],"source":["# Save the trained model\n","torch.save(model.state_dict(), 'model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLqhO58s0GcG","outputId":"7a072e8a-54ae-4e28-b6ce-004d57284b69"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n","model.load_state_dict(torch.load('model.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHTh4BZO0GcG"},"outputs":[],"source":["# Evaluate the model\n","train_loss, train_perplexity = evaluate_model(model, train_data, batch_size, seq_length)\n","val_loss, val_perplexity = evaluate_model(model, val_data, batch_size, seq_length)\n","test_loss, test_perplexity = evaluate_model(model, test_data, batch_size, seq_length)"]},{"cell_type":"markdown","metadata":{"id":"JJ-82nif0GcG"},"source":["Perplexity is a commonly used metric to evaluate language models. It measures how well the model predicts the target words in a sequence. A lower perplexity indicates better performance, as it means the model is more confident in its predictions.\n","\n","Perplexity is calculated as the exponential of the average cross-entropy loss over the evaluation data. It can be interpreted as the average number of equally likely words that the model considers at each step of the sequence.\n","\n","For example, a perplexity of 10 means that, on average, the model is considering 10 equally likely words at each step. A lower perplexity indicates that the model is more certain about its predictions and narrows down the choices more effectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACNINtOz0GcG","outputId":"0871c3f1-0645-463d-856e-4402ed15d036"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Loss: 0.1421 | Train Perplexity: 1.1527\n","Validation Loss: 0.1617 | Validation Perplexity: 1.1755\n","Test Loss: 0.1587 | Test Perplexity: 1.1719\n"]}],"source":["# Print the evaluation results\n","print(f\"Train Loss: {train_loss:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n","print(f\"Validation Loss: {val_loss:.4f} | Validation Perplexity: {val_perplexity:.4f}\")\n","print(f\"Test Loss: {test_loss:.4f} | Test Perplexity: {test_perplexity:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"9BlCa-E20GcG"},"source":["* Train Loss: 0.1421 | Train Perplexity: 1.1527\n","* Validation Loss: 0.1617 | Validation Perplexity: 1.1755\n","* Test Loss: 0.1587 | Test Perplexity: 1.1719"]},{"cell_type":"markdown","metadata":{"id":"mp9Fj6hm0GcG"},"source":["Keep in mind this model is quite bad. To make a reasonably good model you need to train it a lot more (perhaps on a GPU to do it faster as well). An example of a type of LSTM is [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) which you could test out on Huggingface [here](https://huggingface.co/google-bert/bert-large-uncased)."]},{"cell_type":"markdown","metadata":{"id":"d6t7YRdX0GcG"},"source":["## Why are RNNs and LSTMS important to you?\n","\n","These are the building blocks to the transformer architecture, which we'll look at in a different notebook. The transformer architecture underlies the GPT models and other cutting edge models used these days."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}