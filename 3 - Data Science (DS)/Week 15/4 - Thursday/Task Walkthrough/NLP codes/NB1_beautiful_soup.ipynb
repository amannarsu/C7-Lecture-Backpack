{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining text for Natural Language Processing (NLP) Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library designed to help you easily extract information from web pages by parsing HTML and XML documents. Here we provide a step-by-step tutorial on how to use Beautiful Soup for web scraping\n",
    "\n",
    "https://beautiful-soup-4.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install Necessary Libraries: Install the requests and BeautifulSoup libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Make an HTTP Request: Choose a website you want to scrape and send a GET request to it. For this example, let's scrape Google's homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://google.com'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Parse the HTML Content - Once you have the HTML content, you can use Beautiful Soup to parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Extract Data from the HTML. Let's say you want to extract all the headings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Images Maps Play YouTube News Gmail Drive More »Web History | Settings | Sign in\n",
      "Search Images Maps Play YouTube News Gmail Drive More »\n",
      "Web History | Settings | Sign in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Google offered in:  Afrikaans Sesotho isiZulu IsiXhosa Setswana Northern Sotho\n",
      "Google offered in:  Afrikaans Sesotho isiZulu IsiXhosa Setswana Northern Sotho\n",
      "Google offered in:  Afrikaans Sesotho isiZulu IsiXhosa Setswana Northern Sotho\n",
      "AdvertisingBusiness SolutionsAbout GoogleGoogle.co.za\n",
      "AdvertisingBusiness SolutionsAbout GoogleGoogle.co.za\n"
     ]
    }
   ],
   "source": [
    "headings = soup.find_all('div')\n",
    "for heading in headings:\n",
    "    print(heading.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Handle Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping web page\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    # Proceed with scraping\n",
    "    print(\"Scraping web page\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Headlines from a News Site\n",
    "\n",
    "Once you have Beautiful Soup installed on your machine, pull a website into Jupyter notebook just by entering its URL. But note that you can’t access the website directly in Beautiful Soup. You need get the document behind the webpage with requests, and then feed that document into Beautiful Soup using requests’s content attribute along with an HTML parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a website you want to scrape and send a GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/international'\n",
    "news_get_request = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the HTML content, you can use Beautiful Soup to parse the price information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = BeautifulSoup(news_get_request.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your default browser inspector to get a look at the HTML behind the webpage. The inspector is called slightly different names depending on the browser, but the keyboard shortcut is control-shift-I in Windows-based browsers, and command-option-I works in Safari. From there, say that I can see that all headlines on the home page are within an `<h3>` tag.\n",
    "\n",
    "Let’s take a look at the first three of them using findAll()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"dcr-1sp44ag\"><div class=\"dcr-v1s16m\">France</div><span class=\"show-underline dcr-1ay6c8s\">Leftwing parties form ‘Popular Front’ to contest snap election</span></h3>,\n",
       " <h3 class=\"dcr-10yfzki\"><div class=\"dcr-v1s16m\">G7 summit</div><span class=\"show-underline dcr-1ay6c8s\">Joe Biden says ‘democracies can deliver’ as G7 agree $50bn Ukraine aid deal</span></h3>,\n",
       " <h3 class=\"dcr-1iq5aaq\"><a class=\"dcr-1bsckv0\" href=\"/politics/article/2024/jun/13/rishi-sunak-denies-he-is-being-snubbed-after-awkward-start-to-g7-summit\"><div class=\"dcr-17kznwa\">UK</div><span class=\"show-underline dcr-1ay6c8s\">Sunak denies he is being snubbed after awkward G7 start</span></a></h3>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = news.findAll('h3')\n",
    "headlines[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the HTML and CSS syntax is obtained along with the headlines, when we just want the actual headlines. \n",
    "\n",
    "We can isolate the text with Beautiful Soup’s text attribute (or its get_text() function). Here, we use a for loop to make a list of all the headlines on the home page, and then we will display the first four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FranceLeftwing parties form ‘Popular Front’ to contest snap election',\n",
       " 'G7 summitJoe\\xa0Biden says ‘democracies can deliver’ as G7 agree $50bn Ukraine aid deal',\n",
       " 'UKSunak denies he is being snubbed after awkward G7 start',\n",
       " 'Middle East liveHamas official says nobody knows how many of the remaining hostages are alive']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = [headline.text for headline in headlines]\n",
    "all_headlines[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We grabbed the headlines for the day from this news website and made them into a list of strings in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
