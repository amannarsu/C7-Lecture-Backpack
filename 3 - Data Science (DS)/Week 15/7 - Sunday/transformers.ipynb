{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and Implementing Transformers: A Step-by-Step Guide\n",
    "\n",
    "## 1. Introduction to Transformers\n",
    "\n",
    "Transformers have revolutionized Natural Language Processing (NLP) and have applications in various domains. This notebook will guide you through understanding and implementing a simple transformer from scratch.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"transformer_architecture.png\" alt=\"Transformer Architecture Diagram\" style=\"width:40%; height: 40%;\">\n",
    "  <br>\n",
    "  <em>Figure 1: Transformer Architecture Diagram, Taken From \"<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>\"</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Environment\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1178e42b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch # PyTorch (alternative is Tensorflow)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy --> 300 dimensions\n",
    "# openai large embedding model --> 3072 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Embeddings\n",
    "\n",
    "Before we dive into the transformer architecture, let's start with a fundamental concept: word embeddings.\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings are dense vector representations of words. Instead of using sparse, one-hot encoded vectors, we represent each word as a dense vector of floating-point numbers. These vectors are learned from data and capture semantic relationships between words.\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div>\n",
    "    <img src=\"word_embeddings.png\" alt=\"Word Embeddings Visualization\" style=\"width:auto; height: auto;\">\n",
    "    <br>\n",
    "    <em>Figure 2: Words Represented by Vectors</em>\n",
    "  </div>\n",
    "  <div style=\"margin-left: 20px;\"> <!-- Adds some space between the images -->\n",
    "    <img src=\"word_embeddings_similarity.png\" alt=\"Word Embeddings Similarity\" style=\"width:auto; height: auto;\">\n",
    "    <br>\n",
    "    <em>Figure 3: Words Closer to Each Other are More Similar</em>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "Mathematically, an embedding layer can be thought of as a lookup table. If we have a vocabulary of size V and we want to embed each word into a D-dimensional space, we can represent this as a matrix E of shape (V, D).\n",
    "\n",
    "For a given word index i, its embedding vector $e_i$ is the i-th row of E:\n",
    "\n",
    "$$e_i = E[i, :]$$\n",
    "\n",
    "#### Example:\n",
    "Let's say we have a small vocabulary of 5 words: [\"hello\", \"world\", \"transformer\", \"example\", \"embedding\"]\n",
    "\n",
    "We want to represent each word with a 3-dimensional vector. Our embedding matrix E might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # \"hello\"\n",
    "    [0.4, 0.5, 0.6],  # \"world\"\n",
    "    [0.7, 0.8, 0.9],  # \"transformer\"\n",
    "    [1.0, 1.1, 1.2],  # \"example\"\n",
    "    [1.3, 1.4, 1.5]   # \"embedding\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the embedding for \"transformer\" (index 2):\n",
    "transformer_embedding = E[2, :]  # Result: tensor([0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformer_embedding)\n",
    "# Notice for spacy_md and spacy_lg we got 300 dimensions instead of 3\n",
    "# Usually embeddings are much larger than 3 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000000  # Size of our vocabulary\n",
    "embedding_dim = 300  # Dimension of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = SimpleEmbedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input tensor (batch_size=1, sequence_length=3)\n",
    "sample_input = torch.tensor([[3,5,9]]) # \"I love Transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3])\n",
      "Embedded output shape: 3\n"
     ]
    }
   ],
   "source": [
    "embedded_output = embed_layer(sample_input)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Embedded output shape: {embedded_output.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "embedded_output_spacy = nlp(\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_output_spacy.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3908e+00,  3.2225e-01,  1.8754e+00,  1.1043e+00, -5.2238e-01,\n",
       "         -7.4018e-01,  1.6236e-01, -2.3700e-01,  5.0993e-01,  1.6706e+00,\n",
       "          1.5921e+00, -4.1619e-01,  1.8619e+00, -1.0779e+00,  8.8486e-01,\n",
       "         -8.3421e-01,  1.0301e+00, -8.6810e-01, -5.7016e-01,  3.2332e-01,\n",
       "          1.1285e+00, -1.2123e+00,  2.6024e+00, -9.5724e-02, -8.1148e-02,\n",
       "          1.2587e+00,  8.6913e-01, -9.6094e-01,  5.1823e-02, -3.2848e-01,\n",
       "         -2.2472e+00, -4.4790e-01,  4.2347e-01, -3.8746e-01, -2.2964e-01,\n",
       "         -4.0709e-01,  8.7030e-01, -1.0553e+00, -1.3284e+00,  7.0607e-01,\n",
       "          3.5730e-01,  5.8928e-01,  9.1878e-01,  6.6628e-01,  2.4651e-01,\n",
       "          1.3287e-01,  1.2191e-01,  4.7809e-01,  2.7613e-01, -5.8957e-01,\n",
       "          5.6918e-01, -7.9110e-01, -1.9897e-01, -1.3616e+00, -5.1936e-01,\n",
       "          7.6482e-02,  3.4005e-01,  1.4557e+00, -3.4610e-01, -2.6338e-01,\n",
       "         -4.4770e-01, -7.2882e-01, -1.6066e-01, -3.2064e-01, -6.3077e-01,\n",
       "         -7.8877e-01,  1.3062e+00, -9.2758e-01, -2.6274e-01,  9.3150e-01,\n",
       "         -4.5935e-01, -9.4195e-01, -7.0892e-01,  2.1861e+00, -6.4932e-01,\n",
       "          4.5214e-01,  8.5207e-01, -1.6947e+00,  1.1806e+00, -2.8929e+00,\n",
       "         -3.8758e-01, -7.1240e-01, -1.6171e+00, -3.5899e-01,  5.1367e-02,\n",
       "          6.9502e-01,  1.8352e+00, -1.9180e+00, -1.3924e+00,  5.4047e-01,\n",
       "          4.3507e-01, -2.2717e+00, -1.3386e-01, -5.8557e-02,  1.2574e-01,\n",
       "         -5.5258e-01,  7.4480e-02, -1.4929e-01, -5.5225e-01, -9.3420e-02,\n",
       "         -1.0284e+00,  4.0444e-01,  2.1426e+00, -5.1537e-01,  1.0827e+00,\n",
       "          1.2499e+00,  9.8214e-01,  2.2690e-01,  4.9279e-01, -5.1283e-01,\n",
       "          3.0062e-01,  7.7347e-02,  6.4777e-01, -4.3242e-01,  1.1740e+00,\n",
       "          7.0114e-01,  6.6743e-01, -8.0360e-01, -1.3776e+00, -4.4105e-01,\n",
       "          1.4176e-01,  1.1085e+00,  5.5442e-01,  1.5818e+00, -1.2248e+00,\n",
       "          9.6289e-01, -1.5785e+00,  6.7160e-01, -6.0152e-02,  6.9784e-02,\n",
       "         -1.6635e+00, -7.6506e-01,  1.2306e+00,  4.2521e-01, -1.6383e-02,\n",
       "         -1.0749e-01, -1.3086e+00,  6.5981e-01, -7.0325e-02,  2.7448e-01,\n",
       "         -3.4501e-01, -1.1962e-01,  1.1862e+00, -1.2203e+00,  2.9100e-01,\n",
       "         -7.9642e-02,  1.3200e+00, -1.5197e+00, -2.9336e-01,  2.1066e+00,\n",
       "         -1.0875e-01,  6.0834e-01,  7.8943e-01,  7.8247e-01, -6.4659e-02,\n",
       "         -2.2984e-04,  6.8309e-01,  1.0637e-01,  3.5032e-01,  1.2110e-01,\n",
       "          2.9843e-01,  1.3448e+00,  1.4614e+00,  1.0566e+00,  8.1554e-01,\n",
       "         -8.2406e-01,  8.9328e-01, -3.8688e-01, -3.5718e-01, -1.1568e+00,\n",
       "         -1.7660e+00, -2.5380e+00,  9.6943e-02, -7.9121e-01,  3.7120e-01,\n",
       "          1.5118e+00, -8.9146e-01,  5.2475e-01,  3.5178e-01,  2.4913e-01,\n",
       "          1.1900e+00,  1.4109e+00,  7.9801e-01,  4.9413e-01, -1.8495e-01,\n",
       "         -1.0381e+00, -1.0130e-01, -9.2718e-01,  2.3484e-01,  8.8615e-02,\n",
       "         -3.4769e-01,  8.4907e-01,  2.0147e-01,  3.8398e-01,  1.2310e+00,\n",
       "          1.2287e+00,  7.0421e-01, -5.6285e-02, -1.4897e+00, -1.5195e+00,\n",
       "          3.2581e-01, -1.4584e+00,  1.8989e+00, -4.0566e-02, -2.9336e-01,\n",
       "          1.3978e+00, -9.1666e-01, -7.7937e-01, -4.1754e-01,  1.1060e+00,\n",
       "          2.5285e-01, -1.0754e-01,  7.7053e-01, -1.1304e+00,  9.9646e-01,\n",
       "         -1.1810e+00,  9.6260e-01, -1.1049e+00, -7.9095e-01, -2.1609e-01,\n",
       "          1.9485e-03, -2.0979e-01,  1.2010e+00,  6.7560e-01, -1.8900e+00,\n",
       "          1.9432e-01,  1.6020e+00, -1.0372e+00, -7.4869e-01, -3.8440e-01,\n",
       "          1.4350e-01, -8.1268e-02,  1.1262e+00,  4.0618e-02, -6.4643e-02,\n",
       "          3.4456e+00, -1.1129e+00, -4.3420e-01, -1.5212e-02,  5.4272e-01,\n",
       "          1.2508e-01, -8.7617e-01,  1.2223e+00,  3.2682e-01, -1.0487e-01,\n",
       "          2.4768e+00,  5.7691e-01,  1.4731e-01, -1.3136e+00, -6.0611e-01,\n",
       "          6.4498e-01, -2.4771e-01, -1.4078e+00, -8.0111e-02,  5.1941e-01,\n",
       "          1.1709e+00,  2.1780e+00,  1.7792e+00,  2.5832e-01, -2.4341e+00,\n",
       "         -3.4975e-01, -1.3381e+00, -4.3891e-01, -5.8502e-01,  1.8071e+00,\n",
       "         -7.3262e-01,  4.0940e-01, -5.8409e-01,  1.0613e-01, -3.0671e-01,\n",
       "          8.6423e-01, -1.0659e+00, -1.0130e+00, -9.9392e-01,  2.9083e+00,\n",
       "          1.4483e+00, -5.6145e-01, -9.4646e-01, -7.4197e-01,  1.5562e-01,\n",
       "         -2.5844e-01, -7.5015e-01,  1.2355e+00,  1.0141e+00,  1.0132e+00,\n",
       "          6.3464e-01,  8.7688e-01,  8.1428e-01,  1.9737e-01, -6.3676e-01,\n",
       "         -8.7683e-01, -1.5510e+00, -7.8818e-01,  5.6844e-01,  7.6224e-01,\n",
       "          5.5685e-01,  1.2984e+00,  1.7561e+00,  2.1129e-01,  1.4860e+00],\n",
       "        [-3.6094e-01,  1.0463e-01, -3.2587e-01,  3.1892e-01, -1.0977e-01,\n",
       "          9.6497e-02, -1.4932e+00,  5.2380e-01,  7.5307e-01, -2.2192e-01,\n",
       "          5.8191e-01, -1.9369e+00, -1.5334e+00, -1.7966e-01, -6.5778e-01,\n",
       "         -1.2317e+00, -1.2464e+00, -1.4996e+00, -5.4040e-01,  1.2410e+00,\n",
       "         -1.6212e+00, -9.0360e-01,  1.3968e+00,  9.1782e-01,  5.1204e-01,\n",
       "         -8.4058e-01, -1.0445e+00,  5.5477e-01, -9.4926e-01,  1.0457e+00,\n",
       "         -1.1298e+00, -2.8006e+00,  1.2797e+00,  2.2000e-01,  3.2491e-01,\n",
       "          1.3190e+00, -8.4968e-01, -6.9870e-01, -2.0516e-01, -7.8117e-01,\n",
       "          6.8727e-01,  7.8359e-01, -1.1109e+00, -3.1063e+00, -9.8977e-01,\n",
       "         -6.0220e-01, -7.1534e-01, -4.6740e-01,  5.5142e-01,  2.6549e+00,\n",
       "          1.0582e+00, -1.4682e-01, -8.9133e-01,  1.9379e-01,  1.9682e+00,\n",
       "         -7.4036e-01, -8.6657e-01, -3.0639e-01, -5.3594e-01, -3.5751e-01,\n",
       "         -1.2399e+00, -1.5235e+00, -8.1588e-01,  1.1373e+00,  2.1926e-01,\n",
       "          4.1337e-01,  6.1524e-03, -5.6728e-01, -1.7038e-01, -3.0279e-01,\n",
       "         -1.2868e+00, -1.3663e+00, -4.6252e-02, -6.1496e-01,  1.2367e+00,\n",
       "         -8.1436e-01,  1.1462e+00, -1.1787e+00, -3.6673e-02,  6.7181e-01,\n",
       "          9.2423e-01,  2.6972e-01,  6.2854e-01, -7.0662e-01, -8.5584e-01,\n",
       "          9.0406e-01, -5.6593e-01,  3.8410e-01, -7.8158e-01, -1.5094e-01,\n",
       "          4.1993e-01,  1.4059e+00, -8.2709e-01,  1.5608e+00, -1.0952e+00,\n",
       "          1.1855e+00,  1.1881e+00,  2.0556e+00,  6.6027e-01, -1.1078e+00,\n",
       "         -2.9196e-01,  4.5644e-01, -3.1470e-01, -4.1329e-01,  3.9462e-01,\n",
       "          1.1305e+00,  8.2584e-01,  9.4583e-01, -1.5447e-01, -1.6013e+00,\n",
       "         -5.9471e-02, -9.9287e-01,  1.1634e+00,  1.6095e+00, -2.9417e-01,\n",
       "          1.0819e+00,  8.8662e-01, -8.6114e-01, -2.7265e-01,  9.8042e-01,\n",
       "         -1.7533e-01, -1.2277e-01,  7.4141e-01,  3.5395e-01, -5.3458e-01,\n",
       "          6.4537e-01, -2.9891e+00,  1.8371e-01, -4.7270e-01, -9.5882e-01,\n",
       "         -1.5124e+00,  1.5068e+00, -9.3805e-01, -6.3851e-01,  2.1947e-01,\n",
       "         -4.3924e-01, -1.3911e-01, -1.8692e-02,  1.6561e+00,  1.0661e+00,\n",
       "         -1.8189e-01, -1.2380e+00,  5.1422e-01, -1.5104e-01,  1.3774e-01,\n",
       "          1.2251e+00, -7.6426e-01,  9.1838e-01,  4.0577e-01,  2.5105e-01,\n",
       "          1.2815e-01, -1.9803e-01, -1.4780e+00, -5.9103e-01,  8.3575e-01,\n",
       "         -2.2925e-01, -1.2404e+00,  2.4919e-01, -1.1416e+00,  7.8214e-01,\n",
       "          1.0817e-02,  3.8163e-01, -1.6527e+00, -3.8140e-01,  1.0699e-01,\n",
       "         -1.0150e-01,  8.3015e-02,  7.1201e-01, -9.0059e-01,  8.9069e-01,\n",
       "          4.7655e-01, -8.3963e-01,  3.3320e-01, -1.2526e+00, -5.7455e-01,\n",
       "         -1.9059e+00, -9.6654e-01,  3.6773e-01, -5.7858e-01,  1.2373e+00,\n",
       "          8.7134e-01, -5.2276e-01,  1.2400e+00, -9.0577e-01,  7.6803e-01,\n",
       "          1.6222e+00,  8.1580e-02,  2.0282e-01,  3.3024e-01, -9.5337e-01,\n",
       "          1.5735e+00,  1.8697e+00, -1.0639e+00, -2.2726e-01,  2.5006e-01,\n",
       "          1.1618e+00, -1.1422e-01, -5.6295e-02,  8.4975e-01, -8.5991e-01,\n",
       "         -6.1057e-01,  1.0629e+00,  1.2222e+00,  7.7189e-01, -1.2797e+00,\n",
       "         -1.5433e+00, -6.0202e-01,  3.2140e-01, -6.0616e-02, -1.1704e+00,\n",
       "         -2.7736e+00, -2.9825e-02, -9.1662e-01,  4.7027e-01,  1.8778e+00,\n",
       "          5.2237e-01,  5.1757e-02,  4.2602e-01,  9.4751e-01,  4.3643e-01,\n",
       "         -2.0531e-01, -1.4739e+00,  5.0663e-01,  2.7792e-01,  1.3515e+00,\n",
       "         -8.9496e-01, -1.5961e+00,  6.7372e-01, -9.9707e-01, -3.4807e-01,\n",
       "          2.1768e-01,  1.1278e+00, -1.5005e+00, -2.4048e-01, -4.8549e-01,\n",
       "         -6.6165e-02, -9.0293e-01,  6.4402e-01,  7.5918e-01, -2.0203e+00,\n",
       "         -6.7395e-01, -9.1921e-01,  1.2120e+00, -1.3463e+00, -4.8316e-01,\n",
       "          1.7186e+00, -5.6843e-01, -2.9151e+00,  1.0834e+00,  7.7311e-02,\n",
       "          1.2317e+00,  2.9194e+00,  1.9378e+00, -5.5362e-01, -1.3030e+00,\n",
       "          1.0696e+00, -4.5618e-01,  1.3635e+00, -2.4220e+00, -8.3080e-02,\n",
       "          1.0349e-01,  1.1661e-01, -2.5220e-02,  3.7887e-01,  2.4456e-01,\n",
       "         -8.9158e-01,  1.4344e+00, -1.9292e+00, -5.7138e-01, -6.6717e-01,\n",
       "         -9.2035e-02,  9.5487e-01,  1.8483e-01, -1.1677e-01, -2.2911e-01,\n",
       "         -3.4485e-01, -1.0765e+00, -5.4777e-01, -3.2893e-01,  5.8413e-02,\n",
       "          2.1100e+00,  7.7261e-01, -3.4265e-01,  1.2370e+00, -2.4977e-01,\n",
       "          2.2397e-01, -6.8755e-01, -4.8984e-01,  3.9969e-01,  6.9820e-01,\n",
       "          5.2110e-02,  2.8821e-01,  5.9505e-02,  1.7286e+00,  2.9208e-01,\n",
       "         -6.9259e-01, -8.4428e-01, -3.2920e-01, -1.1403e-01, -8.4522e-01],\n",
       "        [-3.1789e-01, -3.7737e-01,  2.2604e+00, -3.3095e-01, -7.1942e-01,\n",
       "          1.2199e+00,  1.4356e+00, -3.1398e-01,  8.9790e-01,  6.3589e-01,\n",
       "         -8.4757e-01, -9.3132e-02, -3.9360e-01, -2.4841e-02, -3.6326e-01,\n",
       "         -6.9412e-01, -9.8163e-01, -5.5561e-02, -1.0469e+00, -1.6153e-01,\n",
       "         -5.7177e-01, -1.5561e+00, -9.6327e-01, -4.3664e-01, -8.5481e-03,\n",
       "          4.4600e-02, -3.5374e-01,  1.5752e-01, -1.1567e+00,  1.8156e+00,\n",
       "         -2.0921e+00, -6.5174e-01,  1.1426e+00, -7.5383e-01, -1.4663e+00,\n",
       "          8.0236e-02, -6.3170e-01, -7.4101e-01,  1.8064e+00,  9.3781e-01,\n",
       "         -3.8453e-01,  6.5850e-01,  7.6168e-01, -6.4509e-01, -3.6308e+00,\n",
       "         -2.1864e+00,  2.6440e-01, -5.5988e-01,  1.4537e+00, -2.9629e-01,\n",
       "         -4.7022e-01, -1.4991e+00,  2.2968e+00,  1.6495e+00,  1.3179e+00,\n",
       "          7.5565e-01,  1.2472e+00,  7.8814e-01,  1.5493e+00, -6.0887e-01,\n",
       "         -2.7026e+00, -6.1087e-01,  1.1898e+00, -4.8020e-01,  2.2536e+00,\n",
       "          1.1718e+00,  8.7930e-01, -7.7967e-01, -7.8098e-02, -3.7233e-01,\n",
       "          3.6377e-01,  1.2563e+00, -1.2208e-01,  1.0121e-01,  4.7121e-01,\n",
       "          6.8402e-01,  5.0992e-01, -7.8017e-01,  6.6291e-01,  6.5567e-01,\n",
       "          5.8466e-02,  7.8824e-01, -1.0858e+00,  1.0520e+00, -3.8932e-01,\n",
       "          1.4754e+00, -1.7087e-01, -2.0884e+00,  7.9635e-01,  4.9622e-01,\n",
       "          6.0295e-01, -5.2264e-01,  1.0361e+00,  5.3184e-01, -3.1479e-01,\n",
       "          2.1018e-02, -5.4547e-02, -8.1160e-01, -2.6107e-01, -6.9258e-01,\n",
       "          1.5523e+00, -2.3087e+00, -2.1958e+00,  3.2025e-01,  7.7272e-01,\n",
       "         -1.6666e-01, -1.1849e-02, -1.1285e-01, -6.8382e-01, -1.2514e+00,\n",
       "         -7.5954e-02,  3.7892e-01,  6.2014e-01, -8.9876e-02,  1.2097e+00,\n",
       "          8.7672e-01,  1.8313e+00, -6.1592e-01, -6.0728e-01, -2.0597e+00,\n",
       "          1.5289e+00,  3.3787e-01,  1.9154e-01,  1.6352e-01,  6.7102e-01,\n",
       "         -4.0961e-01, -5.3023e-01,  2.5329e-01, -1.9900e-01,  6.1014e-01,\n",
       "         -1.4391e+00,  1.6621e+00,  3.5558e-01, -1.8120e+00,  4.6457e-01,\n",
       "         -5.4800e-01, -1.0596e+00,  1.7401e-01,  3.8216e-01, -1.9578e-01,\n",
       "         -1.5132e-01,  6.2558e-01, -6.2190e-01, -1.0873e+00, -1.3252e+00,\n",
       "          3.7723e-01, -5.8415e-02, -1.4766e+00, -9.8602e-01,  1.4866e+00,\n",
       "          1.4713e-01, -1.3660e+00, -6.7085e-01,  9.5211e-01,  1.4749e+00,\n",
       "         -1.4756e+00, -8.6603e-01,  1.2781e+00,  3.5260e-01, -7.5008e-02,\n",
       "          4.0587e-01,  5.3512e-01, -6.8785e-02, -6.1548e-01,  2.6958e-01,\n",
       "         -3.1602e-02, -1.2757e+00, -6.3726e-01, -7.6155e-01, -4.6703e-01,\n",
       "         -1.2028e+00, -2.4588e+00, -4.8989e-01, -1.5937e+00,  9.4815e-01,\n",
       "         -4.2648e-01, -1.4827e+00, -4.5045e-01,  8.8896e-01, -1.1526e+00,\n",
       "          2.9480e-02, -5.1994e-01, -1.6538e-01, -2.7733e-01, -2.4466e-01,\n",
       "         -1.9880e+00, -1.2664e+00, -3.0722e-01,  8.3983e-01, -4.6888e-01,\n",
       "          2.2659e-01,  3.4187e-01,  5.9337e-01,  1.9173e+00, -4.7865e-01,\n",
       "         -5.7824e-02, -1.7239e+00, -9.9090e-01,  1.9552e+00, -6.5327e-02,\n",
       "          1.4630e-01,  1.1357e+00, -2.6885e-01, -9.1267e-01,  6.8663e-01,\n",
       "          1.5644e+00,  1.0132e+00, -1.1486e+00, -7.9156e-01, -3.2136e-01,\n",
       "          5.4563e-01, -1.2671e+00,  5.7798e-01, -2.1042e-02, -1.3801e-01,\n",
       "          9.9371e-02, -1.6284e-01,  1.8978e-01, -1.2572e+00,  2.5705e-01,\n",
       "         -1.0626e+00, -6.3258e-01, -6.2932e-01, -1.6768e+00,  6.7240e-01,\n",
       "          1.9889e+00,  8.1569e-01, -1.4683e+00,  1.6630e+00, -1.4545e+00,\n",
       "         -2.3146e-01,  5.5500e-01,  3.2450e-01,  1.4937e+00,  5.8534e-01,\n",
       "          7.5996e-01, -1.0136e+00, -1.3920e+00,  8.8565e-01,  9.1616e-01,\n",
       "          4.8508e-01, -1.0356e+00,  1.6210e-01, -3.4564e-01,  7.7186e-01,\n",
       "          1.6738e-02,  6.8036e-01, -1.2983e-01,  9.7313e-02,  7.9569e-01,\n",
       "         -2.1607e+00, -5.6940e-01, -2.0023e+00, -1.2304e+00,  8.7704e-01,\n",
       "         -2.0921e+00,  1.5937e+00,  2.5637e+00, -1.2679e-01,  2.3143e-01,\n",
       "          7.9244e-01, -3.0765e-01,  6.7602e-01,  2.6806e+00, -8.7078e-01,\n",
       "          3.6106e-02,  1.0990e+00, -2.8002e-01,  5.3109e-01,  5.3204e-01,\n",
       "         -1.5853e+00,  2.4220e+00,  4.7723e-01,  5.9568e-01,  2.7927e-01,\n",
       "          2.3933e-01,  4.7382e-01,  3.1056e-02, -1.4894e-01, -3.6520e-01,\n",
       "         -1.8156e+00,  1.1129e+00,  1.1716e+00, -1.7179e+00,  1.0240e+00,\n",
       "         -1.0366e+00, -1.9978e+00,  1.5088e+00,  1.9653e-01,  1.0685e+00,\n",
       "          4.8509e-01,  5.9623e-03,  1.0007e+00,  7.0487e-01, -6.9784e-01,\n",
       "          4.7286e-01, -6.5665e-01, -8.6782e-01, -1.0432e-01,  9.7556e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_output[0] # \"Transformer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we've created a simple embedding layer. Each word (represented by an integer) is mapped to a vector of size `embedding_dim`.\n",
    "\n",
    "## 4. Positional Encoding\n",
    "\n",
    "Next, let's implement positional encoding. This is crucial for transformers to understand the order of sequences.\n",
    "\n",
    "\n",
    "Next, let's understand and implement positional encoding. This is crucial for transformers to understand the order of sequences.\n",
    "\n",
    "### Why do we need Positional Encoding?\n",
    "\n",
    "Unlike recurrent neural networks (RNNs), transformers process all words in a sequence simultaneously. This parallelization is great for efficiency, but it means the model loses information about the order of words. Positional encoding solves this by adding position-dependent patterns to the input embeddings.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "For a position pos and dimension i in the embedding, the positional encoding PE is defined as:\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where $d_{model}$ is the dimensionality of the model's embeddings.\n",
    "\n",
    "#### Example:\n",
    "Let's calculate the positional encoding for the word \"transformer\" in a sentence, assuming it's at position 2 (0-indexed) and we're using a 4-dimensional model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoding for 'transformer': tensor([[ 9.0930e-01, -4.1615e-01,  9.8045e-01, -1.9678e-01,  9.9998e-01,\n",
      "          6.3404e-03,  9.8254e-01,  1.8604e-01,  9.4039e-01,  3.4011e-01,\n",
      "          8.8306e-01,  4.6926e-01,  8.1762e-01,  5.7576e-01,  7.4906e-01,\n",
      "          6.6251e-01,  6.8076e-01,  7.3251e-01,  6.1490e-01,  7.8860e-01,\n",
      "          5.5281e-01,  8.3331e-01,  4.9518e-01,  8.6879e-01,  4.4231e-01,\n",
      "          8.9686e-01,  3.9423e-01,  9.1901e-01,  3.5077e-01,  9.3646e-01,\n",
      "          3.1170e-01,  9.5018e-01,  2.7669e-01,  9.6096e-01,  2.4542e-01,\n",
      "          9.6942e-01,  2.1754e-01,  9.7605e-01,  1.9274e-01,  9.8125e-01,\n",
      "          1.7070e-01,  9.8532e-01,  1.5113e-01,  9.8851e-01,  1.3378e-01,\n",
      "          9.9101e-01,  1.1840e-01,  9.9297e-01,  1.0477e-01,  9.9450e-01,\n",
      "          9.2698e-02,  9.9569e-01,  8.2012e-02,  9.9663e-01,  7.2552e-02,\n",
      "          9.9736e-01,  6.4180e-02,  9.9794e-01,  5.6771e-02,  9.9839e-01,\n",
      "          5.0217e-02,  9.9874e-01,  4.4417e-02,  9.9901e-01,  3.9287e-02,\n",
      "          9.9923e-01,  3.4749e-02,  9.9940e-01,  3.0735e-02,  9.9953e-01,\n",
      "          2.7184e-02,  9.9963e-01,  2.4043e-02,  9.9971e-01,  2.1265e-02,\n",
      "          9.9977e-01,  1.8808e-02,  9.9982e-01,  1.6635e-02,  9.9986e-01,\n",
      "          1.4712e-02,  9.9989e-01,  1.3012e-02,  9.9992e-01,  1.1509e-02,\n",
      "          9.9993e-01,  1.0179e-02,  9.9995e-01,  9.0024e-03,  9.9996e-01,\n",
      "          7.9621e-03,  9.9997e-01,  7.0420e-03,  9.9998e-01,  6.2282e-03,\n",
      "          9.9998e-01,  5.5084e-03,  9.9998e-01,  4.8719e-03,  9.9999e-01,\n",
      "          4.3089e-03,  9.9999e-01,  3.8109e-03,  9.9999e-01,  3.3705e-03,\n",
      "          9.9999e-01,  2.9810e-03,  1.0000e+00,  2.6365e-03,  1.0000e+00,\n",
      "          2.3318e-03,  1.0000e+00,  2.0624e-03,  1.0000e+00,  1.8240e-03,\n",
      "          1.0000e+00,  1.6132e-03,  1.0000e+00,  1.4268e-03,  1.0000e+00,\n",
      "          1.2619e-03,  1.0000e+00,  1.1161e-03,  1.0000e+00,  9.8710e-04,\n",
      "          1.0000e+00,  8.7303e-04,  1.0000e+00,  7.7214e-04,  1.0000e+00,\n",
      "          6.8291e-04,  1.0000e+00,  6.0399e-04,  1.0000e+00,  5.3419e-04,\n",
      "          1.0000e+00,  4.7246e-04,  1.0000e+00,  4.1786e-04,  1.0000e+00,\n",
      "          3.6957e-04,  1.0000e+00,  3.2686e-04,  1.0000e+00,  2.8909e-04,\n",
      "          1.0000e+00,  2.5568e-04,  1.0000e+00,  2.2613e-04,  1.0000e+00,\n",
      "          2.0000e-04,  1.0000e+00,  1.7689e-04,  1.0000e+00,  1.5645e-04,\n",
      "          1.0000e+00,  1.3837e-04,  1.0000e+00,  1.2238e-04,  1.0000e+00,\n",
      "          1.0823e-04,  1.0000e+00,  9.5726e-05,  1.0000e+00,  8.4664e-05,\n",
      "          1.0000e+00,  7.4880e-05,  1.0000e+00,  6.6226e-05,  1.0000e+00,\n",
      "          5.8573e-05,  1.0000e+00,  5.1804e-05,  1.0000e+00,  4.5817e-05,\n",
      "          1.0000e+00,  4.0523e-05,  1.0000e+00,  3.5840e-05,  1.0000e+00,\n",
      "          3.1698e-05,  1.0000e+00,  2.8035e-05,  1.0000e+00,  2.4795e-05,\n",
      "          1.0000e+00,  2.1930e-05,  1.0000e+00,  1.9395e-05,  1.0000e+00,\n",
      "          1.7154e-05,  1.0000e+00,  1.5172e-05,  1.0000e+00,  1.3418e-05,\n",
      "          1.0000e+00,  1.1868e-05,  1.0000e+00,  1.0496e-05,  1.0000e+00,\n",
      "          9.2832e-06,  1.0000e+00,  8.2104e-06,  1.0000e+00,  7.2616e-06,\n",
      "          1.0000e+00,  6.4224e-06,  1.0000e+00,  5.6802e-06,  1.0000e+00,\n",
      "          5.0238e-06,  1.0000e+00,  4.4432e-06,  1.0000e+00,  3.9297e-06,\n",
      "          1.0000e+00,  3.4756e-06,  1.0000e+00,  3.0739e-06,  1.0000e+00,\n",
      "          2.7187e-06,  1.0000e+00,  2.4045e-06,  1.0000e+00,  2.1267e-06,\n",
      "          1.0000e+00,  1.8809e-06,  1.0000e+00,  1.6635e-06,  1.0000e+00,\n",
      "          1.4713e-06,  1.0000e+00,  1.3013e-06,  1.0000e+00,  1.1509e-06,\n",
      "          1.0000e+00,  1.0179e-06,  1.0000e+00,  9.0025e-07,  1.0000e+00,\n",
      "          7.9621e-07,  1.0000e+00,  7.0420e-07,  1.0000e+00,  6.2282e-07,\n",
      "          1.0000e+00,  5.5085e-07,  1.0000e+00,  4.8719e-07,  1.0000e+00,\n",
      "          4.3089e-07,  1.0000e+00,  3.8109e-07,  1.0000e+00,  3.3705e-07,\n",
      "          1.0000e+00,  2.9810e-07,  1.0000e+00,  2.6365e-07,  1.0000e+00,\n",
      "          2.3318e-07,  1.0000e+00,  2.0624e-07,  1.0000e+00,  1.8240e-07,\n",
      "          1.0000e+00,  1.6132e-07,  1.0000e+00,  1.4268e-07,  1.0000e+00,\n",
      "          1.2619e-07,  1.0000e+00,  1.1161e-07,  1.0000e+00,  9.8710e-08,\n",
      "          1.0000e+00,  8.7303e-08,  1.0000e+00,  7.7214e-08,  1.0000e+00,\n",
      "          6.8291e-08,  1.0000e+00,  6.0399e-08,  1.0000e+00,  5.3419e-08,\n",
      "          1.0000e+00,  4.7246e-08,  1.0000e+00,  4.1786e-08,  1.0000e+00,\n",
      "          3.6957e-08,  1.0000e+00,  3.2686e-08,  1.0000e+00,  2.8909e-08,\n",
      "          1.0000e+00,  2.5568e-08,  1.0000e+00,  2.2613e-08,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# \"The good transformer (Optimus) beat the bad transformer (Megatron)\"\n",
    "\n",
    "import math\n",
    "\n",
    "d_model = 300\n",
    "pos = 2\n",
    "\n",
    "pe = torch.zeros(1, d_model)\n",
    "for i in range(0, d_model, 2):\n",
    "    pe[0, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "    pe[0, i+1] = math.cos(pos / (10000 ** (2 * i / d_model)))\n",
    "\n",
    "print(\"Positional encoding for 'transformer':\", pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding is unique for position 2 and will be different for other positions, allowing the model to distinguish word positions.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"positional_encoding.png\" alt=\"Positional Encoding\" style=\"width:60%; height: 60%;\">\n",
    "  <br>\n",
    "  <em>Figure 4: Positional Encoding of \"I am a robot\"</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a long enough 'pe' matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Create a vector of shape (max_seq_length, 1)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Create a vector of shape (d_model/2)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register pe as a buffer (won't be considered a model parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input <-- 300 dimensions\n",
    "# positional encoding <-- 300 dimensions\n",
    "\n",
    "# input + positional encoding --> encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 300  # Should match the embedding dimension\n",
    "pos_encoder = PositionalEncoding(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoded output shape: torch.Size([1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "# Use our previous embedded output\n",
    "positional_encoded = pos_encoder(embedded_output)\n",
    "print(f\"Positional encoded output shape: {positional_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tensor of embedded_output:\n",
      "tensor([[-2.3908e+00,  3.2225e-01,  1.8754e+00,  1.1043e+00, -5.2238e-01,\n",
      "         -7.4018e-01,  1.6236e-01, -2.3700e-01,  5.0993e-01,  1.6706e+00,\n",
      "          1.5921e+00, -4.1619e-01,  1.8619e+00, -1.0779e+00,  8.8486e-01,\n",
      "         -8.3421e-01,  1.0301e+00, -8.6810e-01, -5.7016e-01,  3.2332e-01,\n",
      "          1.1285e+00, -1.2123e+00,  2.6024e+00, -9.5724e-02, -8.1148e-02,\n",
      "          1.2587e+00,  8.6913e-01, -9.6094e-01,  5.1823e-02, -3.2848e-01,\n",
      "         -2.2472e+00, -4.4790e-01,  4.2347e-01, -3.8746e-01, -2.2964e-01,\n",
      "         -4.0709e-01,  8.7030e-01, -1.0553e+00, -1.3284e+00,  7.0607e-01,\n",
      "          3.5730e-01,  5.8928e-01,  9.1878e-01,  6.6628e-01,  2.4651e-01,\n",
      "          1.3287e-01,  1.2191e-01,  4.7809e-01,  2.7613e-01, -5.8957e-01,\n",
      "          5.6918e-01, -7.9110e-01, -1.9897e-01, -1.3616e+00, -5.1936e-01,\n",
      "          7.6482e-02,  3.4005e-01,  1.4557e+00, -3.4610e-01, -2.6338e-01,\n",
      "         -4.4770e-01, -7.2882e-01, -1.6066e-01, -3.2064e-01, -6.3077e-01,\n",
      "         -7.8877e-01,  1.3062e+00, -9.2758e-01, -2.6274e-01,  9.3150e-01,\n",
      "         -4.5935e-01, -9.4195e-01, -7.0892e-01,  2.1861e+00, -6.4932e-01,\n",
      "          4.5214e-01,  8.5207e-01, -1.6947e+00,  1.1806e+00, -2.8929e+00,\n",
      "         -3.8758e-01, -7.1240e-01, -1.6171e+00, -3.5899e-01,  5.1367e-02,\n",
      "          6.9502e-01,  1.8352e+00, -1.9180e+00, -1.3924e+00,  5.4047e-01,\n",
      "          4.3507e-01, -2.2717e+00, -1.3386e-01, -5.8557e-02,  1.2574e-01,\n",
      "         -5.5258e-01,  7.4480e-02, -1.4929e-01, -5.5225e-01, -9.3420e-02,\n",
      "         -1.0284e+00,  4.0444e-01,  2.1426e+00, -5.1537e-01,  1.0827e+00,\n",
      "          1.2499e+00,  9.8214e-01,  2.2690e-01,  4.9279e-01, -5.1283e-01,\n",
      "          3.0062e-01,  7.7347e-02,  6.4777e-01, -4.3242e-01,  1.1740e+00,\n",
      "          7.0114e-01,  6.6743e-01, -8.0360e-01, -1.3776e+00, -4.4105e-01,\n",
      "          1.4176e-01,  1.1085e+00,  5.5442e-01,  1.5818e+00, -1.2248e+00,\n",
      "          9.6289e-01, -1.5785e+00,  6.7160e-01, -6.0152e-02,  6.9784e-02,\n",
      "         -1.6635e+00, -7.6506e-01,  1.2306e+00,  4.2521e-01, -1.6383e-02,\n",
      "         -1.0749e-01, -1.3086e+00,  6.5981e-01, -7.0325e-02,  2.7448e-01,\n",
      "         -3.4501e-01, -1.1962e-01,  1.1862e+00, -1.2203e+00,  2.9100e-01,\n",
      "         -7.9642e-02,  1.3200e+00, -1.5197e+00, -2.9336e-01,  2.1066e+00,\n",
      "         -1.0875e-01,  6.0834e-01,  7.8943e-01,  7.8247e-01, -6.4659e-02,\n",
      "         -2.2984e-04,  6.8309e-01,  1.0637e-01,  3.5032e-01,  1.2110e-01,\n",
      "          2.9843e-01,  1.3448e+00,  1.4614e+00,  1.0566e+00,  8.1554e-01,\n",
      "         -8.2406e-01,  8.9328e-01, -3.8688e-01, -3.5718e-01, -1.1568e+00,\n",
      "         -1.7660e+00, -2.5380e+00,  9.6943e-02, -7.9121e-01,  3.7120e-01,\n",
      "          1.5118e+00, -8.9146e-01,  5.2475e-01,  3.5178e-01,  2.4913e-01,\n",
      "          1.1900e+00,  1.4109e+00,  7.9801e-01,  4.9413e-01, -1.8495e-01,\n",
      "         -1.0381e+00, -1.0130e-01, -9.2718e-01,  2.3484e-01,  8.8615e-02,\n",
      "         -3.4769e-01,  8.4907e-01,  2.0147e-01,  3.8398e-01,  1.2310e+00,\n",
      "          1.2287e+00,  7.0421e-01, -5.6285e-02, -1.4897e+00, -1.5195e+00,\n",
      "          3.2581e-01, -1.4584e+00,  1.8989e+00, -4.0566e-02, -2.9336e-01,\n",
      "          1.3978e+00, -9.1666e-01, -7.7937e-01, -4.1754e-01,  1.1060e+00,\n",
      "          2.5285e-01, -1.0754e-01,  7.7053e-01, -1.1304e+00,  9.9646e-01,\n",
      "         -1.1810e+00,  9.6260e-01, -1.1049e+00, -7.9095e-01, -2.1609e-01,\n",
      "          1.9485e-03, -2.0979e-01,  1.2010e+00,  6.7560e-01, -1.8900e+00,\n",
      "          1.9432e-01,  1.6020e+00, -1.0372e+00, -7.4869e-01, -3.8440e-01,\n",
      "          1.4350e-01, -8.1268e-02,  1.1262e+00,  4.0618e-02, -6.4643e-02,\n",
      "          3.4456e+00, -1.1129e+00, -4.3420e-01, -1.5212e-02,  5.4272e-01,\n",
      "          1.2508e-01, -8.7617e-01,  1.2223e+00,  3.2682e-01, -1.0487e-01,\n",
      "          2.4768e+00,  5.7691e-01,  1.4731e-01, -1.3136e+00, -6.0611e-01,\n",
      "          6.4498e-01, -2.4771e-01, -1.4078e+00, -8.0111e-02,  5.1941e-01,\n",
      "          1.1709e+00,  2.1780e+00,  1.7792e+00,  2.5832e-01, -2.4341e+00,\n",
      "         -3.4975e-01, -1.3381e+00, -4.3891e-01, -5.8502e-01,  1.8071e+00,\n",
      "         -7.3262e-01,  4.0940e-01, -5.8409e-01,  1.0613e-01, -3.0671e-01,\n",
      "          8.6423e-01, -1.0659e+00, -1.0130e+00, -9.9392e-01,  2.9083e+00,\n",
      "          1.4483e+00, -5.6145e-01, -9.4646e-01, -7.4197e-01,  1.5562e-01,\n",
      "         -2.5844e-01, -7.5015e-01,  1.2355e+00,  1.0141e+00,  1.0132e+00,\n",
      "          6.3464e-01,  8.7688e-01,  8.1428e-01,  1.9737e-01, -6.3676e-01,\n",
      "         -8.7683e-01, -1.5510e+00, -7.8818e-01,  5.6844e-01,  7.6224e-01,\n",
      "          5.5685e-01,  1.2984e+00,  1.7561e+00,  2.1129e-01,  1.4860e+00],\n",
      "        [-3.6094e-01,  1.0463e-01, -3.2587e-01,  3.1892e-01, -1.0977e-01,\n",
      "          9.6497e-02, -1.4932e+00,  5.2380e-01,  7.5307e-01, -2.2192e-01,\n",
      "          5.8191e-01, -1.9369e+00, -1.5334e+00, -1.7966e-01, -6.5778e-01,\n",
      "         -1.2317e+00, -1.2464e+00, -1.4996e+00, -5.4040e-01,  1.2410e+00,\n",
      "         -1.6212e+00, -9.0360e-01,  1.3968e+00,  9.1782e-01,  5.1204e-01,\n",
      "         -8.4058e-01, -1.0445e+00,  5.5477e-01, -9.4926e-01,  1.0457e+00,\n",
      "         -1.1298e+00, -2.8006e+00,  1.2797e+00,  2.2000e-01,  3.2491e-01,\n",
      "          1.3190e+00, -8.4968e-01, -6.9870e-01, -2.0516e-01, -7.8117e-01,\n",
      "          6.8727e-01,  7.8359e-01, -1.1109e+00, -3.1063e+00, -9.8977e-01,\n",
      "         -6.0220e-01, -7.1534e-01, -4.6740e-01,  5.5142e-01,  2.6549e+00,\n",
      "          1.0582e+00, -1.4682e-01, -8.9133e-01,  1.9379e-01,  1.9682e+00,\n",
      "         -7.4036e-01, -8.6657e-01, -3.0639e-01, -5.3594e-01, -3.5751e-01,\n",
      "         -1.2399e+00, -1.5235e+00, -8.1588e-01,  1.1373e+00,  2.1926e-01,\n",
      "          4.1337e-01,  6.1524e-03, -5.6728e-01, -1.7038e-01, -3.0279e-01,\n",
      "         -1.2868e+00, -1.3663e+00, -4.6252e-02, -6.1496e-01,  1.2367e+00,\n",
      "         -8.1436e-01,  1.1462e+00, -1.1787e+00, -3.6673e-02,  6.7181e-01,\n",
      "          9.2423e-01,  2.6972e-01,  6.2854e-01, -7.0662e-01, -8.5584e-01,\n",
      "          9.0406e-01, -5.6593e-01,  3.8410e-01, -7.8158e-01, -1.5094e-01,\n",
      "          4.1993e-01,  1.4059e+00, -8.2709e-01,  1.5608e+00, -1.0952e+00,\n",
      "          1.1855e+00,  1.1881e+00,  2.0556e+00,  6.6027e-01, -1.1078e+00,\n",
      "         -2.9196e-01,  4.5644e-01, -3.1470e-01, -4.1329e-01,  3.9462e-01,\n",
      "          1.1305e+00,  8.2584e-01,  9.4583e-01, -1.5447e-01, -1.6013e+00,\n",
      "         -5.9471e-02, -9.9287e-01,  1.1634e+00,  1.6095e+00, -2.9417e-01,\n",
      "          1.0819e+00,  8.8662e-01, -8.6114e-01, -2.7265e-01,  9.8042e-01,\n",
      "         -1.7533e-01, -1.2277e-01,  7.4141e-01,  3.5395e-01, -5.3458e-01,\n",
      "          6.4537e-01, -2.9891e+00,  1.8371e-01, -4.7270e-01, -9.5882e-01,\n",
      "         -1.5124e+00,  1.5068e+00, -9.3805e-01, -6.3851e-01,  2.1947e-01,\n",
      "         -4.3924e-01, -1.3911e-01, -1.8692e-02,  1.6561e+00,  1.0661e+00,\n",
      "         -1.8189e-01, -1.2380e+00,  5.1422e-01, -1.5104e-01,  1.3774e-01,\n",
      "          1.2251e+00, -7.6426e-01,  9.1838e-01,  4.0577e-01,  2.5105e-01,\n",
      "          1.2815e-01, -1.9803e-01, -1.4780e+00, -5.9103e-01,  8.3575e-01,\n",
      "         -2.2925e-01, -1.2404e+00,  2.4919e-01, -1.1416e+00,  7.8214e-01,\n",
      "          1.0817e-02,  3.8163e-01, -1.6527e+00, -3.8140e-01,  1.0699e-01,\n",
      "         -1.0150e-01,  8.3015e-02,  7.1201e-01, -9.0059e-01,  8.9069e-01,\n",
      "          4.7655e-01, -8.3963e-01,  3.3320e-01, -1.2526e+00, -5.7455e-01,\n",
      "         -1.9059e+00, -9.6654e-01,  3.6773e-01, -5.7858e-01,  1.2373e+00,\n",
      "          8.7134e-01, -5.2276e-01,  1.2400e+00, -9.0577e-01,  7.6803e-01,\n",
      "          1.6222e+00,  8.1580e-02,  2.0282e-01,  3.3024e-01, -9.5337e-01,\n",
      "          1.5735e+00,  1.8697e+00, -1.0639e+00, -2.2726e-01,  2.5006e-01,\n",
      "          1.1618e+00, -1.1422e-01, -5.6295e-02,  8.4975e-01, -8.5991e-01,\n",
      "         -6.1057e-01,  1.0629e+00,  1.2222e+00,  7.7189e-01, -1.2797e+00,\n",
      "         -1.5433e+00, -6.0202e-01,  3.2140e-01, -6.0616e-02, -1.1704e+00,\n",
      "         -2.7736e+00, -2.9825e-02, -9.1662e-01,  4.7027e-01,  1.8778e+00,\n",
      "          5.2237e-01,  5.1757e-02,  4.2602e-01,  9.4751e-01,  4.3643e-01,\n",
      "         -2.0531e-01, -1.4739e+00,  5.0663e-01,  2.7792e-01,  1.3515e+00,\n",
      "         -8.9496e-01, -1.5961e+00,  6.7372e-01, -9.9707e-01, -3.4807e-01,\n",
      "          2.1768e-01,  1.1278e+00, -1.5005e+00, -2.4048e-01, -4.8549e-01,\n",
      "         -6.6165e-02, -9.0293e-01,  6.4402e-01,  7.5918e-01, -2.0203e+00,\n",
      "         -6.7395e-01, -9.1921e-01,  1.2120e+00, -1.3463e+00, -4.8316e-01,\n",
      "          1.7186e+00, -5.6843e-01, -2.9151e+00,  1.0834e+00,  7.7311e-02,\n",
      "          1.2317e+00,  2.9194e+00,  1.9378e+00, -5.5362e-01, -1.3030e+00,\n",
      "          1.0696e+00, -4.5618e-01,  1.3635e+00, -2.4220e+00, -8.3080e-02,\n",
      "          1.0349e-01,  1.1661e-01, -2.5220e-02,  3.7887e-01,  2.4456e-01,\n",
      "         -8.9158e-01,  1.4344e+00, -1.9292e+00, -5.7138e-01, -6.6717e-01,\n",
      "         -9.2035e-02,  9.5487e-01,  1.8483e-01, -1.1677e-01, -2.2911e-01,\n",
      "         -3.4485e-01, -1.0765e+00, -5.4777e-01, -3.2893e-01,  5.8413e-02,\n",
      "          2.1100e+00,  7.7261e-01, -3.4265e-01,  1.2370e+00, -2.4977e-01,\n",
      "          2.2397e-01, -6.8755e-01, -4.8984e-01,  3.9969e-01,  6.9820e-01,\n",
      "          5.2110e-02,  2.8821e-01,  5.9505e-02,  1.7286e+00,  2.9208e-01,\n",
      "         -6.9259e-01, -8.4428e-01, -3.2920e-01, -1.1403e-01, -8.4522e-01],\n",
      "        [-3.1789e-01, -3.7737e-01,  2.2604e+00, -3.3095e-01, -7.1942e-01,\n",
      "          1.2199e+00,  1.4356e+00, -3.1398e-01,  8.9790e-01,  6.3589e-01,\n",
      "         -8.4757e-01, -9.3132e-02, -3.9360e-01, -2.4841e-02, -3.6326e-01,\n",
      "         -6.9412e-01, -9.8163e-01, -5.5561e-02, -1.0469e+00, -1.6153e-01,\n",
      "         -5.7177e-01, -1.5561e+00, -9.6327e-01, -4.3664e-01, -8.5481e-03,\n",
      "          4.4600e-02, -3.5374e-01,  1.5752e-01, -1.1567e+00,  1.8156e+00,\n",
      "         -2.0921e+00, -6.5174e-01,  1.1426e+00, -7.5383e-01, -1.4663e+00,\n",
      "          8.0236e-02, -6.3170e-01, -7.4101e-01,  1.8064e+00,  9.3781e-01,\n",
      "         -3.8453e-01,  6.5850e-01,  7.6168e-01, -6.4509e-01, -3.6308e+00,\n",
      "         -2.1864e+00,  2.6440e-01, -5.5988e-01,  1.4537e+00, -2.9629e-01,\n",
      "         -4.7022e-01, -1.4991e+00,  2.2968e+00,  1.6495e+00,  1.3179e+00,\n",
      "          7.5565e-01,  1.2472e+00,  7.8814e-01,  1.5493e+00, -6.0887e-01,\n",
      "         -2.7026e+00, -6.1087e-01,  1.1898e+00, -4.8020e-01,  2.2536e+00,\n",
      "          1.1718e+00,  8.7930e-01, -7.7967e-01, -7.8098e-02, -3.7233e-01,\n",
      "          3.6377e-01,  1.2563e+00, -1.2208e-01,  1.0121e-01,  4.7121e-01,\n",
      "          6.8402e-01,  5.0992e-01, -7.8017e-01,  6.6291e-01,  6.5567e-01,\n",
      "          5.8466e-02,  7.8824e-01, -1.0858e+00,  1.0520e+00, -3.8932e-01,\n",
      "          1.4754e+00, -1.7087e-01, -2.0884e+00,  7.9635e-01,  4.9622e-01,\n",
      "          6.0295e-01, -5.2264e-01,  1.0361e+00,  5.3184e-01, -3.1479e-01,\n",
      "          2.1018e-02, -5.4547e-02, -8.1160e-01, -2.6107e-01, -6.9258e-01,\n",
      "          1.5523e+00, -2.3087e+00, -2.1958e+00,  3.2025e-01,  7.7272e-01,\n",
      "         -1.6666e-01, -1.1849e-02, -1.1285e-01, -6.8382e-01, -1.2514e+00,\n",
      "         -7.5954e-02,  3.7892e-01,  6.2014e-01, -8.9876e-02,  1.2097e+00,\n",
      "          8.7672e-01,  1.8313e+00, -6.1592e-01, -6.0728e-01, -2.0597e+00,\n",
      "          1.5289e+00,  3.3787e-01,  1.9154e-01,  1.6352e-01,  6.7102e-01,\n",
      "         -4.0961e-01, -5.3023e-01,  2.5329e-01, -1.9900e-01,  6.1014e-01,\n",
      "         -1.4391e+00,  1.6621e+00,  3.5558e-01, -1.8120e+00,  4.6457e-01,\n",
      "         -5.4800e-01, -1.0596e+00,  1.7401e-01,  3.8216e-01, -1.9578e-01,\n",
      "         -1.5132e-01,  6.2558e-01, -6.2190e-01, -1.0873e+00, -1.3252e+00,\n",
      "          3.7723e-01, -5.8415e-02, -1.4766e+00, -9.8602e-01,  1.4866e+00,\n",
      "          1.4713e-01, -1.3660e+00, -6.7085e-01,  9.5211e-01,  1.4749e+00,\n",
      "         -1.4756e+00, -8.6603e-01,  1.2781e+00,  3.5260e-01, -7.5008e-02,\n",
      "          4.0587e-01,  5.3512e-01, -6.8785e-02, -6.1548e-01,  2.6958e-01,\n",
      "         -3.1602e-02, -1.2757e+00, -6.3726e-01, -7.6155e-01, -4.6703e-01,\n",
      "         -1.2028e+00, -2.4588e+00, -4.8989e-01, -1.5937e+00,  9.4815e-01,\n",
      "         -4.2648e-01, -1.4827e+00, -4.5045e-01,  8.8896e-01, -1.1526e+00,\n",
      "          2.9480e-02, -5.1994e-01, -1.6538e-01, -2.7733e-01, -2.4466e-01,\n",
      "         -1.9880e+00, -1.2664e+00, -3.0722e-01,  8.3983e-01, -4.6888e-01,\n",
      "          2.2659e-01,  3.4187e-01,  5.9337e-01,  1.9173e+00, -4.7865e-01,\n",
      "         -5.7824e-02, -1.7239e+00, -9.9090e-01,  1.9552e+00, -6.5327e-02,\n",
      "          1.4630e-01,  1.1357e+00, -2.6885e-01, -9.1267e-01,  6.8663e-01,\n",
      "          1.5644e+00,  1.0132e+00, -1.1486e+00, -7.9156e-01, -3.2136e-01,\n",
      "          5.4563e-01, -1.2671e+00,  5.7798e-01, -2.1042e-02, -1.3801e-01,\n",
      "          9.9371e-02, -1.6284e-01,  1.8978e-01, -1.2572e+00,  2.5705e-01,\n",
      "         -1.0626e+00, -6.3258e-01, -6.2932e-01, -1.6768e+00,  6.7240e-01,\n",
      "          1.9889e+00,  8.1569e-01, -1.4683e+00,  1.6630e+00, -1.4545e+00,\n",
      "         -2.3146e-01,  5.5500e-01,  3.2450e-01,  1.4937e+00,  5.8534e-01,\n",
      "          7.5996e-01, -1.0136e+00, -1.3920e+00,  8.8565e-01,  9.1616e-01,\n",
      "          4.8508e-01, -1.0356e+00,  1.6210e-01, -3.4564e-01,  7.7186e-01,\n",
      "          1.6738e-02,  6.8036e-01, -1.2983e-01,  9.7313e-02,  7.9569e-01,\n",
      "         -2.1607e+00, -5.6940e-01, -2.0023e+00, -1.2304e+00,  8.7704e-01,\n",
      "         -2.0921e+00,  1.5937e+00,  2.5637e+00, -1.2679e-01,  2.3143e-01,\n",
      "          7.9244e-01, -3.0765e-01,  6.7602e-01,  2.6806e+00, -8.7078e-01,\n",
      "          3.6106e-02,  1.0990e+00, -2.8002e-01,  5.3109e-01,  5.3204e-01,\n",
      "         -1.5853e+00,  2.4220e+00,  4.7723e-01,  5.9568e-01,  2.7927e-01,\n",
      "          2.3933e-01,  4.7382e-01,  3.1056e-02, -1.4894e-01, -3.6520e-01,\n",
      "         -1.8156e+00,  1.1129e+00,  1.1716e+00, -1.7179e+00,  1.0240e+00,\n",
      "         -1.0366e+00, -1.9978e+00,  1.5088e+00,  1.9653e-01,  1.0685e+00,\n",
      "          4.8509e-01,  5.9623e-03,  1.0007e+00,  7.0487e-01, -6.9784e-01,\n",
      "          4.7286e-01, -6.5665e-01, -8.6782e-01, -1.0432e-01,  9.7556e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "First tensor of positional_encoded:\n",
      "tensor([[-2.3908e+00,  1.3222e+00,  1.8754e+00,  2.1043e+00, -5.2238e-01,\n",
      "          2.5982e-01,  1.6236e-01,  7.6300e-01,  5.0993e-01,  2.6706e+00,\n",
      "          1.5921e+00,  5.8381e-01,  1.8619e+00, -7.7893e-02,  8.8486e-01,\n",
      "          1.6579e-01,  1.0301e+00,  1.3190e-01, -5.7016e-01,  1.3233e+00,\n",
      "          1.1285e+00, -2.1231e-01,  2.6024e+00,  9.0428e-01, -8.1148e-02,\n",
      "          2.2587e+00,  8.6913e-01,  3.9063e-02,  5.1823e-02,  6.7152e-01,\n",
      "         -2.2472e+00,  5.5210e-01,  4.2347e-01,  6.1254e-01, -2.2964e-01,\n",
      "          5.9291e-01,  8.7030e-01, -5.5281e-02, -1.3284e+00,  1.7061e+00,\n",
      "          3.5730e-01,  1.5893e+00,  9.1878e-01,  1.6663e+00,  2.4651e-01,\n",
      "          1.1329e+00,  1.2191e-01,  1.4781e+00,  2.7613e-01,  4.1043e-01,\n",
      "          5.6918e-01,  2.0890e-01, -1.9897e-01, -3.6157e-01, -5.1936e-01,\n",
      "          1.0765e+00,  3.4005e-01,  2.4557e+00, -3.4610e-01,  7.3662e-01,\n",
      "         -4.4770e-01,  2.7118e-01, -1.6066e-01,  6.7936e-01, -6.3077e-01,\n",
      "          2.1123e-01,  1.3062e+00,  7.2424e-02, -2.6274e-01,  1.9315e+00,\n",
      "         -4.5935e-01,  5.8054e-02, -7.0892e-01,  3.1861e+00, -6.4932e-01,\n",
      "          1.4521e+00,  8.5207e-01, -6.9467e-01,  1.1806e+00, -1.8929e+00,\n",
      "         -3.8758e-01,  2.8760e-01, -1.6171e+00,  6.4101e-01,  5.1367e-02,\n",
      "          1.6950e+00,  1.8352e+00, -9.1804e-01, -1.3924e+00,  1.5405e+00,\n",
      "          4.3507e-01, -1.2717e+00, -1.3386e-01,  9.4144e-01,  1.2574e-01,\n",
      "          4.4742e-01,  7.4480e-02,  8.5071e-01, -5.5225e-01,  9.0658e-01,\n",
      "         -1.0284e+00,  1.4044e+00,  2.1426e+00,  4.8463e-01,  1.0827e+00,\n",
      "          2.2499e+00,  9.8214e-01,  1.2269e+00,  4.9279e-01,  4.8717e-01,\n",
      "          3.0062e-01,  1.0773e+00,  6.4777e-01,  5.6758e-01,  1.1740e+00,\n",
      "          1.7011e+00,  6.6743e-01,  1.9640e-01, -1.3776e+00,  5.5895e-01,\n",
      "          1.4176e-01,  2.1085e+00,  5.5442e-01,  2.5818e+00, -1.2248e+00,\n",
      "          1.9629e+00, -1.5785e+00,  1.6716e+00, -6.0152e-02,  1.0698e+00,\n",
      "         -1.6635e+00,  2.3494e-01,  1.2306e+00,  1.4252e+00, -1.6383e-02,\n",
      "          8.9251e-01, -1.3086e+00,  1.6598e+00, -7.0325e-02,  1.2745e+00,\n",
      "         -3.4501e-01,  8.8038e-01,  1.1862e+00, -2.2032e-01,  2.9100e-01,\n",
      "          9.2036e-01,  1.3200e+00, -5.1969e-01, -2.9336e-01,  3.1066e+00,\n",
      "         -1.0875e-01,  1.6083e+00,  7.8943e-01,  1.7825e+00, -6.4659e-02,\n",
      "          9.9977e-01,  6.8309e-01,  1.1064e+00,  3.5032e-01,  1.1211e+00,\n",
      "          2.9843e-01,  2.3448e+00,  1.4614e+00,  2.0566e+00,  8.1554e-01,\n",
      "          1.7594e-01,  8.9328e-01,  6.1312e-01, -3.5718e-01, -1.5682e-01,\n",
      "         -1.7660e+00, -1.5380e+00,  9.6943e-02,  2.0879e-01,  3.7120e-01,\n",
      "          2.5118e+00, -8.9146e-01,  1.5247e+00,  3.5178e-01,  1.2491e+00,\n",
      "          1.1900e+00,  2.4109e+00,  7.9801e-01,  1.4941e+00, -1.8495e-01,\n",
      "         -3.8090e-02, -1.0130e-01,  7.2818e-02,  2.3484e-01,  1.0886e+00,\n",
      "         -3.4769e-01,  1.8491e+00,  2.0147e-01,  1.3840e+00,  1.2310e+00,\n",
      "          2.2287e+00,  7.0421e-01,  9.4372e-01, -1.4897e+00, -5.1946e-01,\n",
      "          3.2581e-01, -4.5843e-01,  1.8989e+00,  9.5943e-01, -2.9336e-01,\n",
      "          2.3978e+00, -9.1666e-01,  2.2063e-01, -4.1754e-01,  2.1060e+00,\n",
      "          2.5285e-01,  8.9246e-01,  7.7053e-01, -1.3043e-01,  9.9646e-01,\n",
      "         -1.8096e-01,  9.6260e-01, -1.0493e-01, -7.9095e-01,  7.8391e-01,\n",
      "          1.9485e-03,  7.9021e-01,  1.2010e+00,  1.6756e+00, -1.8900e+00,\n",
      "          1.1943e+00,  1.6020e+00, -3.7178e-02, -7.4869e-01,  6.1560e-01,\n",
      "          1.4350e-01,  9.1873e-01,  1.1262e+00,  1.0406e+00, -6.4643e-02,\n",
      "          4.4456e+00, -1.1129e+00,  5.6580e-01, -1.5212e-02,  1.5427e+00,\n",
      "          1.2508e-01,  1.2383e-01,  1.2223e+00,  1.3268e+00, -1.0487e-01,\n",
      "          3.4768e+00,  5.7691e-01,  1.1473e+00, -1.3136e+00,  3.9389e-01,\n",
      "          6.4498e-01,  7.5229e-01, -1.4078e+00,  9.1989e-01,  5.1941e-01,\n",
      "          2.1709e+00,  2.1780e+00,  2.7792e+00,  2.5832e-01, -1.4341e+00,\n",
      "         -3.4975e-01, -3.3806e-01, -4.3891e-01,  4.1498e-01,  1.8071e+00,\n",
      "          2.6738e-01,  4.0940e-01,  4.1591e-01,  1.0613e-01,  6.9329e-01,\n",
      "          8.6423e-01, -6.5866e-02, -1.0130e+00,  6.0819e-03,  2.9083e+00,\n",
      "          2.4483e+00, -5.6145e-01,  5.3544e-02, -7.4197e-01,  1.1556e+00,\n",
      "         -2.5844e-01,  2.4985e-01,  1.2355e+00,  2.0141e+00,  1.0132e+00,\n",
      "          1.6346e+00,  8.7688e-01,  1.8143e+00,  1.9737e-01,  3.6324e-01,\n",
      "         -8.7683e-01, -5.5098e-01, -7.8818e-01,  1.5684e+00,  7.6224e-01,\n",
      "          1.5569e+00,  1.2984e+00,  2.7561e+00,  2.1129e-01,  2.4860e+00],\n",
      "        [ 4.8053e-01,  6.4493e-01,  4.8195e-01,  9.0835e-01,  6.6379e-01,\n",
      "          7.3022e-01, -7.5410e-01,  1.1974e+00,  1.4579e+00,  4.8742e-01,\n",
      "          1.2530e+00, -1.1955e+00, -8.9550e-01,  5.9042e-01, -5.2095e-02,\n",
      "         -4.3604e-01, -6.7199e-01, -6.8107e-01,  3.8071e-03,  2.0799e+00,\n",
      "         -1.1061e+00, -4.6489e-02,  1.8840e+00,  1.7911e+00,  9.7260e-01,\n",
      "          4.7049e-02, -6.0944e-01,  1.4552e+00, -5.3847e-01,  1.9575e+00,\n",
      "         -7.4208e-01, -1.8788e+00,  1.6454e+00,  1.1507e+00,  6.6978e-01,\n",
      "          2.2577e+00, -5.2457e-01,  2.4697e-01,  1.0124e-01,  1.7074e-01,\n",
      "          9.7597e-01,  1.7410e+00, -8.3892e-01, -2.1440e+00, -7.3364e-01,\n",
      "          3.6444e-01, -4.7415e-01,  5.0307e-01,  7.7851e-01,  3.6288e+00,\n",
      "          1.2720e+00,  8.3007e-01, -6.9010e-01,  1.1733e+00,  2.1576e+00,\n",
      "          2.4154e-01, -6.8833e-01,  6.7759e-01, -3.6821e-01,  6.2832e-01,\n",
      "         -1.0821e+00, -5.3607e-01, -6.6738e-01,  2.1262e+00,  3.5897e-01,\n",
      "          1.4036e+00,  1.3760e-01,  4.2404e-01, -4.6724e-02,  6.8953e-01,\n",
      "         -1.1705e+00, -3.7307e-01,  6.3176e-02,  3.7904e-01,  1.3396e+00,\n",
      "          1.8033e-01,  1.2430e+00, -1.8343e-01,  5.4402e-02,  1.6677e+00,\n",
      "          1.0099e+00,  1.2660e+00,  7.0911e-01,  2.9013e-01, -7.8006e-01,\n",
      "          1.9012e+00, -4.9465e-01,  1.3816e+00, -7.1454e-01,  8.4681e-01,\n",
      "          4.8299e-01,  2.4039e+00, -7.6778e-01,  2.5590e+00, -1.0395e+00,\n",
      "          2.1839e+00,  1.2405e+00,  3.0542e+00,  7.0960e-01, -1.0906e-01,\n",
      "         -2.4556e-01,  1.4554e+00, -2.7106e-01,  5.8576e-01,  4.3566e-01,\n",
      "          2.1296e+00,  8.6444e-01,  1.9451e+00, -1.1817e-01, -6.0201e-01,\n",
      "         -2.5332e-02,  6.5505e-03,  1.1955e+00,  2.6089e+00, -2.6397e-01,\n",
      "          2.0815e+00,  9.1502e-01,  1.3845e-01, -2.4594e-01,  1.9801e+00,\n",
      "         -1.5022e-01,  8.7692e-01,  7.6503e-01,  1.3537e+00, -5.1237e-01,\n",
      "          1.6451e+00, -2.9682e+00,  1.1835e+00, -4.5305e-01,  4.0982e-02,\n",
      "         -1.4939e+00,  2.5066e+00, -9.2067e-01,  3.6134e-01,  2.3581e-01,\n",
      "          5.6062e-01, -1.2374e-01,  9.8119e-01,  1.6705e+00,  2.0660e+00,\n",
      "         -1.6830e-01, -2.3806e-01,  5.2700e-01,  8.4887e-01,  1.4976e-01,\n",
      "          2.2250e+00, -7.5295e-01,  1.9183e+00,  4.1640e-01,  1.2510e+00,\n",
      "          1.3815e-01,  8.0192e-01, -1.4686e+00,  4.0893e-01,  8.4459e-01,\n",
      "          7.7071e-01, -1.2321e+00,  1.2492e+00, -1.1337e+00,  1.7821e+00,\n",
      "          1.8174e-02,  1.3816e+00, -1.6458e+00,  6.1858e-01,  1.1350e-01,\n",
      "          8.9848e-01,  8.9133e-02,  1.7120e+00, -8.9484e-01,  1.8907e+00,\n",
      "          4.8196e-01,  1.6036e-01,  3.3829e-01, -2.5256e-01, -5.6976e-01,\n",
      "         -9.0593e-01, -9.6204e-01,  1.3677e+00, -5.7434e-01,  2.2373e+00,\n",
      "          8.7532e-01,  4.7723e-01,  1.2438e+00,  9.4226e-02,  7.7155e-01,\n",
      "          2.6222e+00,  8.4892e-02,  1.2028e+00,  3.3336e-01,  4.6623e-02,\n",
      "          1.5764e+00,  2.8697e+00, -1.0611e+00,  7.7274e-01,  2.5265e-01,\n",
      "          2.1618e+00, -1.1179e-01,  9.4370e-01,  8.5204e-01,  1.4009e-01,\n",
      "         -6.0841e-01,  2.0629e+00,  1.2242e+00,  1.7719e+00, -1.2778e+00,\n",
      "         -5.4335e-01, -6.0023e-01,  1.3214e+00, -5.8930e-02, -1.7037e-01,\n",
      "         -2.7720e+00,  9.7017e-01, -9.1512e-01,  1.4703e+00,  1.8792e+00,\n",
      "          1.5224e+00,  5.3075e-02,  1.4260e+00,  9.4875e-01,  1.4364e+00,\n",
      "         -2.0414e-01, -4.7393e-01,  5.0773e-01,  1.2779e+00,  1.3525e+00,\n",
      "          1.0504e-01, -1.5951e+00,  1.6737e+00, -9.9616e-01,  6.5193e-01,\n",
      "          2.1854e-01,  2.1278e+00, -1.4997e+00,  7.5952e-01, -4.8473e-01,\n",
      "          9.3383e-01, -9.0222e-01,  1.6440e+00,  7.5985e-01, -1.0203e+00,\n",
      "         -6.7332e-01,  8.0788e-02,  1.2126e+00, -3.4635e-01, -4.8260e-01,\n",
      "          2.7186e+00, -5.6791e-01, -1.9151e+00,  1.0839e+00,  1.0773e+00,\n",
      "          1.2321e+00,  3.9194e+00,  1.9382e+00,  4.4638e-01, -1.3026e+00,\n",
      "          2.0696e+00, -4.5580e-01,  2.3635e+00, -2.4216e+00,  9.1692e-01,\n",
      "          1.0383e-01,  1.1166e+00, -2.4899e-02,  1.3789e+00,  2.4486e-01,\n",
      "          1.0842e-01,  1.4347e+00, -9.2917e-01, -5.7112e-01,  3.3283e-01,\n",
      "         -9.1784e-02,  1.9549e+00,  1.8507e-01,  8.8323e-01, -2.2889e-01,\n",
      "          6.5515e-01, -1.0763e+00,  4.5223e-01, -3.2873e-01,  1.0584e+00,\n",
      "          2.1102e+00,  1.7726e+00, -3.4248e-01,  2.2370e+00, -2.4960e-01,\n",
      "          1.2240e+00, -6.8740e-01,  5.1016e-01,  3.9983e-01,  1.6982e+00,\n",
      "          5.2246e-02,  1.2882e+00,  5.9632e-02,  2.7286e+00,  2.9220e-01,\n",
      "          3.0741e-01, -8.4416e-01,  6.7080e-01, -1.1392e-01,  1.5478e-01],\n",
      "        [ 5.9140e-01, -7.9351e-01,  3.2127e+00, -6.3610e-01,  2.6103e-01,\n",
      "          1.0231e+00,  2.4313e+00, -4.0658e-01,  1.8979e+00,  6.4223e-01,\n",
      "          1.4748e-01,  6.2158e-03,  5.8895e-01,  1.6120e-01,  6.0063e-01,\n",
      "         -4.2784e-01, -4.1245e-02,  2.8455e-01, -1.3379e-01,  2.4616e-01,\n",
      "          3.1128e-01, -1.0868e+00, -1.1227e-01,  8.8528e-02,  8.0907e-01,\n",
      "          6.2036e-01,  4.2975e-01,  7.7894e-01, -4.0765e-01,  2.4781e+00,\n",
      "         -1.3774e+00,  4.7682e-02,  1.8234e+00, -2.1319e-02, -8.1889e-01,\n",
      "          8.4237e-01, -1.6797e-02,  4.7594e-02,  2.3897e+00,  1.7500e+00,\n",
      "          1.6828e-01,  1.4918e+00,  1.2851e+00,  2.0700e-01, -3.1356e+00,\n",
      "         -1.3176e+00,  7.3254e-01,  3.2377e-01,  1.8960e+00,  6.0057e-01,\n",
      "         -5.2539e-02, -5.9051e-01,  2.6911e+00,  2.5685e+00,  1.6898e+00,\n",
      "          1.6839e+00,  1.5979e+00,  1.7246e+00,  1.8800e+00,  3.3487e-01,\n",
      "         -2.3909e+00,  3.3931e-01,  1.4835e+00,  4.7570e-01,  2.5303e+00,\n",
      "          2.1327e+00,  1.1399e+00,  1.8577e-01,  1.6732e-01,  5.9708e-01,\n",
      "          5.9485e-01,  2.2293e+00,  9.5459e-02,  1.0773e+00,  6.7599e-01,\n",
      "          1.6628e+00,  7.0266e-01,  2.0108e-01,  8.4431e-01,  1.6391e+00,\n",
      "          2.2916e-01,  1.7736e+00, -9.2516e-01,  2.0390e+00, -2.3818e-01,\n",
      "          2.4640e+00, -2.8669e-02, -1.0985e+00,  9.3013e-01,  1.4872e+00,\n",
      "          7.2880e-01,  4.6941e-01,  1.1545e+00,  1.5248e+00, -2.0342e-01,\n",
      "          1.0148e+00,  5.0222e-02,  1.8289e-01, -1.6252e-01,  3.0255e-01,\n",
      "          1.6450e+00, -1.3130e+00, -2.1086e+00,  1.3164e+00,  8.5473e-01,\n",
      "          8.2997e-01,  6.5288e-02,  8.8417e-01, -6.1127e-01, -2.5402e-01,\n",
      "         -7.7157e-03,  1.3766e+00,  6.8432e-01,  9.0806e-01,  1.2700e+00,\n",
      "          1.8749e+00,  1.8880e+00,  3.8247e-01, -5.5389e-01, -1.0612e+00,\n",
      "          1.5791e+00,  1.3366e+00,  2.3876e-01,  1.1624e+00,  7.1543e-01,\n",
      "          5.8940e-01, -4.8845e-01,  1.2524e+00, -1.5971e-01,  1.6094e+00,\n",
      "         -1.4022e+00,  2.6614e+00,  3.9032e-01, -8.1260e-01,  4.9725e-01,\n",
      "          4.5146e-01, -1.0289e+00,  1.1735e+00,  4.1107e-01,  8.0380e-01,\n",
      "         -1.2414e-01,  1.6252e+00, -5.9634e-01, -8.7648e-02, -1.3012e+00,\n",
      "          1.3769e+00, -3.5804e-02, -4.7687e-01, -9.6475e-01,  2.4863e+00,\n",
      "          1.6713e-01, -3.6623e-01, -6.5205e-01,  1.9519e+00,  1.4926e+00,\n",
      "         -4.7579e-01, -8.4940e-01,  2.2780e+00,  3.6824e-01,  9.2487e-01,\n",
      "          4.2059e-01,  1.5350e+00, -5.4948e-02,  3.8442e-01,  2.8259e-01,\n",
      "          9.6831e-01, -1.2635e+00,  3.6267e-01, -7.5005e-01,  5.3290e-01,\n",
      "         -1.1920e+00, -1.4588e+00, -4.7971e-01, -5.9375e-01,  9.5772e-01,\n",
      "          5.7347e-01, -1.4737e+00,  5.4951e-01,  8.9743e-01, -1.5259e-01,\n",
      "          3.7442e-02,  4.8003e-01, -1.5789e-01,  7.2265e-01, -2.3761e-01,\n",
      "         -9.8805e-01, -1.2598e+00,  6.9276e-01,  8.4605e-01,  5.3110e-01,\n",
      "          2.3244e-01,  1.3419e+00,  5.9887e-01,  2.9173e+00, -4.7347e-01,\n",
      "          9.4216e-01, -1.7191e+00,  9.0854e-03,  1.9598e+00,  9.3466e-01,\n",
      "          1.5061e-01,  2.1357e+00, -2.6480e-01,  8.7318e-02,  6.9044e-01,\n",
      "          2.5644e+00,  1.0168e+00, -1.4866e-01, -7.8819e-01,  6.7864e-01,\n",
      "          5.4880e-01, -2.6713e-01,  5.8096e-01,  9.7895e-01, -1.3521e-01,\n",
      "          1.0994e+00, -1.6021e-01,  1.1898e+00, -1.2547e+00,  1.2571e+00,\n",
      "         -1.0602e+00,  3.6742e-01, -6.2713e-01, -6.7680e-01,  6.7446e-01,\n",
      "          2.9889e+00,  8.1763e-01, -4.6827e-01,  1.6649e+00, -4.5446e-01,\n",
      "         -2.2974e-01,  1.5550e+00,  3.2612e-01,  2.4937e+00,  5.8685e-01,\n",
      "          1.7600e+00, -1.0122e+00, -3.9197e-01,  8.8700e-01,  1.9162e+00,\n",
      "          4.8634e-01, -3.5639e-02,  1.6329e-01,  6.5436e-01,  7.7298e-01,\n",
      "          1.0167e+00,  6.8141e-01,  8.7017e-01,  9.8300e-02,  1.7957e+00,\n",
      "         -2.1598e+00,  4.3060e-01, -2.0014e+00, -2.3042e-01,  8.7786e-01,\n",
      "         -1.0922e+00,  1.5945e+00,  3.5637e+00, -1.2606e-01,  1.2314e+00,\n",
      "          7.9312e-01,  6.9235e-01,  6.7666e-01,  3.6806e+00, -8.7017e-01,\n",
      "          1.0361e+00,  1.0995e+00,  7.1998e-01,  5.3163e-01,  1.5320e+00,\n",
      "         -1.5848e+00,  3.4220e+00,  4.7770e-01,  1.5957e+00,  2.7971e-01,\n",
      "          1.2393e+00,  4.7424e-01,  1.0311e+00, -1.4854e-01,  6.3480e-01,\n",
      "         -1.8153e+00,  2.1129e+00,  1.1720e+00, -7.1795e-01,  1.0243e+00,\n",
      "         -3.6582e-02, -1.9975e+00,  2.5088e+00,  1.9682e-01,  2.0685e+00,\n",
      "          4.8536e-01,  1.0060e+00,  1.0009e+00,  1.7049e+00, -6.9760e-01,\n",
      "          1.4729e+00, -6.5643e-01,  1.3218e-01, -1.0410e-01,  1.9756e+00]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"First tensor of embedded_output:\\n{embedded_output[0]}\")\n",
    "print(f\"First tensor of positional_encoded:\\n{positional_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanism\n",
    "\n",
    "Now, let's dive into the core of the transformer: the attention mechanism.\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "Attention allows the model to focus on different parts of the input when producing each part of the output. In the context of transformers, we use self-attention, where the model attends to different parts of a single sequence.\n",
    "\n",
    "### Mathematical Representation of Scaled Dot-Product Attention\n",
    "\n",
    "Given query Q, key K, and value V matrices, the attention is computed as:\n",
    "\n",
    "$$Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "#### Example:\n",
    "Let's compute attention for a simple case with 2 words and 3-dimensional embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q is 2x3\n",
    "# Kt = torch.tensor([1, 2],\n",
    "#                   [2, 1],\n",
    "#                   [1, 0])\n",
    "# Kt is 3x2\n",
    "\n",
    "# QKt is (2x3)x(3x2) = (2x2)\n",
    "# V needs to be (2 x something)\n",
    "# V is 2x2\n",
    "\n",
    "# attention_weights is 2x2\n",
    "# attention_weights x V is (2x2)x(2x2) = (2x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.tensor([[1, 0, 1],  # Query for word 1\n",
    "                  [0, 1, 1]]) # Query for word 2\n",
    "K = torch.tensor([[1, 2, 1],  # Key for word 1\n",
    "                  [2, 1, 0]]) # Key for word 2\n",
    "V = torch.tensor([[0.5, 0.8],  # Value for word 1\n",
    "                  [0.2, 0.3]]) # Value for word 2\n",
    "\n",
    "d_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "QKt = torch.matmul(Q, K.transpose(0, 1))\n",
    "scaled = QKt / math.sqrt(d_k)\n",
    "attention_weights = torch.softmax(scaled, dim=-1)\n",
    "output = torch.matmul(attention_weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0351, 0.2595, 0.7054]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0351, 0.2595, 0.7054]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([[2, 4, 5]], dtype=torch.float32)\n",
    "result = torch.softmax(value, dim=1)\n",
    "print(result)\n",
    "\n",
    "# softmax(z) = e^z/sum(e^z))\n",
    "# e^value/sum(e^value)\n",
    "\n",
    "result_from_scratch = torch.exp(value)/torch.sum(torch.exp(value), dim=1, keepdim=True)\n",
    "result_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([[0.5000, 0.5000],\n",
      "        [0.7604, 0.2396]])\n",
      "Output: tensor([[0.3500, 0.5500],\n",
      "        [0.4281, 0.6802]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention weights:\", attention_weights)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how each word attends to both itself and the other word, with the attention weights determining how much information to gather from each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT 4o --> 200K context window\n",
    "# Claude --> 1M context window\n",
    "\n",
    "# Attention complexity is O(n^2) where n is the tokens\n",
    "# 12 tokens = O(144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"attention.gif\" alt=\"Attention\" style=\"width:60%; height: 60%;\">\n",
    "  <br>\n",
    "  <em>Figure 5: Attention in Action</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Scaled Dot-Product Attention\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "    - query: tensor of shape (..., seq_len_q, depth)\n",
    "    - key: tensor of shape (..., seq_len_k, depth)\n",
    "    - value: tensor of shape (..., seq_len_v, depth_v)\n",
    "    - mask: optional tensor of shape (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    Returns:\n",
    "    - output: weighted sum of values\n",
    "    - attention_weights: attention weights\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute dot product of query with keys\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    # Scale matmul_qk\n",
    "    depth = query.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(depth)\n",
    "\n",
    "    # Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "    # Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_k = 3, 300\n",
    "query = torch.rand(1, seq_len, d_k)\n",
    "key = torch.rand(1, seq_len, d_k)\n",
    "value = torch.rand(1, seq_len, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([1, 3, 300])\n",
      "Attention weights shape: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n",
    "Mathematically, multi-head attention first projects Q, K, and V h times (where h is the number of heads) with different learned projections. Then, it performs attention on each of these projected versions of Q, K, and V. Finally, the results are concatenated and once again projected.\n",
    "\n",
    "$$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n",
    "where \n",
    "$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"multi-head_attention.png\" alt=\"Multi-Head Attention\" style=\"width:30%; height: 30%;\">\n",
    "  <br>\n",
    "  <em>Figure 6: Multi-head Attention</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertviz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (4.40.2)\n",
      "Requirement already satisfied: torch>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (4.66.1)\n",
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (1.34.131)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (2.31.0)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (2023.12.25)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bertviz) (0.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.0->bertviz) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.131 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from boto3->bertviz) (1.34.131)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from boto3->bertviz) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bertviz) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bertviz) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bertviz) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bertviz) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.131->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.131->boto3->bertviz) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-876c047e73e94c0d8c05b20cb7fc0e0f\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "/**\n * @fileoverview Transformer Visualization D3 javascript code.\n *\n * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n *\n * Change log:\n *\n * 02/01/19  Jesse Vig   Initial implementation\n * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n * 01/19/21  Jesse Vig   Support light/dark modes\n * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n **/\n\nrequire.config({\n  paths: {\n      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n  }\n});\n\nrequirejs(['jquery', 'd3'], function($, d3) {\n\n        const params = {\"attention\": [{\"name\": null, \"attn\": [[[[0.39812859892845154, 0.08420449495315552, 0.11650005728006363, 0.08422822505235672, 0.3169385492801666], [0.4403926432132721, 0.16762524843215942, 0.09620590507984161, 0.09503644704818726, 0.2007397711277008], [0.5963823795318604, 0.041193846613168716, 0.04006786644458771, 0.0988212302327156, 0.2235347181558609], [0.41296133399009705, 0.1041448786854744, 0.20968465507030487, 0.03963122516870499, 0.2335779368877411], [0.4488774836063385, 0.11013173311948776, 0.15683729946613312, 0.08361057937145233, 0.20054291188716888]], [[0.31460797786712646, 0.12299809604883194, 0.2397768348455429, 0.11702374368906021, 0.20559333264827728], [0.2902417480945587, 0.20040622353553772, 0.02476537600159645, 0.27022963762283325, 0.21435701847076416], [0.573124349117279, 0.054388582706451416, 0.056727733463048935, 0.06762755662202835, 0.24813182651996613], [0.5156468749046326, 0.12912917137145996, 0.01698947884142399, 0.06441456079483032, 0.27381986379623413], [0.3842625021934509, 0.10317757725715637, 0.13557760417461395, 0.1464497298002243, 0.23053252696990967]], [[0.3254447281360626, 0.08374116569757462, 0.055840250104665756, 0.017976531758904457, 0.5169973373413086], [0.01971639133989811, 0.25149986147880554, 0.06887548416852951, 0.36328914761543274, 0.29661911725997925], [0.10034646093845367, 0.2524365484714508, 0.07627968490123749, 0.40147683024406433, 0.16946054995059967], [0.12452451139688492, 0.10800543427467346, 0.12755653262138367, 0.017785096541047096, 0.622128427028656], [0.11012458056211472, 0.0785360112786293, 0.12256093323230743, 0.009271989576518536, 0.6795064806938171]], [[0.17072542011737823, 0.20728741586208344, 0.2670135200023651, 0.1390228420495987, 0.21595075726509094], [0.26569864153862, 0.11868258565664291, 0.2862582206726074, 0.12897178530693054, 0.20038868486881256], [0.27244335412979126, 0.051699794828891754, 0.09819629043340683, 0.345843642950058, 0.23181688785552979], [0.18261635303497314, 0.3169862926006317, 0.32721152901649475, 0.046725835651159286, 0.126460000872612], [0.22701482474803925, 0.165755033493042, 0.18938010931015015, 0.21112999320030212, 0.20672005414962769]], [[0.007127091288566589, 0.0004379256279207766, 0.022014206275343895, 0.0029501740355044603, 0.967470645904541], [0.42274677753448486, 0.23312479257583618, 0.1443091332912445, 0.16343118250370026, 0.03638812527060509], [0.5971082448959351, 0.11923626065254211, 0.11636772751808167, 0.13917654752731323, 0.028111206367611885], [0.3852158784866333, 0.10283047705888748, 0.28131619095802307, 0.21924695372581482, 0.011390475556254387], [0.18065419793128967, 0.18708917498588562, 0.33702725172042847, 0.23839354515075684, 0.056835856288671494]], [[0.05867874622344971, 0.47381454706192017, 0.15331657230854034, 0.06746453791856766, 0.24672555923461914], [0.20623542368412018, 0.0965477004647255, 0.33416467905044556, 0.14792349934577942, 0.21512867510318756], [0.2161286622285843, 0.1436508595943451, 0.34384873509407043, 0.05622369796037674, 0.2401479333639145], [0.14739812910556793, 0.21334627270698547, 0.04442363977432251, 0.3486977815628052, 0.24613410234451294], [0.2947900891304016, 0.14755283296108246, 0.10241878777742386, 0.11510589718818665, 0.34013235569000244]], [[0.8633050322532654, 0.03192812576889992, 0.008997894823551178, 0.04920368641614914, 0.04656527191400528], [0.7927790880203247, 0.08273256570100784, 0.009029827080667019, 0.03354561701416969, 0.08191291242837906], [0.03163503482937813, 0.9443315863609314, 0.016988033428788185, 0.003679740708321333, 0.003365451702848077], [0.27703166007995605, 0.036264851689338684, 0.6388247013092041, 0.028226088732481003, 0.01965264044702053], [0.04520376771688461, 0.015583428554236889, 0.1824723482131958, 0.5941252112388611, 0.16261520981788635]], [[0.7450578808784485, 0.06915444880723953, 0.031729429960250854, 0.059112828224897385, 0.09494541585445404], [0.00011187259951839224, 0.0009886950720101595, 0.9944701194763184, 0.004424868617206812, 4.4117200559412595e-06], [3.651737279142253e-05, 2.4262426450150087e-05, 0.0023016552440822124, 0.9936510920524597, 0.003986537456512451], [0.03556719422340393, 8.44802925712429e-05, 5.064379365649074e-05, 0.008643175475299358, 0.955654501914978], [0.01336983684450388, 0.07693640887737274, 0.0037535002920776606, 0.005584923084825277, 0.900355339050293]], [[0.2002420276403427, 0.16572481393814087, 0.28649911284446716, 0.15019242465496063, 0.19734162092208862], [0.7298053503036499, 0.018269753083586693, 0.04821928218007088, 0.13828669488430023, 0.06541896611452103], [0.5700700283050537, 0.030928699299693108, 0.20050838589668274, 0.05358116701245308, 0.1449117809534073], [0.040217600762844086, 0.18296973407268524, 0.03718414157629013, 0.5333868861198425, 0.20624162256717682], [0.21183793246746063, 0.14378522336483002, 0.12473287433385849, 0.1878192126750946, 0.33182471990585327]], [[0.2088485062122345, 0.2012694925069809, 0.05920461192727089, 0.1820475459098816, 0.3486298620700836], [0.22177153825759888, 0.1256176084280014, 0.0901322215795517, 0.17582377791404724, 0.3866548538208008], [0.1988539844751358, 0.04397144541144371, 0.12674659490585327, 0.2628399133682251, 0.3675881326198578], [0.381658136844635, 0.031081940978765488, 0.07531754672527313, 0.22374005615711212, 0.2882022559642792], [0.20840221643447876, 0.3485042154788971, 0.087862528860569, 0.08325404673814774, 0.271977037191391]], [[0.2029639631509781, 0.2189667522907257, 0.12715792655944824, 0.1636291742324829, 0.2872821092605591], [0.1684870570898056, 0.10994210094213486, 0.26540133357048035, 0.16556482017040253, 0.29060474038124084], [0.21376854181289673, 0.12830539047718048, 0.16635122895240784, 0.2810375690460205, 0.21053734421730042], [0.13971641659736633, 0.22749584913253784, 0.29423457384109497, 0.08925501257181168, 0.24929821491241455], [0.1113729178905487, 0.25880110263824463, 0.17865127325057983, 0.16465938091278076, 0.28651532530784607]], [[0.9364169836044312, 0.01994284801185131, 0.013610798865556717, 0.009583884850144386, 0.02044546976685524], [0.002165835117921233, 0.4076341688632965, 0.005304507911205292, 0.5729205012321472, 0.011974970810115337], [0.002349077258259058, 0.47486892342567444, 0.004356088116765022, 0.5040968656539917, 0.014329034835100174], [0.006774035282433033, 0.5559632182121277, 0.01276898942887783, 0.39648696780204773, 0.028006788343191147], [0.002102751052007079, 0.25343459844589233, 0.005283553618937731, 0.7273691296577454, 0.011809980496764183]]], [[[0.86150723695755, 0.01102343201637268, 0.014990766532719135, 0.03403587266802788, 0.07844261825084686], [0.6944530010223389, 0.031413495540618896, 0.04775696247816086, 0.14440852403640747, 0.08196797966957092], [0.7493158578872681, 0.07719650864601135, 0.0038818384055048227, 0.043472446501255035, 0.12613333761692047], [0.7389408946037292, 0.03224112093448639, 0.04246461018919945, 0.02215440943837166, 0.16419893503189087], [0.8439189791679382, 0.022728195413947105, 0.014804515987634659, 0.0400216206908226, 0.07852669805288315]], [[0.7077445983886719, 0.02400830015540123, 0.018892033025622368, 0.03879658877849579, 0.2105584591627121], [0.5621011257171631, 0.18085189163684845, 0.002685880521312356, 0.00650617852807045, 0.24785494804382324], [0.7365278005599976, 0.003996603190898895, 0.025821594521403313, 0.0009590127738192677, 0.23269501328468323], [0.45564883947372437, 0.0063369558192789555, 0.00016128736024256796, 0.4432847201824188, 0.09456825256347656], [0.8759585022926331, 0.020197667181491852, 0.008991568349301815, 0.007737645413726568, 0.087114617228508]], [[0.9529193639755249, 0.0023617963306605816, 0.003966982942074537, 0.0016806138446554542, 0.03907116875052452], [0.5626932382583618, 0.03428930416703224, 0.07074037939310074, 0.05192645639181137, 0.28035062551498413], [0.7910499572753906, 0.02719038538634777, 0.04529357701539993, 0.018412968143820763, 0.11805307865142822], [0.8574643731117249, 0.0059982663951814175, 0.031218290328979492, 0.024318860843777657, 0.08100023120641708], [0.5586279034614563, 0.0503120981156826, 0.08547285944223404, 0.01948455348610878, 0.2861025333404541]], [[0.7353222370147705, 0.02658945508301258, 0.04014855995774269, 0.03962111845612526, 0.1583186686038971], [0.6679375171661377, 0.0478145070374012, 0.05363592877984047, 0.018511949107050896, 0.21210011839866638], [0.6446590423583984, 0.048745203763246536, 0.05967075750231743, 0.019706444814801216, 0.22721853852272034], [0.7960226535797119, 0.030814018100500107, 0.04308182746171951, 0.015407662838697433, 0.11467380076646805], [0.8526143431663513, 0.035002369433641434, 0.029010385274887085, 0.011327185668051243, 0.07204574346542358]], [[0.9083960652351379, 0.011465168558061123, 0.0038180307019501925, 0.030560893937945366, 0.04575992748141289], [0.8620033264160156, 0.019909989088773727, 0.048430223017930984, 0.03876077011227608, 0.030895618721842766], [0.7221620082855225, 0.0725288912653923, 0.021225206553936005, 0.12634778022766113, 0.05773613601922989], [0.7937374711036682, 0.048048168420791626, 0.04090385138988495, 0.02494475431740284, 0.0923658236861229], [0.7632732391357422, 0.021097427234053612, 0.013590339571237564, 0.07220132648944855, 0.12983763217926025]], [[0.28914931416511536, 0.04229902848601341, 0.14079710841178894, 0.09660840034484863, 0.43114620447158813], [0.6290506720542908, 0.0068943616934120655, 0.006515080574899912, 0.03228936716914177, 0.325250506401062], [0.8430335521697998, 0.008831587620079517, 0.006179891061037779, 0.028032947331666946, 0.11392197757959366], [0.9410265684127808, 0.002805168041959405, 0.009953552857041359, 0.016557645052671432, 0.02965717576444149], [0.9746230840682983, 0.004118828102946281, 0.003780944971367717, 0.005120345391333103, 0.012356755323708057]], [[0.9547500014305115, 0.005879567936062813, 0.003733431687578559, 0.010251244530081749, 0.025385819375514984], [0.47382888197898865, 0.06807529181241989, 0.258858859539032, 0.07540558278560638, 0.12383141368627548], [0.8084253668785095, 0.05096011981368065, 0.018038002774119377, 0.059371914714574814, 0.06320463120937347], [0.7608255743980408, 0.02482014335691929, 0.01381296943873167, 0.12980139255523682, 0.07073986530303955], [0.8701735734939575, 0.023083331063389778, 0.015645232051610947, 0.02752526104450226, 0.06357264518737793]], [[0.2037992924451828, 0.14572791755199432, 0.2542632818222046, 0.1888437271118164, 0.20736581087112427], [0.002494131913408637, 0.2383488565683365, 0.3242523670196533, 0.16116361320018768, 0.2737410366535187], [0.0040359534323215485, 0.36484917998313904, 0.18605759739875793, 0.11727394908666611, 0.32778334617614746], [0.00313374400138855, 0.20493611693382263, 0.22308672964572906, 0.40157777070999146, 0.16726557910442352], [0.006196911912411451, 0.17095044255256653, 0.2527135908603668, 0.15223728120326996, 0.4179018437862396]], [[0.9823846220970154, 0.0029734442941844463, 0.004152062349021435, 0.003237162483856082, 0.007252753246575594], [0.0996360257267952, 0.022524023428559303, 0.42134809494018555, 0.22540044784545898, 0.23109149932861328], [0.41717028617858887, 0.004102666862308979, 0.014848969876766205, 0.41856005787849426, 0.14531804621219635], [0.8074283599853516, 0.001688208314590156, 0.00481018703430891, 0.030125577002763748, 0.15594781935214996], [0.9383941292762756, 0.0026325329672545195, 0.003691755933687091, 0.009337672032415867, 0.045943956822156906]], [[0.9160395264625549, 0.005021241959184408, 0.011203797534108162, 0.018694588914513588, 0.04904096946120262], [0.447532594203949, 0.031231697648763657, 0.1455901861190796, 0.09844273328781128, 0.27720287442207336], [0.6177771091461182, 0.0458185039460659, 0.14309675991535187, 0.024247702211141586, 0.1690598875284195], [0.633057177066803, 0.008272858336567879, 0.09402398020029068, 0.13858330249786377, 0.1260627806186676], [0.751583456993103, 0.011447388678789139, 0.060940876603126526, 0.02801932580769062, 0.14800891280174255]], [[0.9154782891273499, 0.0028852964751422405, 0.004817711655050516, 0.0034006875939667225, 0.07341807335615158], [0.6163118481636047, 0.14956028759479523, 0.1351833939552307, 0.028579620644450188, 0.07036478072404861], [0.7628480195999146, 0.04206465557217598, 0.09178899973630905, 0.007779253181070089, 0.09551910310983658], [0.03062719851732254, 0.007890685461461544, 0.02078201435506344, 0.7110999822616577, 0.22960017621517181], [0.9603108167648315, 0.0026847990229725838, 0.004611091688275337, 0.0018068328499794006, 0.030586441978812218]], [[0.6835585236549377, 0.02388257533311844, 0.03832613304257393, 0.06524277478456497, 0.1889900267124176], [0.5690737962722778, 0.15501995384693146, 0.03527103736996651, 0.007736000698059797, 0.23289917409420013], [0.8473697304725647, 0.009806645102798939, 0.050711460411548615, 0.011547318659722805, 0.08056482672691345], [0.4483983814716339, 0.004906094167381525, 0.003510307753458619, 0.43067872524261475, 0.1125064417719841], [0.7953333258628845, 0.018294425681233406, 0.013681027106940746, 0.0218789204955101, 0.1508122980594635]]], [[[0.8075618743896484, 0.026632875204086304, 0.044284649193286896, 0.0109436996281147, 0.11057695746421814], [0.8021371960639954, 0.037128694355487823, 0.012031815946102142, 0.01649373024702072, 0.13220858573913574], [0.6923310160636902, 0.10287822037935257, 0.03181419521570206, 0.002504308009520173, 0.17047229409217834], [0.6166287064552307, 0.08844469487667084, 0.04454261437058449, 0.09977316111326218, 0.15061089396476746], [0.7983595132827759, 0.0253902617841959, 0.06951236724853516, 0.038459472358226776, 0.06827837973833084]], [[0.9035001993179321, 0.020519819110631943, 0.02002490684390068, 0.005448790267109871, 0.05050618201494217], [0.49516692757606506, 0.09770744293928146, 0.22910617291927338, 0.037163667380809784, 0.1408557891845703], [0.455806702375412, 0.047122035175561905, 0.17663472890853882, 0.26586443185806274, 0.05457214638590813], [0.8415323495864868, 0.02742885798215866, 0.010543317534029484, 0.04245510324835777, 0.07804042845964432], [0.38818374276161194, 0.22453561425209045, 0.11272507905960083, 0.025711216032505035, 0.24884437024593353]], [[0.34633827209472656, 0.06296642869710922, 0.39470013976097107, 0.12611404061317444, 0.06988117098808289], [0.9990118741989136, 0.0009754291386343539, 1.1318454795627986e-07, 5.1881674067999484e-08, 1.248319949809229e-05], [0.0001931708975462243, 0.9997119307518005, 9.481162123847753e-05, 1.91640814328764e-09, 3.0159124975170926e-08], [0.00012071050150552765, 0.0012188144028186798, 0.9981385469436646, 0.0005125118186697364, 9.371639862365555e-06], [0.00030583751504309475, 1.6082004776762915e-06, 0.0024111117236316204, 0.9894910454750061, 0.007790431380271912]], [[0.8520419001579285, 0.030518321320414543, 0.025304516777396202, 0.003216496901586652, 0.08891880512237549], [0.7285164594650269, 0.06120254471898079, 0.05285516381263733, 0.008927748538553715, 0.148498073220253], [0.7898738980293274, 0.04528340324759483, 0.051126811653375626, 0.0029034509789198637, 0.11081250011920929], [0.830098032951355, 0.020199693739414215, 0.00649636285379529, 0.034415628761053085, 0.10879037529230118], [0.7540895938873291, 0.04212600365281105, 0.02824133075773716, 0.009834778495132923, 0.16570822894573212]], [[0.8033361434936523, 0.02309497259557247, 0.01826191321015358, 0.025658147409558296, 0.12964873015880585], [0.6268267035484314, 0.03916175290942192, 0.054781459271907806, 0.13135841488838196, 0.14787165820598602], [0.5098944902420044, 0.06985585391521454, 0.06521427631378174, 0.18045498430728912, 0.1745804399251938], [0.4348451495170593, 0.030069101601839066, 0.08076012879610062, 0.18751351535320282, 0.2668120861053467], [0.5859362483024597, 0.06645137071609497, 0.08959513902664185, 0.10769737511873245, 0.15031979978084564]], [[0.29076841473579407, 0.12563319504261017, 0.1499631106853485, 0.16848056018352509, 0.26515480875968933], [0.18433333933353424, 0.5482984781265259, 0.008978798054158688, 0.011466729454696178, 0.24692268669605255], [0.06115800887346268, 0.011267040856182575, 0.8710530996322632, 0.0011347219115123153, 0.05538712441921234], [0.16254574060440063, 0.008799382485449314, 0.004012760706245899, 0.7406584024429321, 0.08398373425006866], [0.2988787591457367, 0.23354202508926392, 0.10419581830501556, 0.053088150918483734, 0.3102951943874359]], [[0.8325456976890564, 0.03475615009665489, 0.051183443516492844, 0.020574461668729782, 0.0609402060508728], [0.31308820843696594, 0.05082246661186218, 0.14133650064468384, 0.22994005680084229, 0.26481273770332336], [0.4812456965446472, 0.03950098156929016, 0.02189824730157852, 0.27636393904685974, 0.18099112808704376], [0.49746960401535034, 0.15931034088134766, 0.22032402455806732, 0.03933608531951904, 0.08355993032455444], [0.3457396924495697, 0.10277792811393738, 0.17550118267536163, 0.1227261945605278, 0.2532549798488617]], [[0.9390066266059875, 0.029564373195171356, 0.007064778357744217, 0.008065911009907722, 0.016298381611704826], [0.011506844311952591, 0.0035696669947355986, 0.9727144837379456, 0.012195384129881859, 1.3556085832533427e-05], [0.08569967746734619, 0.0004936086479574442, 0.020917309448122978, 0.8883546590805054, 0.004534697625786066], [0.48543262481689453, 0.0004957573255524039, 6.836490501882508e-05, 0.025802258402109146, 0.4882010221481323], [0.7268528938293457, 0.029593439772725105, 0.0024036383256316185, 0.008683495223522186, 0.23246650397777557]], [[0.7625669240951538, 0.02117355726659298, 0.021049052476882935, 0.006135161966085434, 0.18907535076141357], [0.6855987906455994, 0.019688250496983528, 0.061575066298246384, 0.022705378010869026, 0.21043246984481812], [0.6967968940734863, 0.11961257457733154, 0.011808054521679878, 0.019223079085350037, 0.15255936980247498], [0.5582551956176758, 0.0258041899651289, 0.03259358927607536, 0.1291818767786026, 0.2541651725769043], [0.7299277186393738, 0.06106313690543175, 0.024328097701072693, 0.007858549244701862, 0.17682254314422607]], [[0.9744979739189148, 0.006341868080198765, 0.005172171164304018, 0.001526273088529706, 0.012461760081350803], [0.9593694806098938, 0.02276206575334072, 0.008516451343894005, 0.0018446629401296377, 0.007507213857024908], [0.7484623789787292, 0.17058497667312622, 0.04757721722126007, 0.010533315129578114, 0.022842174395918846], [0.5395881533622742, 0.2045271098613739, 0.16165322065353394, 0.0396621972322464, 0.05456935986876488], [0.6883262991905212, 0.07283958792686462, 0.10437922179698944, 0.04363137483596802, 0.09082350879907608]], [[0.8561639785766602, 0.045360881835222244, 0.011380121111869812, 0.0022378568537533283, 0.08485730737447739], [0.5250884890556335, 0.1179773136973381, 0.07048165798187256, 0.010215544141829014, 0.27623701095581055], [0.5471076369285583, 0.10903751850128174, 0.04410327598452568, 0.014452080242335796, 0.2852994501590729], [0.6509661674499512, 0.01319735124707222, 0.05969717353582382, 0.012165619060397148, 0.2639737129211426], [0.5995981097221375, 0.06396745145320892, 0.06310825049877167, 0.023234877735376358, 0.2500913441181183]], [[0.11330137401819229, 0.17999345064163208, 0.17339052259922028, 0.14298976957798004, 0.3903248906135559], [0.6392049789428711, 0.04597223177552223, 0.27429357171058655, 0.02369813248515129, 0.016831060871481895], [0.026937924325466156, 0.052267882972955704, 0.08235551416873932, 0.18130075931549072, 0.657137930393219], [0.01304722111672163, 0.005187025759369135, 0.010086419992148876, 0.04337592050433159, 0.9283033609390259], [0.47385305166244507, 0.10860748589038849, 0.19168438017368317, 0.1770186573266983, 0.048836492002010345]]], [[[0.0925428718328476, 0.2859051525592804, 0.30513399839401245, 0.20095665752887726, 0.11546136438846588], [0.8028484582901001, 0.02718830481171608, 0.042514801025390625, 0.008172291330993176, 0.11927612125873566], [0.6678282022476196, 0.05724462494254112, 0.16610807180404663, 0.010260804556310177, 0.09855834394693375], [0.4908193349838257, 0.008182908408343792, 0.018631940707564354, 0.29275164008140564, 0.18961425125598907], [0.747965931892395, 0.036552805453538895, 0.06164140626788139, 0.022350259125232697, 0.1314895898103714]], [[0.7077264785766602, 0.0743335410952568, 0.07119686156511307, 0.0687444657087326, 0.07799869030714035], [0.9775446653366089, 0.019378740340471268, 0.001510517206043005, 0.0006065372144803405, 0.0009595339652150869], [0.9231095910072327, 0.06532277166843414, 0.008072016760706902, 0.0025668221060186625, 0.0009288052096962929], [0.9333112239837646, 0.012877636589109898, 0.02879805862903595, 0.018004508689045906, 0.007008570712059736], [0.8360661864280701, 0.0023944159038364887, 0.005477460566908121, 0.026815224438905716, 0.12924663722515106]], [[0.49203792214393616, 0.11513146013021469, 0.06997351348400116, 0.02745373547077179, 0.2954033315181732], [0.19588321447372437, 0.026525577530264854, 0.7173383235931396, 0.05147303268313408, 0.008779876865446568], [0.9243248701095581, 0.04720456898212433, 0.007063549477607012, 0.005115954205393791, 0.016291102394461632], [0.9083707928657532, 0.01765601709485054, 0.020900987088680267, 0.03219173476099968, 0.020880376920104027], [0.7030029296875, 0.0616723969578743, 0.09650243818759918, 0.028558604419231415, 0.11026358604431152]], [[0.3861979842185974, 0.12171842902898788, 0.09215591102838516, 0.10663807392120361, 0.29328957200050354], [0.3890783190727234, 0.019186679273843765, 0.16948345303535461, 0.3047948181629181, 0.11745668202638626], [0.5565298795700073, 0.02464941516518593, 0.09101436287164688, 0.1819162517786026, 0.14589011669158936], [0.7707909941673279, 0.03662744536995888, 0.06484215706586838, 0.02816537581384182, 0.0995739996433258], [0.39163291454315186, 0.08133019506931305, 0.12271194905042648, 0.07352811843156815, 0.3307967782020569]], [[0.3198782503604889, 0.16563981771469116, 0.14586742222309113, 0.16373451054096222, 0.20488007366657257], [0.6100217700004578, 0.32574573159217834, 0.0006912790704518557, 1.2755108400597237e-05, 0.06352841854095459], [0.8999199867248535, 0.00036874235956929624, 0.06393839418888092, 1.0344958354835398e-05, 0.03576260432600975], [0.3604011833667755, 4.929097030981211e-06, 5.914606390433619e-06, 0.5481065511703491, 0.09148142486810684], [0.7518201470375061, 0.03668734431266785, 0.008064660243690014, 0.007583109196275473, 0.19584472477436066]], [[0.3021949529647827, 0.09507489949464798, 0.23806802928447723, 0.11573202908039093, 0.24893005192279816], [0.7431187629699707, 0.06196478381752968, 0.02244926430284977, 0.0037439996376633644, 0.16872312128543854], [0.8468388915061951, 0.02058425359427929, 0.048913586884737015, 0.00460232375189662, 0.07906091213226318], [0.8718322515487671, 0.012226397171616554, 0.008680804632604122, 0.031047632917761803, 0.07621288299560547], [0.7017644643783569, 0.01070666778832674, 0.01799548976123333, 0.015202352777123451, 0.25433099269866943]], [[0.5663278698921204, 0.09278355538845062, 0.04392465576529503, 0.02765728533267975, 0.26930662989616394], [0.0002626515051815659, 0.0008943877182900906, 0.9987227320671082, 0.000120071395940613, 1.5444379641849082e-07], [0.6199795007705688, 2.9605431336676702e-05, 0.0004284161841496825, 0.3793579638004303, 0.00020444294204935431], [0.6655687689781189, 4.809102392755449e-06, 0.0002140963333658874, 0.006332992576062679, 0.32787930965423584], [0.2643803358078003, 2.243432618342922e-06, 2.501665676390985e-06, 0.000282155757304281, 0.7353327870368958]], [[0.4574976861476898, 0.07283816486597061, 0.14016039669513702, 0.24340569972991943, 0.08609800040721893], [0.1430225819349289, 0.013570088893175125, 0.7604113221168518, 0.07277479022741318, 0.010221215896308422], [0.9273419976234436, 0.0009463069145567715, 0.016362376511096954, 0.04603741690516472, 0.009311867877840996], [0.8151144981384277, 0.0013239088002592325, 0.023439396172761917, 0.0732715055346489, 0.08685075491666794], [0.9228160381317139, 0.003958503715693951, 0.01059409137815237, 0.020337410271167755, 0.04229382798075676]], [[0.5178878307342529, 0.0880548357963562, 0.09258411824703217, 0.14640218019485474, 0.15507103502750397], [0.9734624028205872, 0.012794425711035728, 0.00833885744214058, 0.0018870700150728226, 0.003517386969178915], [0.8052858710289001, 0.13009439408779144, 0.04481378197669983, 0.010313602164387703, 0.009492459706962109], [0.538370668888092, 0.20390883088111877, 0.1695791631937027, 0.06553962081670761, 0.022601693868637085], [0.6421404480934143, 0.030685199424624443, 0.08521915227174759, 0.08145050704479218, 0.1605048030614853]], [[0.44668686389923096, 0.12103648483753204, 0.11768874526023865, 0.09759735316038132, 0.21699059009552002], [0.8428013920783997, 0.10313965380191803, 0.019941043108701706, 0.0038658929988741875, 0.030252011492848396], [0.8610410690307617, 0.060581181198358536, 0.04533598944544792, 0.013889104127883911, 0.019152792170643806], [0.5497962832450867, 0.0318324975669384, 0.1773797869682312, 0.1617916077375412, 0.07919981330633163], [0.6312271356582642, 0.0721793919801712, 0.08998243510723114, 0.060259707272052765, 0.14635133743286133]], [[0.5338233113288879, 0.09261452406644821, 0.15207819640636444, 0.08529040217399597, 0.13619357347488403], [0.17068898677825928, 0.03173670545220375, 0.37393054366111755, 0.22667193412780762, 0.1969718188047409], [0.6233339905738831, 0.005836189724504948, 0.15927383303642273, 0.0799810141324997, 0.13157501816749573], [0.6919676661491394, 0.00493792025372386, 0.04715385660529137, 0.07754284888505936, 0.17839767038822174], [0.8796846866607666, 0.002689806278795004, 0.0078012775629758835, 0.013742766343057156, 0.09608148783445358]], [[0.18229836225509644, 0.21930232644081116, 0.12189510464668274, 0.17995306849479675, 0.2965511083602905], [0.6484827995300293, 0.04602257162332535, 0.021311338990926743, 0.1612555831670761, 0.12292777746915817], [0.8936607241630554, 0.011244529858231544, 0.0029962114058434963, 0.037577059119939804, 0.05452145263552666], [0.8488944172859192, 0.012210114859044552, 0.017448939383029938, 0.06632887572050095, 0.0551176443696022], [0.6788195967674255, 0.037631839513778687, 0.05830004811286926, 0.051986902952194214, 0.1732616275548935]]], [[[0.40959492325782776, 0.05429363250732422, 0.06378146260976791, 0.07043003290891647, 0.40189996361732483], [0.06353994458913803, 0.0852838084101677, 0.5550304055213928, 0.2263764888048172, 0.06976930797100067], [0.12850846350193024, 0.09997352957725525, 0.32607924938201904, 0.30110228061676025, 0.14433641731739044], [0.23967184126377106, 0.06565079092979431, 0.25642046332359314, 0.17081227898597717, 0.2674446702003479], [0.39771661162376404, 0.061940908432006836, 0.07018207758665085, 0.07837354391813278, 0.3917868137359619]], [[0.3793768584728241, 0.06726551055908203, 0.09567862749099731, 0.07668773829936981, 0.38099128007888794], [0.30586764216423035, 0.11535905301570892, 0.16405488550662994, 0.10858040302991867, 0.3061380386352539], [0.1886037439107895, 0.1870415359735489, 0.30709365010261536, 0.12686456739902496, 0.1903965026140213], [0.35087719559669495, 0.07567872852087021, 0.15041576325893402, 0.08090623468160629, 0.34212207794189453], [0.3608587980270386, 0.07908087968826294, 0.10823724418878555, 0.08835232257843018, 0.3634707033634186]], [[0.4886612892150879, 0.008006134070456028, 0.0029040807858109474, 0.012289500795304775, 0.48813894391059875], [0.433368444442749, 0.044050298631191254, 0.028878523036837578, 0.007748489733785391, 0.48595428466796875], [0.3004518449306488, 0.32341888546943665, 0.04322309046983719, 0.006771002430468798, 0.32613521814346313], [0.4460737705230713, 0.016877474263310432, 0.012701797299087048, 0.02491573803126812, 0.4994312524795532], [0.4876900315284729, 0.008497788570821285, 0.003373831044882536, 0.012318278662860394, 0.48812007904052734]], [[0.5007402300834656, 0.002595712197944522, 0.005970080383121967, 0.003695453517138958, 0.4869985282421112], [0.37734901905059814, 0.13579590618610382, 0.08404601365327835, 0.008508913218975067, 0.3943001925945282], [0.4038362503051758, 0.058842964470386505, 0.10570286214351654, 0.008054489269852638, 0.4235633909702301], [0.47122758626937866, 0.0014408049173653126, 0.014034135267138481, 0.06164379045367241, 0.4516536295413971], [0.4993554949760437, 0.002967272186651826, 0.006737590301781893, 0.004159390926361084, 0.48678022623062134]], [[0.4949313998222351, 0.0016966513358056545, 0.008139267563819885, 0.004542473237961531, 0.49069008231163025], [0.10068279504776001, 0.009597127325832844, 0.7739620208740234, 0.005206095054745674, 0.11055205017328262], [0.425558477640152, 0.0029018463101238012, 0.010415924713015556, 0.06791756302118301, 0.4932061433792114], [0.4403194487094879, 0.0002828071592375636, 0.0017013021279126406, 0.04078410193324089, 0.5169122815132141], [0.4935864210128784, 0.001754936994984746, 0.009219015948474407, 0.004715397953987122, 0.4907241761684418]], [[0.474977046251297, 0.0015133244451135397, 0.0009883801685646176, 0.00028629909502342343, 0.5222349166870117], [0.3982202112674713, 0.09551651030778885, 0.09958402812480927, 0.001591401407495141, 0.40508776903152466], [0.47177740931510925, 0.005325749050825834, 0.012916004285216331, 0.0003645068791229278, 0.5096163153648376], [0.45494410395622253, 0.015348898246884346, 0.005266724154353142, 0.07138411700725555, 0.4530561566352844], [0.47447243332862854, 0.0016963473754003644, 0.0011841662926599383, 0.0003209903370589018, 0.5223259925842285]], [[0.4998774230480194, 0.005190551280975342, 0.0021379285026341677, 0.0034405856858938932, 0.48935356736183167], [0.4189440608024597, 0.06876073032617569, 0.03702399879693985, 0.010337965562939644, 0.46493327617645264], [0.33884918689727783, 0.13039980828762054, 0.10284601897001266, 0.04493069648742676, 0.3829742670059204], [0.11683087795972824, 0.03621778264641762, 0.6081046462059021, 0.10325279831886292, 0.13559392094612122], [0.49777188897132874, 0.005736065562814474, 0.0024068010970950127, 0.003845043247565627, 0.490240216255188]], [[0.4812639653682709, 0.0030788027215749025, 0.004553120583295822, 0.0048520672135055065, 0.5062519907951355], [0.37581685185432434, 0.09865214675664902, 0.10034596174955368, 0.015404261648654938, 0.4097808003425598], [0.3436965346336365, 0.10171371698379517, 0.16281311213970184, 0.021135136485099792, 0.3706414997577667], [0.400690495967865, 0.059875715523958206, 0.09169015288352966, 0.025139067322015762, 0.42260459065437317], [0.4810774028301239, 0.0031639691442251205, 0.004619162064045668, 0.004814601968973875, 0.5063248872756958]], [[0.5032819509506226, 0.0020475832279771566, 0.0006230121944099665, 0.0014712836127728224, 0.49257612228393555], [0.12062101066112518, 0.034975528717041016, 0.6429721117019653, 0.06818945705890656, 0.13324196636676788], [0.36317744851112366, 0.051622577011585236, 0.02145581692457199, 0.1556985080242157, 0.4080456495285034], [0.4650093913078308, 0.002287032315507531, 0.0006076024146750569, 0.02962421253323555, 0.5024717450141907], [0.5020348429679871, 0.0022230257745832205, 0.000737378082703799, 0.0016209266614168882, 0.4933837950229645]], [[0.443921834230423, 0.01785440184175968, 0.01694926992058754, 0.0710066556930542, 0.4502679109573364], [0.37562865018844604, 0.18100129067897797, 0.0022831889800727367, 3.1294382552005118e-06, 0.4410836696624756], [0.3824291229248047, 0.00153095624409616, 0.17764459550380707, 6.106456567067653e-06, 0.43838921189308167], [0.17730757594108582, 5.439510459837038e-07, 1.9611832158261677e-06, 0.6211252212524414, 0.20156466960906982], [0.44000154733657837, 0.01891253888607025, 0.01759394444525242, 0.0758637934923172, 0.4476282000541687]], [[0.4982190728187561, 0.0006397091201506555, 0.0006178262992762029, 0.002741475123912096, 0.49778199195861816], [0.027724890038371086, 0.023010872304439545, 0.8792904615402222, 0.038424052298069, 0.031549710780382156], [0.3858417570590973, 0.00703799445182085, 0.07740623503923416, 0.08003607392311096, 0.4496779441833496], [0.39817678928375244, 0.004763432312756777, 0.03593549132347107, 0.08799103647470474, 0.47313329577445984], [0.4968620836734772, 0.0006671541486866772, 0.000651753565762192, 0.0028595817275345325, 0.4989594519138336]], [[0.48733609914779663, 0.00026300703757442534, 0.0007382174953818321, 0.0023367879912257195, 0.5093259215354919], [0.4415494203567505, 0.06136578321456909, 0.026715964078903198, 0.03578482195734978, 0.4345840513706207], [0.4670204222202301, 0.03386926278471947, 0.01814279332756996, 0.01435072161257267, 0.46661683917045593], [0.3937506675720215, 0.12634851038455963, 0.01825418882071972, 0.06028754636645317, 0.40135908126831055], [0.4879203140735626, 0.0002553584345150739, 0.0006770117324776947, 0.002141436794772744, 0.5090058445930481]]], [[[0.4817788600921631, 0.014009306207299232, 0.004834875930100679, 0.017202207818627357, 0.4821748435497284], [0.4426496624946594, 0.026294423267245293, 0.01909424550831318, 0.07196718454360962, 0.4399943947792053], [0.4668445885181427, 0.013169193640351295, 0.0091101648285985, 0.04883608967065811, 0.4620400071144104], [0.4304269552230835, 0.052272990345954895, 0.009001723490655422, 0.07808775454759598, 0.430210679769516], [0.48184987902641296, 0.013958467170596123, 0.004790756851434708, 0.0171622596681118, 0.4822385907173157]], [[0.4961519241333008, 0.0014797230251133442, 0.004726368002593517, 0.0012834967346861959, 0.4963585138320923], [0.468368798494339, 0.056764867156744, 0.0021079499274492264, 3.2063881008070894e-06, 0.47275519371032715], [0.44463786482810974, 0.0004175677604507655, 0.11075666546821594, 4.783437543665059e-05, 0.44414013624191284], [0.4650593101978302, 1.8835545745332638e-07, 1.1074494068452623e-05, 0.06908060610294342, 0.46584877371788025], [0.496097594499588, 0.0015011490322649479, 0.004767234902828932, 0.0012910603545606136, 0.4963429570198059]], [[0.4921876788139343, 0.0028378849383443594, 0.0035123920533806086, 0.007491580676287413, 0.49397045373916626], [0.14827069640159607, 0.05229279398918152, 0.5104207396507263, 0.1415422111749649, 0.14747360348701477], [0.1226036474108696, 0.05132735148072243, 0.262734979391098, 0.44090163707733154, 0.12243235856294632], [0.4240064024925232, 0.006534110754728317, 0.06730727851390839, 0.07831960916519165, 0.42383256554603577], [0.4922385513782501, 0.002814197214320302, 0.0034874947741627693, 0.007435672450810671, 0.4940240979194641]], [[0.49330756068229675, 0.00497034378349781, 0.005492065101861954, 0.003942323848605156, 0.4922877252101898], [0.2367422729730606, 0.07611255347728729, 0.19197940826416016, 0.25902748107910156, 0.2361382693052292], [0.34184718132019043, 0.053303319960832596, 0.09109252691268921, 0.17190556228160858, 0.3418514132499695], [0.4478435516357422, 0.020028453320264816, 0.046622417867183685, 0.038136810064315796, 0.4473687410354614], [0.4932989478111267, 0.004983332473784685, 0.005496129393577576, 0.003945962060242891, 0.492275595664978]], [[0.47729745507240295, 0.012674051336944103, 0.01583195850253105, 0.01453222893178463, 0.4796643555164337], [0.4152601361274719, 0.09185740351676941, 0.03576567769050598, 0.041895791888237, 0.4152209162712097], [0.40288662910461426, 0.044344350695610046, 0.10087300091981888, 0.047698114067316055, 0.40419796109199524], [0.41655460000038147, 0.02073313668370247, 0.026610134169459343, 0.11677291989326477, 0.4193292558193207], [0.4771374762058258, 0.012691129930317402, 0.01602034829556942, 0.014652919955551624, 0.4794982075691223]], [[0.4868564009666443, 0.007223149761557579, 0.01408612821251154, 0.005648543126881123, 0.4861857295036316], [0.44920438528060913, 0.04316867142915726, 0.05385047197341919, 0.006645179353654385, 0.4471312463283539], [0.3281577229499817, 0.12411533296108246, 0.15032683312892914, 0.07103791832923889, 0.32636213302612305], [0.36490464210510254, 0.05420185625553131, 0.1297674924135208, 0.08803100138902664, 0.3630950152873993], [0.486915647983551, 0.00718471547588706, 0.014024276286363602, 0.005631355568766594, 0.4862440824508667]], [[0.1805923581123352, 0.29167020320892334, 0.21840432286262512, 0.12844833731651306, 0.1808847039937973], [0.014069700613617897, 0.9558773040771484, 0.015575476922094822, 0.00028581180959008634, 0.01419173926115036], [0.12573735415935516, 0.28837263584136963, 0.4354502260684967, 0.024607179686427116, 0.12583252787590027], [0.016987336799502373, 0.00026380550116300583, 0.0003578217583708465, 0.9654571413993835, 0.01693386398255825], [0.17981594800949097, 0.29284822940826416, 0.21932628750801086, 0.12789463996887207, 0.18011488020420074]], [[0.48601099848747253, 0.010652577504515648, 0.006800749339163303, 0.011157579720020294, 0.4853781461715698], [0.47697913646698, 0.016559263691306114, 0.027875587344169617, 0.003519274527207017, 0.47506678104400635], [0.34711766242980957, 0.22285118699073792, 0.04684285447001457, 0.038398079574108124, 0.34479019045829773], [0.4706366956233978, 0.001151782227680087, 0.05393939092755318, 0.008971654810011387, 0.46530047059059143], [0.48624902963638306, 0.010430023074150085, 0.00668819434940815, 0.011006639339029789, 0.48562607169151306]], [[0.48981615900993347, 0.0019146146951243281, 0.008693644776940346, 0.008749968372285366, 0.49082571268081665], [0.24478043615818024, 0.0785006582736969, 0.3613436818122864, 0.06996500492095947, 0.24541017413139343], [0.14826785027980804, 0.0009166857344098389, 0.011923618614673615, 0.6897923350334167, 0.14909954369068146], [0.3991607129573822, 0.0017541362904012203, 0.007416578475385904, 0.19150738418102264, 0.40016111731529236], [0.4897554814815521, 0.001909558312036097, 0.008739324286580086, 0.008822796866297722, 0.49077287316322327]], [[0.49669790267944336, 0.0008520630071870983, 0.0011623951140791178, 0.003931917250156403, 0.4973558187484741], [0.42928948998451233, 0.09096802026033401, 0.03035910055041313, 0.018449420109391212, 0.43093395233154297], [0.4009980261325836, 0.09262705594301224, 0.07639957219362259, 0.028515636920928955, 0.4014596939086914], [0.3279275596141815, 0.029582394286990166, 0.13406753540039062, 0.1792496144771576, 0.32917290925979614], [0.49668601155281067, 0.0008538129623048007, 0.001167978742159903, 0.003948094788938761, 0.49734413623809814]], [[0.4839753806591034, 0.006494804285466671, 0.006212383043020964, 0.017903389409184456, 0.4854139983654022], [0.2541459798812866, 0.1760035902261734, 0.2902880311012268, 0.025471484288573265, 0.25409096479415894], [0.42207038402557373, 0.05166592821478844, 0.0747106671333313, 0.02809074893593788, 0.4234623610973358], [0.46162670850753784, 0.03233770653605461, 0.009555348195135593, 0.03180970996618271, 0.4646705985069275], [0.48395082354545593, 0.006498354487121105, 0.006231661885976791, 0.017932100221514702, 0.4853871464729309]], [[0.48840540647506714, 0.0038880694191902876, 0.010091274976730347, 0.00904436968266964, 0.488570898771286], [0.10468792170286179, 0.009839711710810661, 0.7795509099960327, 0.0019797373097389936, 0.10394170880317688], [0.3929654657840729, 0.023450173437595367, 0.06614939123392105, 0.1268039047718048, 0.3906311094760895], [0.4941321015357971, 0.0001732797536533326, 0.004572147969156504, 0.005805921740829945, 0.4953165650367737], [0.4885699450969696, 0.0038438274059444666, 0.009986025281250477, 0.008867367170751095, 0.48873281478881836]]], [[[0.48979905247688293, 0.002507725264877081, 0.004087162669748068, 0.01404272299259901, 0.48956334590911865], [0.22152818739414215, 0.06453882157802582, 0.47110071778297424, 0.021435467526316643, 0.22139671444892883], [0.4426223039627075, 0.021185772493481636, 0.059582073241472244, 0.034250613301992416, 0.44235917925834656], [0.4390523433685303, 0.007711827289313078, 0.02876008115708828, 0.08560919761657715, 0.4388665556907654], [0.489798367023468, 0.002508103381842375, 0.004087010398507118, 0.01404398214071989, 0.4895625412464142]], [[0.47666704654693604, 0.019431892782449722, 0.017596296966075897, 0.009727579541504383, 0.47657716274261475], [0.2628547251224518, 0.15430249273777008, 0.18746629357337952, 0.13258850574493408, 0.26278799772262573], [0.2977747917175293, 0.13268841803073883, 0.19528070092201233, 0.07650627195835114, 0.2977498769760132], [0.286952406167984, 0.09506742656230927, 0.12578760087490082, 0.2053280770778656, 0.2868645489215851], [0.4766683578491211, 0.01943368837237358, 0.017593547701835632, 0.009726027026772499, 0.4765782952308655]], [[0.48762041330337524, 0.006608296651393175, 0.004639135207980871, 0.013794390484690666, 0.48733776807785034], [0.3723337650299072, 0.12154881656169891, 0.06472326815128326, 0.06908204406499863, 0.37231209874153137], [0.41782063245773315, 0.0713270902633667, 0.04030657187104225, 0.05276183411478996, 0.4177839756011963], [0.4036065340042114, 0.09516313672065735, 0.06601689755916595, 0.03163566067814827, 0.40357786417007446], [0.48761099576950073, 0.006613464560359716, 0.004643064923584461, 0.013803639449179173, 0.48732882738113403]], [[0.4863165318965912, 0.008002812042832375, 0.008044085465371609, 0.011487176641821861, 0.4861493706703186], [0.2827712893486023, 0.4327700734138489, 0.001868967548944056, 3.4003883229161147e-06, 0.2825862467288971], [0.4700019657611847, 0.0021104419138282537, 0.05819087475538254, 9.988302190322429e-05, 0.4695969223976135], [0.4836139380931854, 2.5319600354123395e-06, 8.002026152098551e-05, 0.033443983644247055, 0.4828595221042633], [0.4863179326057434, 0.008006434887647629, 0.008044167421758175, 0.011480717919766903, 0.4861507713794708]], [[0.4710860252380371, 0.016140108928084373, 0.015779957175254822, 0.025895075872540474, 0.471098929643631], [0.20768167078495026, 0.16293977200984955, 0.20225736498832703, 0.2194325029850006, 0.20768871903419495], [0.2186593860387802, 0.08490460366010666, 0.21614447236061096, 0.2616439759731293, 0.21864758431911469], [0.2484387755393982, 0.13966046273708344, 0.12537358701229095, 0.23813848197460175, 0.2483886331319809], [0.4710945785045624, 0.016140036284923553, 0.015773408114910126, 0.025884393602609634, 0.4711076021194458]], [[0.47754743695259094, 0.008987524546682835, 0.01846064254641533, 0.01755334623157978, 0.47745099663734436], [0.05791627988219261, 0.06299231946468353, 0.7088204026222229, 0.11238355189561844, 0.05788741260766983], [0.16724349558353424, 0.006518272683024406, 0.04036980867385864, 0.6187527179718018, 0.1671157330274582], [0.45374393463134766, 0.0013418933376669884, 0.002324504777789116, 0.08915052562952042, 0.4534391760826111], [0.4775562584400177, 0.008985569700598717, 0.018452957272529602, 0.01754535734653473, 0.4774598777294159]], [[0.4748324751853943, 0.009532969444990158, 0.032714616507291794, 0.008148503489792347, 0.4747714698314667], [0.4135317802429199, 0.007073105312883854, 0.01615591160953045, 0.14981479942798615, 0.41342443227767944], [0.4180392026901245, 0.019951442256569862, 0.012698440812528133, 0.1312713772058487, 0.4180395007133484], [0.38901665806770325, 0.17228037118911743, 0.04284355789422989, 0.0068061621859669685, 0.3890532851219177], [0.4748350977897644, 0.00952940247952938, 0.03271612152457237, 0.008145482279360294, 0.4747738540172577]], [[0.49614420533180237, 0.002720596268773079, 0.001477761659771204, 0.0036322646774351597, 0.49602508544921875], [0.40098506212234497, 0.15179693698883057, 0.020599443465471268, 0.025708217173814774, 0.4009104073047638], [0.3865618109703064, 0.15294137597084045, 0.03145945817232132, 0.042587023228406906, 0.38645032048225403], [0.3706977069377899, 0.04800170660018921, 0.05226818844676018, 0.15837939083576202, 0.370652973651886], [0.496145635843277, 0.002719711512327194, 0.0014772353461012244, 0.0036311009898781776, 0.49602627754211426]], [[0.48888739943504333, 0.008912374265491962, 0.006896123290061951, 0.006396391894668341, 0.48890769481658936], [0.3360751271247864, 0.3018724024295807, 0.023070378229022026, 0.0028388050850480795, 0.33614325523376465], [0.3708612620830536, 0.16784317791461945, 0.07531731575727463, 0.015043637715280056, 0.37093448638916016], [0.39889615774154663, 0.001584879937581718, 0.004515653941780329, 0.19617408514022827, 0.39882931113243103], [0.4888833165168762, 0.008918880484998226, 0.006898726802319288, 0.0063954973593354225, 0.48890361189842224]], [[0.4819447100162506, 0.010410476475954056, 0.022072549909353256, 0.0036589214578270912, 0.4819134473800659], [0.483244925737381, 0.027722375467419624, 0.005465395748615265, 0.000402916717575863, 0.483164519071579], [0.3334560692310333, 0.286855012178421, 0.039762601256370544, 0.006461369805037975, 0.3334648907184601], [0.1682664155960083, 0.09485192596912384, 0.503048837184906, 0.06558938324451447, 0.168243408203125], [0.48195886611938477, 0.01040161307901144, 0.022056827321648598, 0.0036549726501107216, 0.4819276034832001]], [[0.4853173792362213, 0.009219003841280937, 0.00986368115991354, 0.010297100059688091, 0.48530274629592896], [0.362272173166275, 0.1637485921382904, 0.09590896964073181, 0.015809768810868263, 0.3622604310512543], [0.11382914334535599, 0.5357295274734497, 0.1154843270778656, 0.12113848328590393, 0.11381853371858597], [0.1343785971403122, 0.23820707201957703, 0.44396838545799255, 0.04913261532783508, 0.13431331515312195], [0.48532041907310486, 0.009214806370437145, 0.009861892089247704, 0.010297274217009544, 0.4853056073188782]], [[0.47715139389038086, 0.007145703304558992, 0.013929951936006546, 0.02479197084903717, 0.47698089480400085], [0.27621105313301086, 0.36866602301597595, 0.06577088683843613, 0.013145087286829948, 0.27620697021484375], [0.3985412120819092, 0.12516415119171143, 0.06538026034832001, 0.012492102570831776, 0.3984222710132599], [0.4473690390586853, 0.00335119036026299, 0.019773174077272415, 0.08245280385017395, 0.44705384969711304], [0.47716447710990906, 0.007142707239836454, 0.013923639431595802, 0.024774804711341858, 0.4769943058490753]]], [[[0.47808948159217834, 0.007281437050551176, 0.024506045505404472, 0.012058079242706299, 0.4780648648738861], [0.3579900562763214, 0.20356206595897675, 0.05009980499744415, 0.030381226912140846, 0.35796675086021423], [0.3973197937011719, 0.022014616057276726, 0.026890533044934273, 0.15648572146892548, 0.3972893953323364], [0.3629828095436096, 0.024403182789683342, 0.00790365133434534, 0.24174293875694275, 0.3629674017429352], [0.47808775305747986, 0.007282049395143986, 0.024508193135261536, 0.012058955617249012, 0.47806301712989807]], [[0.455862820148468, 0.01941966451704502, 0.026634911075234413, 0.04222912713885307, 0.4558534622192383], [0.27694594860076904, 0.338771790266037, 0.10168758779764175, 0.005643848795443773, 0.27695080637931824], [0.3551108241081238, 0.05020199716091156, 0.21913498640060425, 0.020439041778445244, 0.35511311888694763], [0.277272492647171, 0.02760542556643486, 0.033349376171827316, 0.38449615240097046, 0.27727648615837097], [0.45586326718330383, 0.019419869408011436, 0.026635825634002686, 0.04222715273499489, 0.45585381984710693]], [[0.4778192341327667, 0.009223760105669498, 0.017248891294002533, 0.01789969950914383, 0.47780829668045044], [0.1900583803653717, 0.5716685056686401, 0.03905266895890236, 0.009165801107883453, 0.19005466997623444], [0.3062015175819397, 0.2335892915725708, 0.13528332114219666, 0.018732396885752678, 0.3061935007572174], [0.26840049028396606, 0.002234573243185878, 0.0495724156498909, 0.4114195704460144, 0.2683729827404022], [0.477817565202713, 0.009224828332662582, 0.017250152304768562, 0.017900830134749413, 0.47780662775039673]], [[0.46978065371513367, 0.014826392754912376, 0.022317813709378242, 0.02331610396504402, 0.4697590470314026], [0.46005237102508545, 0.004809224978089333, 0.01193075068295002, 0.0631861761212349, 0.4600214958190918], [0.481091171503067, 0.01293609943240881, 0.010699690319597721, 0.014194900169968605, 0.4810780882835388], [0.46130743622779846, 0.0231168232858181, 0.01822870969772339, 0.036047592759132385, 0.46129950881004333], [0.469779908657074, 0.014826449565589428, 0.022318080067634583, 0.023317327722907066, 0.4697583019733429]], [[0.4760849177837372, 0.008933698758482933, 0.02077779732644558, 0.018115218728780746, 0.47608843445777893], [0.35449284315109253, 0.23786568641662598, 0.0514572449028492, 0.0016777823911979795, 0.35450637340545654], [0.26372191309928894, 0.006507512181997299, 0.3711912930011749, 0.09485648572444916, 0.2637227773666382], [0.4033638536930084, 6.22203151579015e-05, 0.001595812733285129, 0.19168096780776978, 0.4032971262931824], [0.4760831594467163, 0.008934864774346352, 0.020779820159077644, 0.018115689978003502, 0.47608643770217896]], [[0.49468469619750977, 0.0025946523528546095, 0.002240551868453622, 0.005793370772153139, 0.49468669295310974], [0.14160749316215515, 0.02259853295981884, 0.3238156735897064, 0.37037160992622375, 0.14160673320293427], [0.060664691030979156, 0.02945769391953945, 0.03180539980530739, 0.8174073100090027, 0.06066487729549408], [0.25868719816207886, 0.0014156574616208673, 0.007068931125104427, 0.47414588928222656, 0.25868237018585205], [0.494684636592865, 0.002594838384538889, 0.0022406161297112703, 0.005793636664748192, 0.4946862757205963]], [[0.4953448176383972, 0.0006121412152424455, 0.0014146327739581466, 0.007340936455875635, 0.4952874183654785], [0.2678588032722473, 0.2505573332309723, 0.19985084235668182, 0.013878468424081802, 0.2678545117378235], [0.25940603017807007, 0.20853672921657562, 0.26166999340057373, 0.010984796099364758, 0.25940245389938354], [0.363413542509079, 0.0035847947001457214, 0.037506986409425735, 0.2320890575647354, 0.36340558528900146], [0.49534475803375244, 0.0006121463957242668, 0.0014147479087114334, 0.007341023068875074, 0.49528735876083374]], [[0.4639595150947571, 0.029652295634150505, 0.03143758326768875, 0.011011987924575806, 0.4639385938644409], [0.33577579259872437, 0.18206530809402466, 0.056597281247377396, 0.08977171033620834, 0.33578988909721375], [0.16348150372505188, 0.36412861943244934, 0.14833706617355347, 0.16057652235031128, 0.16347622871398926], [0.1193908303976059, 0.265571266412735, 0.3835916519165039, 0.11205795407295227, 0.11938817799091339], [0.4639608561992645, 0.029651060700416565, 0.03143652155995369, 0.011011473834514618, 0.4639400541782379]], [[0.4702458083629608, 0.02223491109907627, 0.010472757741808891, 0.026819758117198944, 0.4702266454696655], [0.031936850398778915, 0.12971322238445282, 0.4982397258281708, 0.3081740736961365, 0.03193606436252594], [0.0911107212305069, 0.239487886428833, 0.29643765091896057, 0.2818516492843628, 0.0911121666431427], [0.1806309074163437, 0.25055986642837524, 0.2319122552871704, 0.15626589953899384, 0.1806311011314392], [0.4702455997467041, 0.022235071286559105, 0.010472932830452919, 0.02681993879377842, 0.4702264964580536]], [[0.492051362991333, 0.0020638941787183285, 0.0018273836467415094, 0.0120204733684659, 0.4920368194580078], [0.4835202693939209, 0.0312655046582222, 0.0006681146332994103, 0.0010071407305076718, 0.483538955450058], [0.4909178614616394, 0.0004985079285688698, 0.016639424487948418, 0.0010512343142181635, 0.49089303612709045], [0.4665030837059021, 0.00031395265250466764, 0.0001298972056247294, 0.06655972450971603, 0.46649330854415894], [0.49205127358436584, 0.0020641593728214502, 0.0018275427864864469, 0.012020442634820938, 0.49203673005104065]], [[0.43022778630256653, 0.02683558687567711, 0.06677568703889847, 0.04592980816960335, 0.4302311837673187], [0.16168621182441711, 0.3529948592185974, 0.29639652371406555, 0.02722153253853321, 0.16170084476470947], [0.2827041745185852, 0.029084406793117523, 0.23199936747550964, 0.17349004745483398, 0.28272202610969543], [0.36404773592948914, 0.00480692507699132, 0.01739957556128502, 0.249677836894989, 0.3640679717063904], [0.4302269220352173, 0.026836354285478592, 0.06677795201539993, 0.045928437262773514, 0.43023034930229187]], [[0.49033012986183167, 0.0035296259447932243, 0.0031811045482754707, 0.012668514624238014, 0.4902906119823456], [0.15338578820228577, 0.029600726440548897, 0.5604209899902344, 0.10320960730314255, 0.15338297188282013], [0.34026843309402466, 0.022556597366929054, 0.12964223325252533, 0.16727480292320251, 0.3402578830718994], [0.4064265787601471, 0.0069129252806305885, 0.041669320315122604, 0.13857489824295044, 0.40641629695892334], [0.4903305768966675, 0.0035294692497700453, 0.0031810258515179157, 0.012668227776885033, 0.49029070138931274]]], [[[0.46521344780921936, 0.022221136838197708, 0.03051237389445305, 0.016843074932694435, 0.4652099013328552], [0.27061137557029724, 0.06018805876374245, 0.13286031782627106, 0.2657277584075928, 0.27061253786087036], [0.08963223546743393, 0.19099053740501404, 0.1594363898038864, 0.47030842304229736, 0.08963243663311005], [0.07432340085506439, 0.11891758441925049, 0.5241298675537109, 0.20830516517162323, 0.07432406395673752], [0.4652128219604492, 0.022221693769097328, 0.030513009056448936, 0.01684344932436943, 0.465209037065506]], [[0.4824947714805603, 0.023125482723116875, 0.004610082134604454, 0.007278709672391415, 0.48249098658561707], [0.0141671821475029, 0.07881838828325272, 0.4634423553943634, 0.42940497398376465, 0.014167066663503647], [0.016713906079530716, 0.029312677681446075, 0.08261927217245102, 0.854640543460846, 0.016713634133338928], [0.1268448531627655, 0.07540470361709595, 0.1337461620569229, 0.5371618866920471, 0.1268424242734909], [0.4824947416782379, 0.023125547915697098, 0.004610009491443634, 0.007278654258698225, 0.48249104619026184]], [[0.4662327170372009, 0.02267344482243061, 0.019434664398431778, 0.025433583185076714, 0.46622559428215027], [0.32660797238349915, 0.19024117290973663, 0.14247794449329376, 0.014067023061215878, 0.3266058564186096], [0.20206721127033234, 0.005418628454208374, 0.38979166746139526, 0.20066143572330475, 0.20206105709075928], [0.41682881116867065, 0.00018945713236462325, 0.0005938022513873875, 0.16557061672210693, 0.4168172776699066], [0.4662321209907532, 0.022674178704619408, 0.019434917718172073, 0.02543361857533455, 0.46622511744499207]], [[0.3943685293197632, 0.10623802989721298, 0.07086728513240814, 0.03415926173329353, 0.39436689019203186], [0.22130873799324036, 0.2105962187051773, 0.27354538440704346, 0.07324033975601196, 0.22130931913852692], [0.14241890609264374, 0.39547181129455566, 0.27904078364372253, 0.040650416165590286, 0.1424180567264557], [0.030410651117563248, 0.19132837653160095, 0.6697739362716675, 0.07807672023773193, 0.03041030280292034], [0.3943682909011841, 0.10623836517333984, 0.07086728513240814, 0.03415936231613159, 0.39436665177345276]], [[0.38529327511787415, 0.07753518223762512, 0.06094109266996384, 0.09094595909118652, 0.3852846026420593], [0.3559390902519226, 0.03654053807258606, 0.025612957775592804, 0.22596970200538635, 0.3559377193450928], [0.39952486753463745, 0.025481777265667915, 0.03435778617858887, 0.1411094069480896, 0.3995262086391449], [0.367321640253067, 0.10825656354427338, 0.036711737513542175, 0.12038833647966385, 0.3673217296600342], [0.38529297709465027, 0.07753567397594452, 0.060941047966480255, 0.09094595164060593, 0.38528433442115784]], [[0.47358569502830505, 0.008342333137989044, 0.005012500565499067, 0.03948959708213806, 0.4735698997974396], [0.4131517708301544, 0.08919757604598999, 0.030289923772215843, 0.05421767756342888, 0.4131430387496948], [0.3359307646751404, 0.15347543358802795, 0.1379309892654419, 0.03674069419503212, 0.3359220325946808], [0.17516128718852997, 0.12186642736196518, 0.3451583981513977, 0.18265676498413086, 0.17515714466571808], [0.47358590364456177, 0.008342240937054157, 0.005012402310967445, 0.03948916122317314, 0.473570317029953]], [[0.49032095074653625, 0.005928895901888609, 0.004759328905493021, 0.008671458810567856, 0.49031931161880493], [0.08415696769952774, 0.2652190625667572, 0.2423056811094284, 0.324163019657135, 0.08415533602237701], [0.2220737189054489, 0.1031329482793808, 0.21044085919857025, 0.24228109419345856, 0.2220713347196579], [0.25105971097946167, 0.0581354983150959, 0.1017104834318161, 0.33803731203079224, 0.2510569393634796], [0.49032095074653625, 0.005928833968937397, 0.004759287927299738, 0.00867133866995573, 0.49031955003738403]], [[0.4969921410083771, 0.0018784685526043177, 0.0013020994374528527, 0.002844630042091012, 0.4969826638698578], [0.20305542647838593, 0.06645923852920532, 0.12701034545898438, 0.400423139333725, 0.20305177569389343], [0.22986890375614166, 0.013328954577445984, 0.03960181772708893, 0.4873350262641907, 0.22986529767513275], [0.42808830738067627, 0.019350886344909668, 0.021899694576859474, 0.10258065164089203, 0.4280804395675659], [0.4969920814037323, 0.0018784879939630628, 0.0013021054910495877, 0.002844623988494277, 0.496982604265213]], [[0.4710555970668793, 0.027552129700779915, 0.017268052324652672, 0.013076421804726124, 0.47104784846305847], [0.10370083898305893, 0.27804189920425415, 0.42208191752433777, 0.09247591346502304, 0.1036994457244873], [0.29916805028915405, 0.05484556034207344, 0.17236344516277313, 0.1744549423456192, 0.29916808009147644], [0.20915457606315613, 0.007823671214282513, 0.016034869477152824, 0.5578322410583496, 0.2091546654701233], [0.47105517983436584, 0.027552422136068344, 0.01726818084716797, 0.013076544739305973, 0.47104766964912415]], [[0.4498398005962372, 0.04222094267606735, 0.03473106026649475, 0.023374756798148155, 0.449833482503891], [0.36557433009147644, 0.14594289660453796, 0.04237433522939682, 0.08053389936685562, 0.36557450890541077], [0.27793294191360474, 0.08510398864746094, 0.20759916305541992, 0.15143238008022308, 0.2779315114021301], [0.306990385055542, 0.04035225510597229, 0.06762367486953735, 0.2780449688434601, 0.3069886565208435], [0.44983944296836853, 0.04222111403942108, 0.034731172025203705, 0.02337495982646942, 0.44983333349227905]], [[0.4587034285068512, 0.03287816792726517, 0.022019952535629272, 0.027702031657099724, 0.4586964249610901], [0.11508491635322571, 0.14820659160614014, 0.26708754897117615, 0.3545377850532532, 0.11508315801620483], [0.20046164095401764, 0.227748304605484, 0.19565249979496002, 0.17567798495292664, 0.2004595398902893], [0.0904587134718895, 0.32389676570892334, 0.23250341415405273, 0.2626825273036957, 0.09045858681201935], [0.45870354771614075, 0.032878123223781586, 0.022019922733306885, 0.02770199254155159, 0.4586964249610901]], [[0.27668705582618713, 0.1234254464507103, 0.1265638768672943, 0.1966380923986435, 0.27668553590774536], [0.13295358419418335, 0.4424873888492584, 0.26941990852355957, 0.02218569442629814, 0.13295333087444305], [0.13521656394004822, 0.0428212508559227, 0.5521104335784912, 0.13463620841503143, 0.13521553575992584], [0.17040209472179413, 0.0071485452353954315, 0.03804003447294235, 0.6140069365501404, 0.1704023778438568], [0.2766878306865692, 0.12342648208141327, 0.1265634298324585, 0.19663597643375397, 0.27668631076812744]]], [[[0.2431693971157074, 0.2207258641719818, 0.14214250445365906, 0.15079328417778015, 0.24316896498203278], [0.3338344395160675, 0.045750394463539124, 0.07503077387809753, 0.21155236661434174, 0.3338319659233093], [0.31150609254837036, 0.21868927776813507, 0.03919888660311699, 0.11910246312618256, 0.31150326132774353], [0.3829919397830963, 0.03773926943540573, 0.013330532237887383, 0.18294917047023773, 0.38298916816711426], [0.24316951632499695, 0.22072598338127136, 0.1421421468257904, 0.15079329907894135, 0.24316908419132233]], [[0.4653320610523224, 0.013603842817246914, 0.020614925771951675, 0.03511406108736992, 0.4653351604938507], [0.04249252378940582, 0.39751118421554565, 0.19487009942531586, 0.322634220123291, 0.04249196499586105], [0.05969427525997162, 0.35544249415397644, 0.26033589243888855, 0.26483383774757385, 0.059693459421396255], [0.1709892898797989, 0.3138110935688019, 0.21828621625900269, 0.12592647969722748, 0.17098695039749146], [0.4653318226337433, 0.013603813014924526, 0.020614933222532272, 0.03511425480246544, 0.4653351604938507]], [[0.02632676437497139, 0.24816736578941345, 0.24436138570308685, 0.45481762290000916, 0.02632683515548706], [0.0881715714931488, 0.3515334129333496, 0.42088982462882996, 0.05123502388596535, 0.08817015588283539], [0.01593995839357376, 0.24327750504016876, 0.6087366342544556, 0.11610616743564606, 0.01593981496989727], [0.0020227618515491486, 0.01754109188914299, 0.013887926936149597, 0.9645254611968994, 0.002022739499807358], [0.026327012106776237, 0.24816714227199554, 0.24436138570308685, 0.4548173248767853, 0.02632707543671131]], [[0.17118674516677856, 0.1407853662967682, 0.1258692741394043, 0.39097318053245544, 0.17118534445762634], [0.36702069640159607, 0.08120176196098328, 0.11131954193115234, 0.07343918085098267, 0.36701884865760803], [0.35648807883262634, 0.03268606960773468, 0.1820538192987442, 0.07228586077690125, 0.3564862012863159], [0.37002477049827576, 0.011914169415831566, 0.024155599996447563, 0.22388321161270142, 0.3700222074985504], [0.17118723690509796, 0.1407840996980667, 0.12586845457553864, 0.39097437262535095, 0.17118583619594574]], [[0.32558274269104004, 0.11013711243867874, 0.11258591711521149, 0.12611284852027893, 0.325581431388855], [0.2520216405391693, 0.2419423758983612, 0.2334759682416916, 0.02053813636302948, 0.2520218789577484], [0.1730595827102661, 0.1632179617881775, 0.48511406779289246, 0.005548194516450167, 0.1730601191520691], [0.17110632359981537, 0.015016493387520313, 0.040263768285512924, 0.6025078296661377, 0.17110557854175568], [0.3255831003189087, 0.11013705283403397, 0.11258593201637268, 0.1261122077703476, 0.32558169960975647]], [[0.365114688873291, 0.14171099662780762, 0.08483272790908813, 0.04322920739650726, 0.3651123344898224], [0.24345748126506805, 0.06292268633842468, 0.24149790406227112, 0.20866556465625763, 0.24345634877681732], [0.20030830800533295, 0.10078523308038712, 0.18948934972286224, 0.3091077506542206, 0.20030942559242249], [0.03243851289153099, 0.05224013701081276, 0.48561063408851624, 0.39727210998535156, 0.03243856877088547], [0.3651151955127716, 0.14171043038368225, 0.08483211696147919, 0.04322938993573189, 0.36511290073394775]], [[0.49633556604385376, 0.003915973007678986, 0.001643052906729281, 0.0017744108336046338, 0.4963310658931732], [0.0617477111518383, 0.02824953757226467, 0.16586259007453918, 0.6823931336402893, 0.061747048050165176], [0.026304040104150772, 0.027380013838410378, 0.06695375591516495, 0.8530585765838623, 0.026303619146347046], [0.10470063984394073, 0.03559919446706772, 0.1074596419930458, 0.6475414633750916, 0.1046990305185318], [0.49633562564849854, 0.003915917593985796, 0.0016430311370640993, 0.001774396630935371, 0.496331125497818]], [[0.06181718781590462, 0.49096211791038513, 0.3126992881298065, 0.07270458340644836, 0.06181690841913223], [0.06580235809087753, 0.5483505129814148, 0.23113566637039185, 0.0889095664024353, 0.06580190360546112], [0.020882532000541687, 0.29287686944007874, 0.36043813824653625, 0.3049200475215912, 0.02088242955505848], [0.018180793151259422, 0.0077734654769301414, 0.0013040800113230944, 0.9545609354972839, 0.018180767074227333], [0.06181693077087402, 0.49096181988716125, 0.3127005398273468, 0.07270406931638718, 0.06181665137410164]], [[0.4957488477230072, 0.001612695399671793, 0.001600316958501935, 0.005286893807351589, 0.4957513213157654], [0.12387380748987198, 0.03192019462585449, 0.08695664256811142, 0.6333765983581543, 0.12387274205684662], [0.11833402514457703, 0.058795683085918427, 0.14102908968925476, 0.5635080933570862, 0.11833305656909943], [0.12240507453680038, 0.04386157914996147, 0.13692277669906616, 0.5744063258171082, 0.12240426242351532], [0.4957488477230072, 0.001612679217942059, 0.00160030007828027, 0.005286790430545807, 0.4957513213157654]], [[0.4880926311016083, 0.010881597176194191, 0.006801765412092209, 0.00613366486504674, 0.48809030652046204], [0.04204130545258522, 0.3378174901008606, 0.4707828164100647, 0.10731801390647888, 0.04204026982188225], [0.05221950262784958, 0.24388183653354645, 0.5306691527366638, 0.12101134657859802, 0.05221822112798691], [0.016058508306741714, 0.36740046739578247, 0.5275203585624695, 0.07296273112297058, 0.016057904809713364], [0.48809269070625305, 0.010881569236516953, 0.0068017300218343735, 0.006133627612143755, 0.4880903661251068]], [[0.37578022480010986, 0.0940610021352768, 0.11251789331436157, 0.041861385107040405, 0.3757794499397278], [0.4390966594219208, 0.04512602463364601, 0.07634027302265167, 0.0003430648648645729, 0.4390939474105835], [0.1198432520031929, 0.018310679122805595, 0.7419145107269287, 9.041052544489503e-05, 0.11984114348888397], [0.09158115833997726, 3.2187770557357e-05, 3.828587432508357e-05, 0.8167684674263, 0.09157988429069519], [0.37578001618385315, 0.09406142681837082, 0.11251815408468246, 0.04186128452420235, 0.3757791519165039]], [[0.08715175092220306, 0.2565235197544098, 0.38846516609191895, 0.18070852756500244, 0.08715100586414337], [0.4055399000644684, 0.11468283832073212, 0.06057359278202057, 0.01366441510617733, 0.40553927421569824], [0.3876444399356842, 0.09646293520927429, 0.11828012764453888, 0.009970497339963913, 0.3876419961452484], [0.43426331877708435, 0.035652726888656616, 0.021061154082417488, 0.07476243376731873, 0.43426036834716797], [0.08715198934078217, 0.2565227746963501, 0.38846510648727417, 0.18070891499519348, 0.08715123683214188]]], [[[0.3871358335018158, 0.08074275404214859, 0.0982009544968605, 0.04678463935852051, 0.3871358335018158], [0.1382356882095337, 0.036534179002046585, 0.08968294411897659, 0.5973124504089355, 0.13823474943637848], [0.1455637663602829, 0.04448186233639717, 0.060767706483602524, 0.6036238074302673, 0.14556285738945007], [0.35793423652648926, 0.05486004799604416, 0.09457958489656448, 0.134694442152977, 0.3579317331314087], [0.38713598251342773, 0.08074261993169785, 0.09820090234279633, 0.046784523874521255, 0.38713598251342773]], [[0.4950232207775116, 0.0024408234748989344, 0.0026898805517703295, 0.004825655370950699, 0.49502038955688477], [0.03650088608264923, 0.045663781464099884, 0.5229125022888184, 0.3584219813346863, 0.03650080785155296], [0.010871626436710358, 0.05565575510263443, 0.3205089867115021, 0.6020920872688293, 0.010871615260839462], [0.01879463903605938, 0.07512817531824112, 0.40726572275161743, 0.480016827583313, 0.018794577568769455], [0.4950232207775116, 0.00244081299751997, 0.002689862623810768, 0.004825623240321875, 0.4950205087661743]], [[0.23460529744625092, 0.13316559791564941, 0.200856551527977, 0.19676750898361206, 0.23460505902767181], [0.13875173032283783, 0.3600679934024811, 0.2452237755060196, 0.11720500886440277, 0.13875141739845276], [0.1580633670091629, 0.2717624008655548, 0.294734388589859, 0.1173764169216156, 0.1580633521080017], [0.15019004046916962, 0.16801956295967102, 0.27914169430732727, 0.2524581551551819, 0.1501905769109726], [0.23460616171360016, 0.13316509127616882, 0.20085592567920685, 0.1967669278383255, 0.23460592329502106]], [[0.49597597122192383, 0.004442839417606592, 0.0020255327690392733, 0.0015843781875446439, 0.4959712326526642], [0.36290082335472107, 0.0428246445953846, 0.15878309309482574, 0.07259160280227661, 0.36289986968040466], [0.26052385568618774, 0.045160189270973206, 0.15500850975513458, 0.27878624200820923, 0.26052120327949524], [0.05847161263227463, 0.013789813034236431, 0.03059985861182213, 0.838667631149292, 0.05847100168466568], [0.4959758520126343, 0.004442855715751648, 0.0020255332347005606, 0.0015843799337744713, 0.49597135186195374]], [[0.116797536611557, 0.1103675365447998, 0.17902833223342896, 0.47700971364974976, 0.11679689586162567], [0.4290395975112915, 0.031626082956790924, 0.054678015410900116, 0.05561905726790428, 0.42903730273246765], [0.43198803067207336, 0.023162661120295525, 0.034919921308755875, 0.07794423401355743, 0.43198519945144653], [0.4361257255077362, 0.035229574888944626, 0.041246626526117325, 0.05127502605319023, 0.4361230134963989], [0.11679743975400925, 0.11036721616983414, 0.17902810871601105, 0.47701048851013184, 0.11679679900407791]], [[0.4065249264240265, 0.0322457030415535, 0.07208862155675888, 0.08261865377426147, 0.4065220057964325], [0.2058335244655609, 0.0848330557346344, 0.3965284526348114, 0.10697321593761444, 0.20583178102970123], [0.10225370526313782, 0.10130731016397476, 0.5453317761421204, 0.14885428547859192, 0.10225287824869156], [0.05794677138328552, 0.1787347048521042, 0.4224098324775696, 0.2829623520374298, 0.05794635787606239], [0.40652525424957275, 0.032245658338069916, 0.07208842784166336, 0.08261837065219879, 0.40652230381965637]], [[0.046607453376054764, 0.2948465943336487, 0.35639193654060364, 0.2555469274520874, 0.04660714790225029], [0.4990055561065674, 7.661077688680962e-05, 0.00023901047825347632, 0.001673014834523201, 0.4990057945251465], [0.4995713233947754, 0.0001936060143634677, 6.901197775732726e-05, 0.0005940626142546535, 0.4995720386505127], [0.4997074007987976, 0.000241904504946433, 6.438595301005989e-05, 0.0002784388780128211, 0.4997078776359558], [0.04660724848508835, 0.2948472797870636, 0.35639238357543945, 0.2555461525917053, 0.04660692811012268]], [[0.49979183077812195, 0.00010156060307053849, 3.4005533962044865e-05, 0.00028146099066361785, 0.49979111552238464], [0.12386222928762436, 0.22273871302604675, 0.32787972688674927, 0.20165754854679108, 0.12386177480220795], [0.1096557080745697, 0.12418356537818909, 0.2912224233150482, 0.3652835488319397, 0.10965479910373688], [0.09111590683460236, 0.0640089213848114, 0.11149656027555466, 0.6422628164291382, 0.09111574292182922], [0.49979183077812195, 0.00010156031203223392, 3.400537389097735e-05, 0.0002814596227835864, 0.49979111552238464]], [[0.3624536991119385, 0.1370953470468521, 0.08635912090539932, 0.0516396202147007, 0.3624521791934967], [0.17812326550483704, 0.03280974552035332, 0.43308505415916443, 0.17785878479480743, 0.1781231015920639], [0.07715483754873276, 0.05777476355433464, 0.5876836776733398, 0.20023201406002045, 0.07715467363595963], [0.10276390612125397, 0.021705077961087227, 0.535855770111084, 0.23691138625144958, 0.10276392102241516], [0.3624535799026489, 0.13709574937820435, 0.08635912835597992, 0.051639556884765625, 0.36245205998420715]], [[0.4943197965621948, 0.003434767248108983, 0.0031476060394197702, 0.00478132301941514, 0.4943164885044098], [0.08182074874639511, 0.17341601848602295, 0.4665917754173279, 0.19635117053985596, 0.08182033151388168], [0.032541289925575256, 0.1121058464050293, 0.4159659743309021, 0.40684574842453003, 0.03254105523228645], [0.0385742112994194, 0.0661221593618393, 0.49200931191444397, 0.36472055315971375, 0.03857378661632538], [0.49431973695755005, 0.003434761893004179, 0.0031475997529923916, 0.004781327210366726, 0.49431654810905457]], [[0.2703591585159302, 0.2124943882226944, 0.12432364374399185, 0.12246575951576233, 0.27035704255104065], [0.09815420210361481, 0.2004934549331665, 0.4739946126937866, 0.1292043775320053, 0.09815338253974915], [0.15756376087665558, 0.1502688229084015, 0.389694482088089, 0.14491020143032074, 0.15756262838840485], [0.14573298394680023, 0.1482735276222229, 0.4059308171272278, 0.1543310135602951, 0.14573167264461517], [0.2703588306903839, 0.21249453723430634, 0.12432382255792618, 0.12246614694595337, 0.2703567147254944]], [[0.2080855518579483, 0.17281928658485413, 0.20487487316131592, 0.20613592863082886, 0.208084374666214], [0.3152410686016083, 0.06211375817656517, 0.09517773985862732, 0.21222814917564392, 0.3152393102645874], [0.27267980575561523, 0.06631080061197281, 0.11686210334300995, 0.2714688777923584, 0.2726784646511078], [0.194766566157341, 0.06632304936647415, 0.19146981835365295, 0.35267478227615356, 0.19476577639579773], [0.20808538794517517, 0.1728193461894989, 0.20487482845783234, 0.20613613724708557, 0.20808421075344086]]], [[[0.09973762184381485, 0.2702225148677826, 0.19540077447891235, 0.3349018692970276, 0.09973721206188202], [0.1178576722741127, 0.25771045684814453, 0.20978562533855438, 0.2967887818813324, 0.11785747855901718], [0.1586630493402481, 0.25436633825302124, 0.1839894950389862, 0.2443183958530426, 0.15866275131702423], [0.05105980113148689, 0.2520650029182434, 0.20665283501148224, 0.43916255235671997, 0.051059748977422714], [0.0997379869222641, 0.27022233605384827, 0.19540069997310638, 0.3349014222621918, 0.09973757714033127]], [[0.04839006066322327, 0.1024048700928688, 0.22351710498332977, 0.5772980451583862, 0.04838995635509491], [0.46237555146217346, 0.017642805352807045, 0.024417543783783913, 0.03318898752331734, 0.46237510442733765], [0.46288585662841797, 0.025310201570391655, 0.023886138573288918, 0.02503214403986931, 0.46288564801216125], [0.42792585492134094, 0.05797911435365677, 0.04120657965540886, 0.044963132590055466, 0.42792534828186035], [0.048390213400125504, 0.10240471363067627, 0.22351722419261932, 0.5772976875305176, 0.048390090465545654]], [[0.21554896235466003, 0.19117669761180878, 0.2163446992635727, 0.16138039529323578, 0.21554917097091675], [0.08823447674512863, 0.3607996702194214, 0.24688126146793365, 0.2158500850200653, 0.08823453634977341], [0.1469239592552185, 0.2821255326271057, 0.21025168895721436, 0.21377484500408173, 0.1469239443540573], [0.11872931569814682, 0.39256247878074646, 0.21574389934539795, 0.1542350947856903, 0.11872917413711548], [0.21554870903491974, 0.1911768615245819, 0.21634501218795776, 0.16138054430484772, 0.21554891765117645]], [[0.39599665999412537, 0.06814909726381302, 0.0517309308052063, 0.0881272628903389, 0.39599594473838806], [0.3878648579120636, 0.08900360018014908, 0.09446198493242264, 0.04080493375658989, 0.3878646790981293], [0.35992005467414856, 0.11716636270284653, 0.11328393220901489, 0.04970982298254967, 0.35991984605789185], [0.3635428845882416, 0.10914801806211472, 0.1070450022816658, 0.05672121047973633, 0.36354291439056396], [0.39599689841270447, 0.06814900040626526, 0.05173085257411003, 0.08812706172466278, 0.39599618315696716]], [[0.2655988335609436, 0.13103824853897095, 0.14141441881656647, 0.19635039567947388, 0.26559802889823914], [0.13378877937793732, 0.23610585927963257, 0.2572654187679291, 0.23905137181282043, 0.133788600564003], [0.16116352379322052, 0.22475580871105194, 0.249106302857399, 0.20381110906600952, 0.16116325557231903], [0.17454683780670166, 0.1790708601474762, 0.19587652385234833, 0.2759596109390259, 0.1745462268590927], [0.2655988335609436, 0.13103824853897095, 0.14141441881656647, 0.1963503658771515, 0.26559802889823914]], [[0.298153817653656, 0.17582936584949493, 0.16710923612117767, 0.0607551671564579, 0.29815247654914856], [0.15245674550533295, 0.29259759187698364, 0.29070958495140076, 0.11177914589643478, 0.15245695412158966], [0.062342751771211624, 0.346208781003952, 0.4027464985847473, 0.12635917961597443, 0.06234276294708252], [0.03777262195944786, 0.4040672481060028, 0.41415032744407654, 0.10623720288276672, 0.03777264803647995], [0.2981540560722351, 0.17582914233207703, 0.167108952999115, 0.060755085200071335, 0.29815277457237244]], [[0.040911220014095306, 0.13839779794216156, 0.3601778745651245, 0.41960200667381287, 0.04091111198067665], [0.030130064114928246, 0.3045351803302765, 0.35292840003967285, 0.28227636218070984, 0.030129998922348022], [0.06706054508686066, 0.2899547219276428, 0.3951887786388397, 0.18073561787605286, 0.06706038117408752], [0.12541387975215912, 0.2455875426530838, 0.26655852794647217, 0.2370264232158661, 0.12541362643241882], [0.040911197662353516, 0.13839779794216156, 0.3601780831813812, 0.41960179805755615, 0.04091109335422516]], [[0.2750934958457947, 0.0911068320274353, 0.2965455651283264, 0.062161240726709366, 0.27509286999702454], [0.2513631582260132, 0.08587400615215302, 0.19744372367858887, 0.21395643055438995, 0.251362681388855], [0.2320610135793686, 0.11019331216812134, 0.239192932844162, 0.1864919662475586, 0.2320607751607895], [0.17053933441638947, 0.23305588960647583, 0.2708626985549927, 0.1550028920173645, 0.17053915560245514], [0.27509352564811707, 0.09110675007104874, 0.2965455949306488, 0.06216127425432205, 0.27509286999702454]], [[0.04927831515669823, 0.23688657581806183, 0.2041250765323639, 0.4604318141937256, 0.049278195947408676], [0.04855746775865555, 0.2252120077610016, 0.15234223008155823, 0.5253309607505798, 0.04855736345052719], [0.060827113687992096, 0.16917826235294342, 0.10316870361566544, 0.6059989929199219, 0.06082687899470329], [0.09261418133974075, 0.17184603214263916, 0.15872395038604736, 0.48420199751853943, 0.0926138237118721], [0.049278322607278824, 0.23688672482967377, 0.20412510633468628, 0.46043166518211365, 0.04927820712327957]], [[0.18273894488811493, 0.23914743959903717, 0.3226308226585388, 0.0727439895272255, 0.18273882567882538], [0.31212127208709717, 0.09981619566679001, 0.15082980692386627, 0.12511193752288818, 0.3121207654476166], [0.25183114409446716, 0.12728413939476013, 0.21663321554660797, 0.15242090821266174, 0.2518306374549866], [0.21056142449378967, 0.11421580612659454, 0.22908298671245575, 0.2355787307024002, 0.21056100726127625], [0.18273915350437164, 0.23914755880832672, 0.3226306438446045, 0.0727437362074852, 0.1827390193939209]], [[0.09501391649246216, 0.26832157373428345, 0.20033034682273865, 0.3413202464580536, 0.09501391649246216], [0.15276160836219788, 0.140481635928154, 0.251766175031662, 0.30222904682159424, 0.15276160836219788], [0.07245560735464096, 0.10967995226383209, 0.24253030121326447, 0.5028784275054932, 0.07245562970638275], [0.10476388037204742, 0.1313188225030899, 0.29872703552246094, 0.3604263365268707, 0.10476386547088623], [0.09501394629478455, 0.26832154393196106, 0.20033033192157745, 0.341320276260376, 0.09501392394304276]], [[0.13019254803657532, 0.3251677453517914, 0.3404768407344818, 0.07397060841321945, 0.13019226491451263], [0.09115801751613617, 0.3701153099536896, 0.3008773922920227, 0.14669115841388702, 0.09115815162658691], [0.08317068964242935, 0.3448351323604584, 0.40731173753738403, 0.08151165395975113, 0.08317084610462189], [0.17480799555778503, 0.16190089285373688, 0.36883053183555603, 0.11965267360210419, 0.17480795085430145], [0.13019223511219025, 0.32516807317733765, 0.34047695994377136, 0.07397066056728363, 0.13019201159477234]]]], \"left_text\": [\"[CLS]\", \"i\", \"love\", \"transformers\", \"[SEP]\"], \"right_text\": [\"[CLS]\", \"i\", \"love\", \"transformers\", \"[SEP]\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-876c047e73e94c0d8c05b20cb7fc0e0f\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[0.39812859892845154, 0.08420449495315552, 0.11650005728006363, 0.08422822505235672, 0.3169385492801666], [0.4403926432132721, 0.16762524843215942, 0.09620590507984161, 0.09503644704818726, 0.2007397711277008], [0.5963823795318604, 0.041193846613168716, 0.04006786644458771, 0.0988212302327156, 0.2235347181558609], [0.41296133399009705, 0.1041448786854744, 0.20968465507030487, 0.03963122516870499, 0.2335779368877411], [0.4488774836063385, 0.11013173311948776, 0.15683729946613312, 0.08361057937145233, 0.20054291188716888]], [[0.31460797786712646, 0.12299809604883194, 0.2397768348455429, 0.11702374368906021, 0.20559333264827728], [0.2902417480945587, 0.20040622353553772, 0.02476537600159645, 0.27022963762283325, 0.21435701847076416], [0.573124349117279, 0.054388582706451416, 0.056727733463048935, 0.06762755662202835, 0.24813182651996613], [0.5156468749046326, 0.12912917137145996, 0.01698947884142399, 0.06441456079483032, 0.27381986379623413], [0.3842625021934509, 0.10317757725715637, 0.13557760417461395, 0.1464497298002243, 0.23053252696990967]], [[0.3254447281360626, 0.08374116569757462, 0.055840250104665756, 0.017976531758904457, 0.5169973373413086], [0.01971639133989811, 0.25149986147880554, 0.06887548416852951, 0.36328914761543274, 0.29661911725997925], [0.10034646093845367, 0.2524365484714508, 0.07627968490123749, 0.40147683024406433, 0.16946054995059967], [0.12452451139688492, 0.10800543427467346, 0.12755653262138367, 0.017785096541047096, 0.622128427028656], [0.11012458056211472, 0.0785360112786293, 0.12256093323230743, 0.009271989576518536, 0.6795064806938171]], [[0.17072542011737823, 0.20728741586208344, 0.2670135200023651, 0.1390228420495987, 0.21595075726509094], [0.26569864153862, 0.11868258565664291, 0.2862582206726074, 0.12897178530693054, 0.20038868486881256], [0.27244335412979126, 0.051699794828891754, 0.09819629043340683, 0.345843642950058, 0.23181688785552979], [0.18261635303497314, 0.3169862926006317, 0.32721152901649475, 0.046725835651159286, 0.126460000872612], [0.22701482474803925, 0.165755033493042, 0.18938010931015015, 0.21112999320030212, 0.20672005414962769]], [[0.007127091288566589, 0.0004379256279207766, 0.022014206275343895, 0.0029501740355044603, 0.967470645904541], [0.42274677753448486, 0.23312479257583618, 0.1443091332912445, 0.16343118250370026, 0.03638812527060509], [0.5971082448959351, 0.11923626065254211, 0.11636772751808167, 0.13917654752731323, 0.028111206367611885], [0.3852158784866333, 0.10283047705888748, 0.28131619095802307, 0.21924695372581482, 0.011390475556254387], [0.18065419793128967, 0.18708917498588562, 0.33702725172042847, 0.23839354515075684, 0.056835856288671494]], [[0.05867874622344971, 0.47381454706192017, 0.15331657230854034, 0.06746453791856766, 0.24672555923461914], [0.20623542368412018, 0.0965477004647255, 0.33416467905044556, 0.14792349934577942, 0.21512867510318756], [0.2161286622285843, 0.1436508595943451, 0.34384873509407043, 0.05622369796037674, 0.2401479333639145], [0.14739812910556793, 0.21334627270698547, 0.04442363977432251, 0.3486977815628052, 0.24613410234451294], [0.2947900891304016, 0.14755283296108246, 0.10241878777742386, 0.11510589718818665, 0.34013235569000244]], [[0.8633050322532654, 0.03192812576889992, 0.008997894823551178, 0.04920368641614914, 0.04656527191400528], [0.7927790880203247, 0.08273256570100784, 0.009029827080667019, 0.03354561701416969, 0.08191291242837906], [0.03163503482937813, 0.9443315863609314, 0.016988033428788185, 0.003679740708321333, 0.003365451702848077], [0.27703166007995605, 0.036264851689338684, 0.6388247013092041, 0.028226088732481003, 0.01965264044702053], [0.04520376771688461, 0.015583428554236889, 0.1824723482131958, 0.5941252112388611, 0.16261520981788635]], [[0.7450578808784485, 0.06915444880723953, 0.031729429960250854, 0.059112828224897385, 0.09494541585445404], [0.00011187259951839224, 0.0009886950720101595, 0.9944701194763184, 0.004424868617206812, 4.4117200559412595e-06], [3.651737279142253e-05, 2.4262426450150087e-05, 0.0023016552440822124, 0.9936510920524597, 0.003986537456512451], [0.03556719422340393, 8.44802925712429e-05, 5.064379365649074e-05, 0.008643175475299358, 0.955654501914978], [0.01336983684450388, 0.07693640887737274, 0.0037535002920776606, 0.005584923084825277, 0.900355339050293]], [[0.2002420276403427, 0.16572481393814087, 0.28649911284446716, 0.15019242465496063, 0.19734162092208862], [0.7298053503036499, 0.018269753083586693, 0.04821928218007088, 0.13828669488430023, 0.06541896611452103], [0.5700700283050537, 0.030928699299693108, 0.20050838589668274, 0.05358116701245308, 0.1449117809534073], [0.040217600762844086, 0.18296973407268524, 0.03718414157629013, 0.5333868861198425, 0.20624162256717682], [0.21183793246746063, 0.14378522336483002, 0.12473287433385849, 0.1878192126750946, 0.33182471990585327]], [[0.2088485062122345, 0.2012694925069809, 0.05920461192727089, 0.1820475459098816, 0.3486298620700836], [0.22177153825759888, 0.1256176084280014, 0.0901322215795517, 0.17582377791404724, 0.3866548538208008], [0.1988539844751358, 0.04397144541144371, 0.12674659490585327, 0.2628399133682251, 0.3675881326198578], [0.381658136844635, 0.031081940978765488, 0.07531754672527313, 0.22374005615711212, 0.2882022559642792], [0.20840221643447876, 0.3485042154788971, 0.087862528860569, 0.08325404673814774, 0.271977037191391]], [[0.2029639631509781, 0.2189667522907257, 0.12715792655944824, 0.1636291742324829, 0.2872821092605591], [0.1684870570898056, 0.10994210094213486, 0.26540133357048035, 0.16556482017040253, 0.29060474038124084], [0.21376854181289673, 0.12830539047718048, 0.16635122895240784, 0.2810375690460205, 0.21053734421730042], [0.13971641659736633, 0.22749584913253784, 0.29423457384109497, 0.08925501257181168, 0.24929821491241455], [0.1113729178905487, 0.25880110263824463, 0.17865127325057983, 0.16465938091278076, 0.28651532530784607]], [[0.9364169836044312, 0.01994284801185131, 0.013610798865556717, 0.009583884850144386, 0.02044546976685524], [0.002165835117921233, 0.4076341688632965, 0.005304507911205292, 0.5729205012321472, 0.011974970810115337], [0.002349077258259058, 0.47486892342567444, 0.004356088116765022, 0.5040968656539917, 0.014329034835100174], [0.006774035282433033, 0.5559632182121277, 0.01276898942887783, 0.39648696780204773, 0.028006788343191147], [0.002102751052007079, 0.25343459844589233, 0.005283553618937731, 0.7273691296577454, 0.011809980496764183]]], [[[0.86150723695755, 0.01102343201637268, 0.014990766532719135, 0.03403587266802788, 0.07844261825084686], [0.6944530010223389, 0.031413495540618896, 0.04775696247816086, 0.14440852403640747, 0.08196797966957092], [0.7493158578872681, 0.07719650864601135, 0.0038818384055048227, 0.043472446501255035, 0.12613333761692047], [0.7389408946037292, 0.03224112093448639, 0.04246461018919945, 0.02215440943837166, 0.16419893503189087], [0.8439189791679382, 0.022728195413947105, 0.014804515987634659, 0.0400216206908226, 0.07852669805288315]], [[0.7077445983886719, 0.02400830015540123, 0.018892033025622368, 0.03879658877849579, 0.2105584591627121], [0.5621011257171631, 0.18085189163684845, 0.002685880521312356, 0.00650617852807045, 0.24785494804382324], [0.7365278005599976, 0.003996603190898895, 0.025821594521403313, 0.0009590127738192677, 0.23269501328468323], [0.45564883947372437, 0.0063369558192789555, 0.00016128736024256796, 0.4432847201824188, 0.09456825256347656], [0.8759585022926331, 0.020197667181491852, 0.008991568349301815, 0.007737645413726568, 0.087114617228508]], [[0.9529193639755249, 0.0023617963306605816, 0.003966982942074537, 0.0016806138446554542, 0.03907116875052452], [0.5626932382583618, 0.03428930416703224, 0.07074037939310074, 0.05192645639181137, 0.28035062551498413], [0.7910499572753906, 0.02719038538634777, 0.04529357701539993, 0.018412968143820763, 0.11805307865142822], [0.8574643731117249, 0.0059982663951814175, 0.031218290328979492, 0.024318860843777657, 0.08100023120641708], [0.5586279034614563, 0.0503120981156826, 0.08547285944223404, 0.01948455348610878, 0.2861025333404541]], [[0.7353222370147705, 0.02658945508301258, 0.04014855995774269, 0.03962111845612526, 0.1583186686038971], [0.6679375171661377, 0.0478145070374012, 0.05363592877984047, 0.018511949107050896, 0.21210011839866638], [0.6446590423583984, 0.048745203763246536, 0.05967075750231743, 0.019706444814801216, 0.22721853852272034], [0.7960226535797119, 0.030814018100500107, 0.04308182746171951, 0.015407662838697433, 0.11467380076646805], [0.8526143431663513, 0.035002369433641434, 0.029010385274887085, 0.011327185668051243, 0.07204574346542358]], [[0.9083960652351379, 0.011465168558061123, 0.0038180307019501925, 0.030560893937945366, 0.04575992748141289], [0.8620033264160156, 0.019909989088773727, 0.048430223017930984, 0.03876077011227608, 0.030895618721842766], [0.7221620082855225, 0.0725288912653923, 0.021225206553936005, 0.12634778022766113, 0.05773613601922989], [0.7937374711036682, 0.048048168420791626, 0.04090385138988495, 0.02494475431740284, 0.0923658236861229], [0.7632732391357422, 0.021097427234053612, 0.013590339571237564, 0.07220132648944855, 0.12983763217926025]], [[0.28914931416511536, 0.04229902848601341, 0.14079710841178894, 0.09660840034484863, 0.43114620447158813], [0.6290506720542908, 0.0068943616934120655, 0.006515080574899912, 0.03228936716914177, 0.325250506401062], [0.8430335521697998, 0.008831587620079517, 0.006179891061037779, 0.028032947331666946, 0.11392197757959366], [0.9410265684127808, 0.002805168041959405, 0.009953552857041359, 0.016557645052671432, 0.02965717576444149], [0.9746230840682983, 0.004118828102946281, 0.003780944971367717, 0.005120345391333103, 0.012356755323708057]], [[0.9547500014305115, 0.005879567936062813, 0.003733431687578559, 0.010251244530081749, 0.025385819375514984], [0.47382888197898865, 0.06807529181241989, 0.258858859539032, 0.07540558278560638, 0.12383141368627548], [0.8084253668785095, 0.05096011981368065, 0.018038002774119377, 0.059371914714574814, 0.06320463120937347], [0.7608255743980408, 0.02482014335691929, 0.01381296943873167, 0.12980139255523682, 0.07073986530303955], [0.8701735734939575, 0.023083331063389778, 0.015645232051610947, 0.02752526104450226, 0.06357264518737793]], [[0.2037992924451828, 0.14572791755199432, 0.2542632818222046, 0.1888437271118164, 0.20736581087112427], [0.002494131913408637, 0.2383488565683365, 0.3242523670196533, 0.16116361320018768, 0.2737410366535187], [0.0040359534323215485, 0.36484917998313904, 0.18605759739875793, 0.11727394908666611, 0.32778334617614746], [0.00313374400138855, 0.20493611693382263, 0.22308672964572906, 0.40157777070999146, 0.16726557910442352], [0.006196911912411451, 0.17095044255256653, 0.2527135908603668, 0.15223728120326996, 0.4179018437862396]], [[0.9823846220970154, 0.0029734442941844463, 0.004152062349021435, 0.003237162483856082, 0.007252753246575594], [0.0996360257267952, 0.022524023428559303, 0.42134809494018555, 0.22540044784545898, 0.23109149932861328], [0.41717028617858887, 0.004102666862308979, 0.014848969876766205, 0.41856005787849426, 0.14531804621219635], [0.8074283599853516, 0.001688208314590156, 0.00481018703430891, 0.030125577002763748, 0.15594781935214996], [0.9383941292762756, 0.0026325329672545195, 0.003691755933687091, 0.009337672032415867, 0.045943956822156906]], [[0.9160395264625549, 0.005021241959184408, 0.011203797534108162, 0.018694588914513588, 0.04904096946120262], [0.447532594203949, 0.031231697648763657, 0.1455901861190796, 0.09844273328781128, 0.27720287442207336], [0.6177771091461182, 0.0458185039460659, 0.14309675991535187, 0.024247702211141586, 0.1690598875284195], [0.633057177066803, 0.008272858336567879, 0.09402398020029068, 0.13858330249786377, 0.1260627806186676], [0.751583456993103, 0.011447388678789139, 0.060940876603126526, 0.02801932580769062, 0.14800891280174255]], [[0.9154782891273499, 0.0028852964751422405, 0.004817711655050516, 0.0034006875939667225, 0.07341807335615158], [0.6163118481636047, 0.14956028759479523, 0.1351833939552307, 0.028579620644450188, 0.07036478072404861], [0.7628480195999146, 0.04206465557217598, 0.09178899973630905, 0.007779253181070089, 0.09551910310983658], [0.03062719851732254, 0.007890685461461544, 0.02078201435506344, 0.7110999822616577, 0.22960017621517181], [0.9603108167648315, 0.0026847990229725838, 0.004611091688275337, 0.0018068328499794006, 0.030586441978812218]], [[0.6835585236549377, 0.02388257533311844, 0.03832613304257393, 0.06524277478456497, 0.1889900267124176], [0.5690737962722778, 0.15501995384693146, 0.03527103736996651, 0.007736000698059797, 0.23289917409420013], [0.8473697304725647, 0.009806645102798939, 0.050711460411548615, 0.011547318659722805, 0.08056482672691345], [0.4483983814716339, 0.004906094167381525, 0.003510307753458619, 0.43067872524261475, 0.1125064417719841], [0.7953333258628845, 0.018294425681233406, 0.013681027106940746, 0.0218789204955101, 0.1508122980594635]]], [[[0.8075618743896484, 0.026632875204086304, 0.044284649193286896, 0.0109436996281147, 0.11057695746421814], [0.8021371960639954, 0.037128694355487823, 0.012031815946102142, 0.01649373024702072, 0.13220858573913574], [0.6923310160636902, 0.10287822037935257, 0.03181419521570206, 0.002504308009520173, 0.17047229409217834], [0.6166287064552307, 0.08844469487667084, 0.04454261437058449, 0.09977316111326218, 0.15061089396476746], [0.7983595132827759, 0.0253902617841959, 0.06951236724853516, 0.038459472358226776, 0.06827837973833084]], [[0.9035001993179321, 0.020519819110631943, 0.02002490684390068, 0.005448790267109871, 0.05050618201494217], [0.49516692757606506, 0.09770744293928146, 0.22910617291927338, 0.037163667380809784, 0.1408557891845703], [0.455806702375412, 0.047122035175561905, 0.17663472890853882, 0.26586443185806274, 0.05457214638590813], [0.8415323495864868, 0.02742885798215866, 0.010543317534029484, 0.04245510324835777, 0.07804042845964432], [0.38818374276161194, 0.22453561425209045, 0.11272507905960083, 0.025711216032505035, 0.24884437024593353]], [[0.34633827209472656, 0.06296642869710922, 0.39470013976097107, 0.12611404061317444, 0.06988117098808289], [0.9990118741989136, 0.0009754291386343539, 1.1318454795627986e-07, 5.1881674067999484e-08, 1.248319949809229e-05], [0.0001931708975462243, 0.9997119307518005, 9.481162123847753e-05, 1.91640814328764e-09, 3.0159124975170926e-08], [0.00012071050150552765, 0.0012188144028186798, 0.9981385469436646, 0.0005125118186697364, 9.371639862365555e-06], [0.00030583751504309475, 1.6082004776762915e-06, 0.0024111117236316204, 0.9894910454750061, 0.007790431380271912]], [[0.8520419001579285, 0.030518321320414543, 0.025304516777396202, 0.003216496901586652, 0.08891880512237549], [0.7285164594650269, 0.06120254471898079, 0.05285516381263733, 0.008927748538553715, 0.148498073220253], [0.7898738980293274, 0.04528340324759483, 0.051126811653375626, 0.0029034509789198637, 0.11081250011920929], [0.830098032951355, 0.020199693739414215, 0.00649636285379529, 0.034415628761053085, 0.10879037529230118], [0.7540895938873291, 0.04212600365281105, 0.02824133075773716, 0.009834778495132923, 0.16570822894573212]], [[0.8033361434936523, 0.02309497259557247, 0.01826191321015358, 0.025658147409558296, 0.12964873015880585], [0.6268267035484314, 0.03916175290942192, 0.054781459271907806, 0.13135841488838196, 0.14787165820598602], [0.5098944902420044, 0.06985585391521454, 0.06521427631378174, 0.18045498430728912, 0.1745804399251938], [0.4348451495170593, 0.030069101601839066, 0.08076012879610062, 0.18751351535320282, 0.2668120861053467], [0.5859362483024597, 0.06645137071609497, 0.08959513902664185, 0.10769737511873245, 0.15031979978084564]], [[0.29076841473579407, 0.12563319504261017, 0.1499631106853485, 0.16848056018352509, 0.26515480875968933], [0.18433333933353424, 0.5482984781265259, 0.008978798054158688, 0.011466729454696178, 0.24692268669605255], [0.06115800887346268, 0.011267040856182575, 0.8710530996322632, 0.0011347219115123153, 0.05538712441921234], [0.16254574060440063, 0.008799382485449314, 0.004012760706245899, 0.7406584024429321, 0.08398373425006866], [0.2988787591457367, 0.23354202508926392, 0.10419581830501556, 0.053088150918483734, 0.3102951943874359]], [[0.8325456976890564, 0.03475615009665489, 0.051183443516492844, 0.020574461668729782, 0.0609402060508728], [0.31308820843696594, 0.05082246661186218, 0.14133650064468384, 0.22994005680084229, 0.26481273770332336], [0.4812456965446472, 0.03950098156929016, 0.02189824730157852, 0.27636393904685974, 0.18099112808704376], [0.49746960401535034, 0.15931034088134766, 0.22032402455806732, 0.03933608531951904, 0.08355993032455444], [0.3457396924495697, 0.10277792811393738, 0.17550118267536163, 0.1227261945605278, 0.2532549798488617]], [[0.9390066266059875, 0.029564373195171356, 0.007064778357744217, 0.008065911009907722, 0.016298381611704826], [0.011506844311952591, 0.0035696669947355986, 0.9727144837379456, 0.012195384129881859, 1.3556085832533427e-05], [0.08569967746734619, 0.0004936086479574442, 0.020917309448122978, 0.8883546590805054, 0.004534697625786066], [0.48543262481689453, 0.0004957573255524039, 6.836490501882508e-05, 0.025802258402109146, 0.4882010221481323], [0.7268528938293457, 0.029593439772725105, 0.0024036383256316185, 0.008683495223522186, 0.23246650397777557]], [[0.7625669240951538, 0.02117355726659298, 0.021049052476882935, 0.006135161966085434, 0.18907535076141357], [0.6855987906455994, 0.019688250496983528, 0.061575066298246384, 0.022705378010869026, 0.21043246984481812], [0.6967968940734863, 0.11961257457733154, 0.011808054521679878, 0.019223079085350037, 0.15255936980247498], [0.5582551956176758, 0.0258041899651289, 0.03259358927607536, 0.1291818767786026, 0.2541651725769043], [0.7299277186393738, 0.06106313690543175, 0.024328097701072693, 0.007858549244701862, 0.17682254314422607]], [[0.9744979739189148, 0.006341868080198765, 0.005172171164304018, 0.001526273088529706, 0.012461760081350803], [0.9593694806098938, 0.02276206575334072, 0.008516451343894005, 0.0018446629401296377, 0.007507213857024908], [0.7484623789787292, 0.17058497667312622, 0.04757721722126007, 0.010533315129578114, 0.022842174395918846], [0.5395881533622742, 0.2045271098613739, 0.16165322065353394, 0.0396621972322464, 0.05456935986876488], [0.6883262991905212, 0.07283958792686462, 0.10437922179698944, 0.04363137483596802, 0.09082350879907608]], [[0.8561639785766602, 0.045360881835222244, 0.011380121111869812, 0.0022378568537533283, 0.08485730737447739], [0.5250884890556335, 0.1179773136973381, 0.07048165798187256, 0.010215544141829014, 0.27623701095581055], [0.5471076369285583, 0.10903751850128174, 0.04410327598452568, 0.014452080242335796, 0.2852994501590729], [0.6509661674499512, 0.01319735124707222, 0.05969717353582382, 0.012165619060397148, 0.2639737129211426], [0.5995981097221375, 0.06396745145320892, 0.06310825049877167, 0.023234877735376358, 0.2500913441181183]], [[0.11330137401819229, 0.17999345064163208, 0.17339052259922028, 0.14298976957798004, 0.3903248906135559], [0.6392049789428711, 0.04597223177552223, 0.27429357171058655, 0.02369813248515129, 0.016831060871481895], [0.026937924325466156, 0.052267882972955704, 0.08235551416873932, 0.18130075931549072, 0.657137930393219], [0.01304722111672163, 0.005187025759369135, 0.010086419992148876, 0.04337592050433159, 0.9283033609390259], [0.47385305166244507, 0.10860748589038849, 0.19168438017368317, 0.1770186573266983, 0.048836492002010345]]], [[[0.0925428718328476, 0.2859051525592804, 0.30513399839401245, 0.20095665752887726, 0.11546136438846588], [0.8028484582901001, 0.02718830481171608, 0.042514801025390625, 0.008172291330993176, 0.11927612125873566], [0.6678282022476196, 0.05724462494254112, 0.16610807180404663, 0.010260804556310177, 0.09855834394693375], [0.4908193349838257, 0.008182908408343792, 0.018631940707564354, 0.29275164008140564, 0.18961425125598907], [0.747965931892395, 0.036552805453538895, 0.06164140626788139, 0.022350259125232697, 0.1314895898103714]], [[0.7077264785766602, 0.0743335410952568, 0.07119686156511307, 0.0687444657087326, 0.07799869030714035], [0.9775446653366089, 0.019378740340471268, 0.001510517206043005, 0.0006065372144803405, 0.0009595339652150869], [0.9231095910072327, 0.06532277166843414, 0.008072016760706902, 0.0025668221060186625, 0.0009288052096962929], [0.9333112239837646, 0.012877636589109898, 0.02879805862903595, 0.018004508689045906, 0.007008570712059736], [0.8360661864280701, 0.0023944159038364887, 0.005477460566908121, 0.026815224438905716, 0.12924663722515106]], [[0.49203792214393616, 0.11513146013021469, 0.06997351348400116, 0.02745373547077179, 0.2954033315181732], [0.19588321447372437, 0.026525577530264854, 0.7173383235931396, 0.05147303268313408, 0.008779876865446568], [0.9243248701095581, 0.04720456898212433, 0.007063549477607012, 0.005115954205393791, 0.016291102394461632], [0.9083707928657532, 0.01765601709485054, 0.020900987088680267, 0.03219173476099968, 0.020880376920104027], [0.7030029296875, 0.0616723969578743, 0.09650243818759918, 0.028558604419231415, 0.11026358604431152]], [[0.3861979842185974, 0.12171842902898788, 0.09215591102838516, 0.10663807392120361, 0.29328957200050354], [0.3890783190727234, 0.019186679273843765, 0.16948345303535461, 0.3047948181629181, 0.11745668202638626], [0.5565298795700073, 0.02464941516518593, 0.09101436287164688, 0.1819162517786026, 0.14589011669158936], [0.7707909941673279, 0.03662744536995888, 0.06484215706586838, 0.02816537581384182, 0.0995739996433258], [0.39163291454315186, 0.08133019506931305, 0.12271194905042648, 0.07352811843156815, 0.3307967782020569]], [[0.3198782503604889, 0.16563981771469116, 0.14586742222309113, 0.16373451054096222, 0.20488007366657257], [0.6100217700004578, 0.32574573159217834, 0.0006912790704518557, 1.2755108400597237e-05, 0.06352841854095459], [0.8999199867248535, 0.00036874235956929624, 0.06393839418888092, 1.0344958354835398e-05, 0.03576260432600975], [0.3604011833667755, 4.929097030981211e-06, 5.914606390433619e-06, 0.5481065511703491, 0.09148142486810684], [0.7518201470375061, 0.03668734431266785, 0.008064660243690014, 0.007583109196275473, 0.19584472477436066]], [[0.3021949529647827, 0.09507489949464798, 0.23806802928447723, 0.11573202908039093, 0.24893005192279816], [0.7431187629699707, 0.06196478381752968, 0.02244926430284977, 0.0037439996376633644, 0.16872312128543854], [0.8468388915061951, 0.02058425359427929, 0.048913586884737015, 0.00460232375189662, 0.07906091213226318], [0.8718322515487671, 0.012226397171616554, 0.008680804632604122, 0.031047632917761803, 0.07621288299560547], [0.7017644643783569, 0.01070666778832674, 0.01799548976123333, 0.015202352777123451, 0.25433099269866943]], [[0.5663278698921204, 0.09278355538845062, 0.04392465576529503, 0.02765728533267975, 0.26930662989616394], [0.0002626515051815659, 0.0008943877182900906, 0.9987227320671082, 0.000120071395940613, 1.5444379641849082e-07], [0.6199795007705688, 2.9605431336676702e-05, 0.0004284161841496825, 0.3793579638004303, 0.00020444294204935431], [0.6655687689781189, 4.809102392755449e-06, 0.0002140963333658874, 0.006332992576062679, 0.32787930965423584], [0.2643803358078003, 2.243432618342922e-06, 2.501665676390985e-06, 0.000282155757304281, 0.7353327870368958]], [[0.4574976861476898, 0.07283816486597061, 0.14016039669513702, 0.24340569972991943, 0.08609800040721893], [0.1430225819349289, 0.013570088893175125, 0.7604113221168518, 0.07277479022741318, 0.010221215896308422], [0.9273419976234436, 0.0009463069145567715, 0.016362376511096954, 0.04603741690516472, 0.009311867877840996], [0.8151144981384277, 0.0013239088002592325, 0.023439396172761917, 0.0732715055346489, 0.08685075491666794], [0.9228160381317139, 0.003958503715693951, 0.01059409137815237, 0.020337410271167755, 0.04229382798075676]], [[0.5178878307342529, 0.0880548357963562, 0.09258411824703217, 0.14640218019485474, 0.15507103502750397], [0.9734624028205872, 0.012794425711035728, 0.00833885744214058, 0.0018870700150728226, 0.003517386969178915], [0.8052858710289001, 0.13009439408779144, 0.04481378197669983, 0.010313602164387703, 0.009492459706962109], [0.538370668888092, 0.20390883088111877, 0.1695791631937027, 0.06553962081670761, 0.022601693868637085], [0.6421404480934143, 0.030685199424624443, 0.08521915227174759, 0.08145050704479218, 0.1605048030614853]], [[0.44668686389923096, 0.12103648483753204, 0.11768874526023865, 0.09759735316038132, 0.21699059009552002], [0.8428013920783997, 0.10313965380191803, 0.019941043108701706, 0.0038658929988741875, 0.030252011492848396], [0.8610410690307617, 0.060581181198358536, 0.04533598944544792, 0.013889104127883911, 0.019152792170643806], [0.5497962832450867, 0.0318324975669384, 0.1773797869682312, 0.1617916077375412, 0.07919981330633163], [0.6312271356582642, 0.0721793919801712, 0.08998243510723114, 0.060259707272052765, 0.14635133743286133]], [[0.5338233113288879, 0.09261452406644821, 0.15207819640636444, 0.08529040217399597, 0.13619357347488403], [0.17068898677825928, 0.03173670545220375, 0.37393054366111755, 0.22667193412780762, 0.1969718188047409], [0.6233339905738831, 0.005836189724504948, 0.15927383303642273, 0.0799810141324997, 0.13157501816749573], [0.6919676661491394, 0.00493792025372386, 0.04715385660529137, 0.07754284888505936, 0.17839767038822174], [0.8796846866607666, 0.002689806278795004, 0.0078012775629758835, 0.013742766343057156, 0.09608148783445358]], [[0.18229836225509644, 0.21930232644081116, 0.12189510464668274, 0.17995306849479675, 0.2965511083602905], [0.6484827995300293, 0.04602257162332535, 0.021311338990926743, 0.1612555831670761, 0.12292777746915817], [0.8936607241630554, 0.011244529858231544, 0.0029962114058434963, 0.037577059119939804, 0.05452145263552666], [0.8488944172859192, 0.012210114859044552, 0.017448939383029938, 0.06632887572050095, 0.0551176443696022], [0.6788195967674255, 0.037631839513778687, 0.05830004811286926, 0.051986902952194214, 0.1732616275548935]]], [[[0.40959492325782776, 0.05429363250732422, 0.06378146260976791, 0.07043003290891647, 0.40189996361732483], [0.06353994458913803, 0.0852838084101677, 0.5550304055213928, 0.2263764888048172, 0.06976930797100067], [0.12850846350193024, 0.09997352957725525, 0.32607924938201904, 0.30110228061676025, 0.14433641731739044], [0.23967184126377106, 0.06565079092979431, 0.25642046332359314, 0.17081227898597717, 0.2674446702003479], [0.39771661162376404, 0.061940908432006836, 0.07018207758665085, 0.07837354391813278, 0.3917868137359619]], [[0.3793768584728241, 0.06726551055908203, 0.09567862749099731, 0.07668773829936981, 0.38099128007888794], [0.30586764216423035, 0.11535905301570892, 0.16405488550662994, 0.10858040302991867, 0.3061380386352539], [0.1886037439107895, 0.1870415359735489, 0.30709365010261536, 0.12686456739902496, 0.1903965026140213], [0.35087719559669495, 0.07567872852087021, 0.15041576325893402, 0.08090623468160629, 0.34212207794189453], [0.3608587980270386, 0.07908087968826294, 0.10823724418878555, 0.08835232257843018, 0.3634707033634186]], [[0.4886612892150879, 0.008006134070456028, 0.0029040807858109474, 0.012289500795304775, 0.48813894391059875], [0.433368444442749, 0.044050298631191254, 0.028878523036837578, 0.007748489733785391, 0.48595428466796875], [0.3004518449306488, 0.32341888546943665, 0.04322309046983719, 0.006771002430468798, 0.32613521814346313], [0.4460737705230713, 0.016877474263310432, 0.012701797299087048, 0.02491573803126812, 0.4994312524795532], [0.4876900315284729, 0.008497788570821285, 0.003373831044882536, 0.012318278662860394, 0.48812007904052734]], [[0.5007402300834656, 0.002595712197944522, 0.005970080383121967, 0.003695453517138958, 0.4869985282421112], [0.37734901905059814, 0.13579590618610382, 0.08404601365327835, 0.008508913218975067, 0.3943001925945282], [0.4038362503051758, 0.058842964470386505, 0.10570286214351654, 0.008054489269852638, 0.4235633909702301], [0.47122758626937866, 0.0014408049173653126, 0.014034135267138481, 0.06164379045367241, 0.4516536295413971], [0.4993554949760437, 0.002967272186651826, 0.006737590301781893, 0.004159390926361084, 0.48678022623062134]], [[0.4949313998222351, 0.0016966513358056545, 0.008139267563819885, 0.004542473237961531, 0.49069008231163025], [0.10068279504776001, 0.009597127325832844, 0.7739620208740234, 0.005206095054745674, 0.11055205017328262], [0.425558477640152, 0.0029018463101238012, 0.010415924713015556, 0.06791756302118301, 0.4932061433792114], [0.4403194487094879, 0.0002828071592375636, 0.0017013021279126406, 0.04078410193324089, 0.5169122815132141], [0.4935864210128784, 0.001754936994984746, 0.009219015948474407, 0.004715397953987122, 0.4907241761684418]], [[0.474977046251297, 0.0015133244451135397, 0.0009883801685646176, 0.00028629909502342343, 0.5222349166870117], [0.3982202112674713, 0.09551651030778885, 0.09958402812480927, 0.001591401407495141, 0.40508776903152466], [0.47177740931510925, 0.005325749050825834, 0.012916004285216331, 0.0003645068791229278, 0.5096163153648376], [0.45494410395622253, 0.015348898246884346, 0.005266724154353142, 0.07138411700725555, 0.4530561566352844], [0.47447243332862854, 0.0016963473754003644, 0.0011841662926599383, 0.0003209903370589018, 0.5223259925842285]], [[0.4998774230480194, 0.005190551280975342, 0.0021379285026341677, 0.0034405856858938932, 0.48935356736183167], [0.4189440608024597, 0.06876073032617569, 0.03702399879693985, 0.010337965562939644, 0.46493327617645264], [0.33884918689727783, 0.13039980828762054, 0.10284601897001266, 0.04493069648742676, 0.3829742670059204], [0.11683087795972824, 0.03621778264641762, 0.6081046462059021, 0.10325279831886292, 0.13559392094612122], [0.49777188897132874, 0.005736065562814474, 0.0024068010970950127, 0.003845043247565627, 0.490240216255188]], [[0.4812639653682709, 0.0030788027215749025, 0.004553120583295822, 0.0048520672135055065, 0.5062519907951355], [0.37581685185432434, 0.09865214675664902, 0.10034596174955368, 0.015404261648654938, 0.4097808003425598], [0.3436965346336365, 0.10171371698379517, 0.16281311213970184, 0.021135136485099792, 0.3706414997577667], [0.400690495967865, 0.059875715523958206, 0.09169015288352966, 0.025139067322015762, 0.42260459065437317], [0.4810774028301239, 0.0031639691442251205, 0.004619162064045668, 0.004814601968973875, 0.5063248872756958]], [[0.5032819509506226, 0.0020475832279771566, 0.0006230121944099665, 0.0014712836127728224, 0.49257612228393555], [0.12062101066112518, 0.034975528717041016, 0.6429721117019653, 0.06818945705890656, 0.13324196636676788], [0.36317744851112366, 0.051622577011585236, 0.02145581692457199, 0.1556985080242157, 0.4080456495285034], [0.4650093913078308, 0.002287032315507531, 0.0006076024146750569, 0.02962421253323555, 0.5024717450141907], [0.5020348429679871, 0.0022230257745832205, 0.000737378082703799, 0.0016209266614168882, 0.4933837950229645]], [[0.443921834230423, 0.01785440184175968, 0.01694926992058754, 0.0710066556930542, 0.4502679109573364], [0.37562865018844604, 0.18100129067897797, 0.0022831889800727367, 3.1294382552005118e-06, 0.4410836696624756], [0.3824291229248047, 0.00153095624409616, 0.17764459550380707, 6.106456567067653e-06, 0.43838921189308167], [0.17730757594108582, 5.439510459837038e-07, 1.9611832158261677e-06, 0.6211252212524414, 0.20156466960906982], [0.44000154733657837, 0.01891253888607025, 0.01759394444525242, 0.0758637934923172, 0.4476282000541687]], [[0.4982190728187561, 0.0006397091201506555, 0.0006178262992762029, 0.002741475123912096, 0.49778199195861816], [0.027724890038371086, 0.023010872304439545, 0.8792904615402222, 0.038424052298069, 0.031549710780382156], [0.3858417570590973, 0.00703799445182085, 0.07740623503923416, 0.08003607392311096, 0.4496779441833496], [0.39817678928375244, 0.004763432312756777, 0.03593549132347107, 0.08799103647470474, 0.47313329577445984], [0.4968620836734772, 0.0006671541486866772, 0.000651753565762192, 0.0028595817275345325, 0.4989594519138336]], [[0.48733609914779663, 0.00026300703757442534, 0.0007382174953818321, 0.0023367879912257195, 0.5093259215354919], [0.4415494203567505, 0.06136578321456909, 0.026715964078903198, 0.03578482195734978, 0.4345840513706207], [0.4670204222202301, 0.03386926278471947, 0.01814279332756996, 0.01435072161257267, 0.46661683917045593], [0.3937506675720215, 0.12634851038455963, 0.01825418882071972, 0.06028754636645317, 0.40135908126831055], [0.4879203140735626, 0.0002553584345150739, 0.0006770117324776947, 0.002141436794772744, 0.5090058445930481]]], [[[0.4817788600921631, 0.014009306207299232, 0.004834875930100679, 0.017202207818627357, 0.4821748435497284], [0.4426496624946594, 0.026294423267245293, 0.01909424550831318, 0.07196718454360962, 0.4399943947792053], [0.4668445885181427, 0.013169193640351295, 0.0091101648285985, 0.04883608967065811, 0.4620400071144104], [0.4304269552230835, 0.052272990345954895, 0.009001723490655422, 0.07808775454759598, 0.430210679769516], [0.48184987902641296, 0.013958467170596123, 0.004790756851434708, 0.0171622596681118, 0.4822385907173157]], [[0.4961519241333008, 0.0014797230251133442, 0.004726368002593517, 0.0012834967346861959, 0.4963585138320923], [0.468368798494339, 0.056764867156744, 0.0021079499274492264, 3.2063881008070894e-06, 0.47275519371032715], [0.44463786482810974, 0.0004175677604507655, 0.11075666546821594, 4.783437543665059e-05, 0.44414013624191284], [0.4650593101978302, 1.8835545745332638e-07, 1.1074494068452623e-05, 0.06908060610294342, 0.46584877371788025], [0.496097594499588, 0.0015011490322649479, 0.004767234902828932, 0.0012910603545606136, 0.4963429570198059]], [[0.4921876788139343, 0.0028378849383443594, 0.0035123920533806086, 0.007491580676287413, 0.49397045373916626], [0.14827069640159607, 0.05229279398918152, 0.5104207396507263, 0.1415422111749649, 0.14747360348701477], [0.1226036474108696, 0.05132735148072243, 0.262734979391098, 0.44090163707733154, 0.12243235856294632], [0.4240064024925232, 0.006534110754728317, 0.06730727851390839, 0.07831960916519165, 0.42383256554603577], [0.4922385513782501, 0.002814197214320302, 0.0034874947741627693, 0.007435672450810671, 0.4940240979194641]], [[0.49330756068229675, 0.00497034378349781, 0.005492065101861954, 0.003942323848605156, 0.4922877252101898], [0.2367422729730606, 0.07611255347728729, 0.19197940826416016, 0.25902748107910156, 0.2361382693052292], [0.34184718132019043, 0.053303319960832596, 0.09109252691268921, 0.17190556228160858, 0.3418514132499695], [0.4478435516357422, 0.020028453320264816, 0.046622417867183685, 0.038136810064315796, 0.4473687410354614], [0.4932989478111267, 0.004983332473784685, 0.005496129393577576, 0.003945962060242891, 0.492275595664978]], [[0.47729745507240295, 0.012674051336944103, 0.01583195850253105, 0.01453222893178463, 0.4796643555164337], [0.4152601361274719, 0.09185740351676941, 0.03576567769050598, 0.041895791888237, 0.4152209162712097], [0.40288662910461426, 0.044344350695610046, 0.10087300091981888, 0.047698114067316055, 0.40419796109199524], [0.41655460000038147, 0.02073313668370247, 0.026610134169459343, 0.11677291989326477, 0.4193292558193207], [0.4771374762058258, 0.012691129930317402, 0.01602034829556942, 0.014652919955551624, 0.4794982075691223]], [[0.4868564009666443, 0.007223149761557579, 0.01408612821251154, 0.005648543126881123, 0.4861857295036316], [0.44920438528060913, 0.04316867142915726, 0.05385047197341919, 0.006645179353654385, 0.4471312463283539], [0.3281577229499817, 0.12411533296108246, 0.15032683312892914, 0.07103791832923889, 0.32636213302612305], [0.36490464210510254, 0.05420185625553131, 0.1297674924135208, 0.08803100138902664, 0.3630950152873993], [0.486915647983551, 0.00718471547588706, 0.014024276286363602, 0.005631355568766594, 0.4862440824508667]], [[0.1805923581123352, 0.29167020320892334, 0.21840432286262512, 0.12844833731651306, 0.1808847039937973], [0.014069700613617897, 0.9558773040771484, 0.015575476922094822, 0.00028581180959008634, 0.01419173926115036], [0.12573735415935516, 0.28837263584136963, 0.4354502260684967, 0.024607179686427116, 0.12583252787590027], [0.016987336799502373, 0.00026380550116300583, 0.0003578217583708465, 0.9654571413993835, 0.01693386398255825], [0.17981594800949097, 0.29284822940826416, 0.21932628750801086, 0.12789463996887207, 0.18011488020420074]], [[0.48601099848747253, 0.010652577504515648, 0.006800749339163303, 0.011157579720020294, 0.4853781461715698], [0.47697913646698, 0.016559263691306114, 0.027875587344169617, 0.003519274527207017, 0.47506678104400635], [0.34711766242980957, 0.22285118699073792, 0.04684285447001457, 0.038398079574108124, 0.34479019045829773], [0.4706366956233978, 0.001151782227680087, 0.05393939092755318, 0.008971654810011387, 0.46530047059059143], [0.48624902963638306, 0.010430023074150085, 0.00668819434940815, 0.011006639339029789, 0.48562607169151306]], [[0.48981615900993347, 0.0019146146951243281, 0.008693644776940346, 0.008749968372285366, 0.49082571268081665], [0.24478043615818024, 0.0785006582736969, 0.3613436818122864, 0.06996500492095947, 0.24541017413139343], [0.14826785027980804, 0.0009166857344098389, 0.011923618614673615, 0.6897923350334167, 0.14909954369068146], [0.3991607129573822, 0.0017541362904012203, 0.007416578475385904, 0.19150738418102264, 0.40016111731529236], [0.4897554814815521, 0.001909558312036097, 0.008739324286580086, 0.008822796866297722, 0.49077287316322327]], [[0.49669790267944336, 0.0008520630071870983, 0.0011623951140791178, 0.003931917250156403, 0.4973558187484741], [0.42928948998451233, 0.09096802026033401, 0.03035910055041313, 0.018449420109391212, 0.43093395233154297], [0.4009980261325836, 0.09262705594301224, 0.07639957219362259, 0.028515636920928955, 0.4014596939086914], [0.3279275596141815, 0.029582394286990166, 0.13406753540039062, 0.1792496144771576, 0.32917290925979614], [0.49668601155281067, 0.0008538129623048007, 0.001167978742159903, 0.003948094788938761, 0.49734413623809814]], [[0.4839753806591034, 0.006494804285466671, 0.006212383043020964, 0.017903389409184456, 0.4854139983654022], [0.2541459798812866, 0.1760035902261734, 0.2902880311012268, 0.025471484288573265, 0.25409096479415894], [0.42207038402557373, 0.05166592821478844, 0.0747106671333313, 0.02809074893593788, 0.4234623610973358], [0.46162670850753784, 0.03233770653605461, 0.009555348195135593, 0.03180970996618271, 0.4646705985069275], [0.48395082354545593, 0.006498354487121105, 0.006231661885976791, 0.017932100221514702, 0.4853871464729309]], [[0.48840540647506714, 0.0038880694191902876, 0.010091274976730347, 0.00904436968266964, 0.488570898771286], [0.10468792170286179, 0.009839711710810661, 0.7795509099960327, 0.0019797373097389936, 0.10394170880317688], [0.3929654657840729, 0.023450173437595367, 0.06614939123392105, 0.1268039047718048, 0.3906311094760895], [0.4941321015357971, 0.0001732797536533326, 0.004572147969156504, 0.005805921740829945, 0.4953165650367737], [0.4885699450969696, 0.0038438274059444666, 0.009986025281250477, 0.008867367170751095, 0.48873281478881836]]], [[[0.48979905247688293, 0.002507725264877081, 0.004087162669748068, 0.01404272299259901, 0.48956334590911865], [0.22152818739414215, 0.06453882157802582, 0.47110071778297424, 0.021435467526316643, 0.22139671444892883], [0.4426223039627075, 0.021185772493481636, 0.059582073241472244, 0.034250613301992416, 0.44235917925834656], [0.4390523433685303, 0.007711827289313078, 0.02876008115708828, 0.08560919761657715, 0.4388665556907654], [0.489798367023468, 0.002508103381842375, 0.004087010398507118, 0.01404398214071989, 0.4895625412464142]], [[0.47666704654693604, 0.019431892782449722, 0.017596296966075897, 0.009727579541504383, 0.47657716274261475], [0.2628547251224518, 0.15430249273777008, 0.18746629357337952, 0.13258850574493408, 0.26278799772262573], [0.2977747917175293, 0.13268841803073883, 0.19528070092201233, 0.07650627195835114, 0.2977498769760132], [0.286952406167984, 0.09506742656230927, 0.12578760087490082, 0.2053280770778656, 0.2868645489215851], [0.4766683578491211, 0.01943368837237358, 0.017593547701835632, 0.009726027026772499, 0.4765782952308655]], [[0.48762041330337524, 0.006608296651393175, 0.004639135207980871, 0.013794390484690666, 0.48733776807785034], [0.3723337650299072, 0.12154881656169891, 0.06472326815128326, 0.06908204406499863, 0.37231209874153137], [0.41782063245773315, 0.0713270902633667, 0.04030657187104225, 0.05276183411478996, 0.4177839756011963], [0.4036065340042114, 0.09516313672065735, 0.06601689755916595, 0.03163566067814827, 0.40357786417007446], [0.48761099576950073, 0.006613464560359716, 0.004643064923584461, 0.013803639449179173, 0.48732882738113403]], [[0.4863165318965912, 0.008002812042832375, 0.008044085465371609, 0.011487176641821861, 0.4861493706703186], [0.2827712893486023, 0.4327700734138489, 0.001868967548944056, 3.4003883229161147e-06, 0.2825862467288971], [0.4700019657611847, 0.0021104419138282537, 0.05819087475538254, 9.988302190322429e-05, 0.4695969223976135], [0.4836139380931854, 2.5319600354123395e-06, 8.002026152098551e-05, 0.033443983644247055, 0.4828595221042633], [0.4863179326057434, 0.008006434887647629, 0.008044167421758175, 0.011480717919766903, 0.4861507713794708]], [[0.4710860252380371, 0.016140108928084373, 0.015779957175254822, 0.025895075872540474, 0.471098929643631], [0.20768167078495026, 0.16293977200984955, 0.20225736498832703, 0.2194325029850006, 0.20768871903419495], [0.2186593860387802, 0.08490460366010666, 0.21614447236061096, 0.2616439759731293, 0.21864758431911469], [0.2484387755393982, 0.13966046273708344, 0.12537358701229095, 0.23813848197460175, 0.2483886331319809], [0.4710945785045624, 0.016140036284923553, 0.015773408114910126, 0.025884393602609634, 0.4711076021194458]], [[0.47754743695259094, 0.008987524546682835, 0.01846064254641533, 0.01755334623157978, 0.47745099663734436], [0.05791627988219261, 0.06299231946468353, 0.7088204026222229, 0.11238355189561844, 0.05788741260766983], [0.16724349558353424, 0.006518272683024406, 0.04036980867385864, 0.6187527179718018, 0.1671157330274582], [0.45374393463134766, 0.0013418933376669884, 0.002324504777789116, 0.08915052562952042, 0.4534391760826111], [0.4775562584400177, 0.008985569700598717, 0.018452957272529602, 0.01754535734653473, 0.4774598777294159]], [[0.4748324751853943, 0.009532969444990158, 0.032714616507291794, 0.008148503489792347, 0.4747714698314667], [0.4135317802429199, 0.007073105312883854, 0.01615591160953045, 0.14981479942798615, 0.41342443227767944], [0.4180392026901245, 0.019951442256569862, 0.012698440812528133, 0.1312713772058487, 0.4180395007133484], [0.38901665806770325, 0.17228037118911743, 0.04284355789422989, 0.0068061621859669685, 0.3890532851219177], [0.4748350977897644, 0.00952940247952938, 0.03271612152457237, 0.008145482279360294, 0.4747738540172577]], [[0.49614420533180237, 0.002720596268773079, 0.001477761659771204, 0.0036322646774351597, 0.49602508544921875], [0.40098506212234497, 0.15179693698883057, 0.020599443465471268, 0.025708217173814774, 0.4009104073047638], [0.3865618109703064, 0.15294137597084045, 0.03145945817232132, 0.042587023228406906, 0.38645032048225403], [0.3706977069377899, 0.04800170660018921, 0.05226818844676018, 0.15837939083576202, 0.370652973651886], [0.496145635843277, 0.002719711512327194, 0.0014772353461012244, 0.0036311009898781776, 0.49602627754211426]], [[0.48888739943504333, 0.008912374265491962, 0.006896123290061951, 0.006396391894668341, 0.48890769481658936], [0.3360751271247864, 0.3018724024295807, 0.023070378229022026, 0.0028388050850480795, 0.33614325523376465], [0.3708612620830536, 0.16784317791461945, 0.07531731575727463, 0.015043637715280056, 0.37093448638916016], [0.39889615774154663, 0.001584879937581718, 0.004515653941780329, 0.19617408514022827, 0.39882931113243103], [0.4888833165168762, 0.008918880484998226, 0.006898726802319288, 0.0063954973593354225, 0.48890361189842224]], [[0.4819447100162506, 0.010410476475954056, 0.022072549909353256, 0.0036589214578270912, 0.4819134473800659], [0.483244925737381, 0.027722375467419624, 0.005465395748615265, 0.000402916717575863, 0.483164519071579], [0.3334560692310333, 0.286855012178421, 0.039762601256370544, 0.006461369805037975, 0.3334648907184601], [0.1682664155960083, 0.09485192596912384, 0.503048837184906, 0.06558938324451447, 0.168243408203125], [0.48195886611938477, 0.01040161307901144, 0.022056827321648598, 0.0036549726501107216, 0.4819276034832001]], [[0.4853173792362213, 0.009219003841280937, 0.00986368115991354, 0.010297100059688091, 0.48530274629592896], [0.362272173166275, 0.1637485921382904, 0.09590896964073181, 0.015809768810868263, 0.3622604310512543], [0.11382914334535599, 0.5357295274734497, 0.1154843270778656, 0.12113848328590393, 0.11381853371858597], [0.1343785971403122, 0.23820707201957703, 0.44396838545799255, 0.04913261532783508, 0.13431331515312195], [0.48532041907310486, 0.009214806370437145, 0.009861892089247704, 0.010297274217009544, 0.4853056073188782]], [[0.47715139389038086, 0.007145703304558992, 0.013929951936006546, 0.02479197084903717, 0.47698089480400085], [0.27621105313301086, 0.36866602301597595, 0.06577088683843613, 0.013145087286829948, 0.27620697021484375], [0.3985412120819092, 0.12516415119171143, 0.06538026034832001, 0.012492102570831776, 0.3984222710132599], [0.4473690390586853, 0.00335119036026299, 0.019773174077272415, 0.08245280385017395, 0.44705384969711304], [0.47716447710990906, 0.007142707239836454, 0.013923639431595802, 0.024774804711341858, 0.4769943058490753]]], [[[0.47808948159217834, 0.007281437050551176, 0.024506045505404472, 0.012058079242706299, 0.4780648648738861], [0.3579900562763214, 0.20356206595897675, 0.05009980499744415, 0.030381226912140846, 0.35796675086021423], [0.3973197937011719, 0.022014616057276726, 0.026890533044934273, 0.15648572146892548, 0.3972893953323364], [0.3629828095436096, 0.024403182789683342, 0.00790365133434534, 0.24174293875694275, 0.3629674017429352], [0.47808775305747986, 0.007282049395143986, 0.024508193135261536, 0.012058955617249012, 0.47806301712989807]], [[0.455862820148468, 0.01941966451704502, 0.026634911075234413, 0.04222912713885307, 0.4558534622192383], [0.27694594860076904, 0.338771790266037, 0.10168758779764175, 0.005643848795443773, 0.27695080637931824], [0.3551108241081238, 0.05020199716091156, 0.21913498640060425, 0.020439041778445244, 0.35511311888694763], [0.277272492647171, 0.02760542556643486, 0.033349376171827316, 0.38449615240097046, 0.27727648615837097], [0.45586326718330383, 0.019419869408011436, 0.026635825634002686, 0.04222715273499489, 0.45585381984710693]], [[0.4778192341327667, 0.009223760105669498, 0.017248891294002533, 0.01789969950914383, 0.47780829668045044], [0.1900583803653717, 0.5716685056686401, 0.03905266895890236, 0.009165801107883453, 0.19005466997623444], [0.3062015175819397, 0.2335892915725708, 0.13528332114219666, 0.018732396885752678, 0.3061935007572174], [0.26840049028396606, 0.002234573243185878, 0.0495724156498909, 0.4114195704460144, 0.2683729827404022], [0.477817565202713, 0.009224828332662582, 0.017250152304768562, 0.017900830134749413, 0.47780662775039673]], [[0.46978065371513367, 0.014826392754912376, 0.022317813709378242, 0.02331610396504402, 0.4697590470314026], [0.46005237102508545, 0.004809224978089333, 0.01193075068295002, 0.0631861761212349, 0.4600214958190918], [0.481091171503067, 0.01293609943240881, 0.010699690319597721, 0.014194900169968605, 0.4810780882835388], [0.46130743622779846, 0.0231168232858181, 0.01822870969772339, 0.036047592759132385, 0.46129950881004333], [0.469779908657074, 0.014826449565589428, 0.022318080067634583, 0.023317327722907066, 0.4697583019733429]], [[0.4760849177837372, 0.008933698758482933, 0.02077779732644558, 0.018115218728780746, 0.47608843445777893], [0.35449284315109253, 0.23786568641662598, 0.0514572449028492, 0.0016777823911979795, 0.35450637340545654], [0.26372191309928894, 0.006507512181997299, 0.3711912930011749, 0.09485648572444916, 0.2637227773666382], [0.4033638536930084, 6.22203151579015e-05, 0.001595812733285129, 0.19168096780776978, 0.4032971262931824], [0.4760831594467163, 0.008934864774346352, 0.020779820159077644, 0.018115689978003502, 0.47608643770217896]], [[0.49468469619750977, 0.0025946523528546095, 0.002240551868453622, 0.005793370772153139, 0.49468669295310974], [0.14160749316215515, 0.02259853295981884, 0.3238156735897064, 0.37037160992622375, 0.14160673320293427], [0.060664691030979156, 0.02945769391953945, 0.03180539980530739, 0.8174073100090027, 0.06066487729549408], [0.25868719816207886, 0.0014156574616208673, 0.007068931125104427, 0.47414588928222656, 0.25868237018585205], [0.494684636592865, 0.002594838384538889, 0.0022406161297112703, 0.005793636664748192, 0.4946862757205963]], [[0.4953448176383972, 0.0006121412152424455, 0.0014146327739581466, 0.007340936455875635, 0.4952874183654785], [0.2678588032722473, 0.2505573332309723, 0.19985084235668182, 0.013878468424081802, 0.2678545117378235], [0.25940603017807007, 0.20853672921657562, 0.26166999340057373, 0.010984796099364758, 0.25940245389938354], [0.363413542509079, 0.0035847947001457214, 0.037506986409425735, 0.2320890575647354, 0.36340558528900146], [0.49534475803375244, 0.0006121463957242668, 0.0014147479087114334, 0.007341023068875074, 0.49528735876083374]], [[0.4639595150947571, 0.029652295634150505, 0.03143758326768875, 0.011011987924575806, 0.4639385938644409], [0.33577579259872437, 0.18206530809402466, 0.056597281247377396, 0.08977171033620834, 0.33578988909721375], [0.16348150372505188, 0.36412861943244934, 0.14833706617355347, 0.16057652235031128, 0.16347622871398926], [0.1193908303976059, 0.265571266412735, 0.3835916519165039, 0.11205795407295227, 0.11938817799091339], [0.4639608561992645, 0.029651060700416565, 0.03143652155995369, 0.011011473834514618, 0.4639400541782379]], [[0.4702458083629608, 0.02223491109907627, 0.010472757741808891, 0.026819758117198944, 0.4702266454696655], [0.031936850398778915, 0.12971322238445282, 0.4982397258281708, 0.3081740736961365, 0.03193606436252594], [0.0911107212305069, 0.239487886428833, 0.29643765091896057, 0.2818516492843628, 0.0911121666431427], [0.1806309074163437, 0.25055986642837524, 0.2319122552871704, 0.15626589953899384, 0.1806311011314392], [0.4702455997467041, 0.022235071286559105, 0.010472932830452919, 0.02681993879377842, 0.4702264964580536]], [[0.492051362991333, 0.0020638941787183285, 0.0018273836467415094, 0.0120204733684659, 0.4920368194580078], [0.4835202693939209, 0.0312655046582222, 0.0006681146332994103, 0.0010071407305076718, 0.483538955450058], [0.4909178614616394, 0.0004985079285688698, 0.016639424487948418, 0.0010512343142181635, 0.49089303612709045], [0.4665030837059021, 0.00031395265250466764, 0.0001298972056247294, 0.06655972450971603, 0.46649330854415894], [0.49205127358436584, 0.0020641593728214502, 0.0018275427864864469, 0.012020442634820938, 0.49203673005104065]], [[0.43022778630256653, 0.02683558687567711, 0.06677568703889847, 0.04592980816960335, 0.4302311837673187], [0.16168621182441711, 0.3529948592185974, 0.29639652371406555, 0.02722153253853321, 0.16170084476470947], [0.2827041745185852, 0.029084406793117523, 0.23199936747550964, 0.17349004745483398, 0.28272202610969543], [0.36404773592948914, 0.00480692507699132, 0.01739957556128502, 0.249677836894989, 0.3640679717063904], [0.4302269220352173, 0.026836354285478592, 0.06677795201539993, 0.045928437262773514, 0.43023034930229187]], [[0.49033012986183167, 0.0035296259447932243, 0.0031811045482754707, 0.012668514624238014, 0.4902906119823456], [0.15338578820228577, 0.029600726440548897, 0.5604209899902344, 0.10320960730314255, 0.15338297188282013], [0.34026843309402466, 0.022556597366929054, 0.12964223325252533, 0.16727480292320251, 0.3402578830718994], [0.4064265787601471, 0.0069129252806305885, 0.041669320315122604, 0.13857489824295044, 0.40641629695892334], [0.4903305768966675, 0.0035294692497700453, 0.0031810258515179157, 0.012668227776885033, 0.49029070138931274]]], [[[0.46521344780921936, 0.022221136838197708, 0.03051237389445305, 0.016843074932694435, 0.4652099013328552], [0.27061137557029724, 0.06018805876374245, 0.13286031782627106, 0.2657277584075928, 0.27061253786087036], [0.08963223546743393, 0.19099053740501404, 0.1594363898038864, 0.47030842304229736, 0.08963243663311005], [0.07432340085506439, 0.11891758441925049, 0.5241298675537109, 0.20830516517162323, 0.07432406395673752], [0.4652128219604492, 0.022221693769097328, 0.030513009056448936, 0.01684344932436943, 0.465209037065506]], [[0.4824947714805603, 0.023125482723116875, 0.004610082134604454, 0.007278709672391415, 0.48249098658561707], [0.0141671821475029, 0.07881838828325272, 0.4634423553943634, 0.42940497398376465, 0.014167066663503647], [0.016713906079530716, 0.029312677681446075, 0.08261927217245102, 0.854640543460846, 0.016713634133338928], [0.1268448531627655, 0.07540470361709595, 0.1337461620569229, 0.5371618866920471, 0.1268424242734909], [0.4824947416782379, 0.023125547915697098, 0.004610009491443634, 0.007278654258698225, 0.48249104619026184]], [[0.4662327170372009, 0.02267344482243061, 0.019434664398431778, 0.025433583185076714, 0.46622559428215027], [0.32660797238349915, 0.19024117290973663, 0.14247794449329376, 0.014067023061215878, 0.3266058564186096], [0.20206721127033234, 0.005418628454208374, 0.38979166746139526, 0.20066143572330475, 0.20206105709075928], [0.41682881116867065, 0.00018945713236462325, 0.0005938022513873875, 0.16557061672210693, 0.4168172776699066], [0.4662321209907532, 0.022674178704619408, 0.019434917718172073, 0.02543361857533455, 0.46622511744499207]], [[0.3943685293197632, 0.10623802989721298, 0.07086728513240814, 0.03415926173329353, 0.39436689019203186], [0.22130873799324036, 0.2105962187051773, 0.27354538440704346, 0.07324033975601196, 0.22130931913852692], [0.14241890609264374, 0.39547181129455566, 0.27904078364372253, 0.040650416165590286, 0.1424180567264557], [0.030410651117563248, 0.19132837653160095, 0.6697739362716675, 0.07807672023773193, 0.03041030280292034], [0.3943682909011841, 0.10623836517333984, 0.07086728513240814, 0.03415936231613159, 0.39436665177345276]], [[0.38529327511787415, 0.07753518223762512, 0.06094109266996384, 0.09094595909118652, 0.3852846026420593], [0.3559390902519226, 0.03654053807258606, 0.025612957775592804, 0.22596970200538635, 0.3559377193450928], [0.39952486753463745, 0.025481777265667915, 0.03435778617858887, 0.1411094069480896, 0.3995262086391449], [0.367321640253067, 0.10825656354427338, 0.036711737513542175, 0.12038833647966385, 0.3673217296600342], [0.38529297709465027, 0.07753567397594452, 0.060941047966480255, 0.09094595164060593, 0.38528433442115784]], [[0.47358569502830505, 0.008342333137989044, 0.005012500565499067, 0.03948959708213806, 0.4735698997974396], [0.4131517708301544, 0.08919757604598999, 0.030289923772215843, 0.05421767756342888, 0.4131430387496948], [0.3359307646751404, 0.15347543358802795, 0.1379309892654419, 0.03674069419503212, 0.3359220325946808], [0.17516128718852997, 0.12186642736196518, 0.3451583981513977, 0.18265676498413086, 0.17515714466571808], [0.47358590364456177, 0.008342240937054157, 0.005012402310967445, 0.03948916122317314, 0.473570317029953]], [[0.49032095074653625, 0.005928895901888609, 0.004759328905493021, 0.008671458810567856, 0.49031931161880493], [0.08415696769952774, 0.2652190625667572, 0.2423056811094284, 0.324163019657135, 0.08415533602237701], [0.2220737189054489, 0.1031329482793808, 0.21044085919857025, 0.24228109419345856, 0.2220713347196579], [0.25105971097946167, 0.0581354983150959, 0.1017104834318161, 0.33803731203079224, 0.2510569393634796], [0.49032095074653625, 0.005928833968937397, 0.004759287927299738, 0.00867133866995573, 0.49031955003738403]], [[0.4969921410083771, 0.0018784685526043177, 0.0013020994374528527, 0.002844630042091012, 0.4969826638698578], [0.20305542647838593, 0.06645923852920532, 0.12701034545898438, 0.400423139333725, 0.20305177569389343], [0.22986890375614166, 0.013328954577445984, 0.03960181772708893, 0.4873350262641907, 0.22986529767513275], [0.42808830738067627, 0.019350886344909668, 0.021899694576859474, 0.10258065164089203, 0.4280804395675659], [0.4969920814037323, 0.0018784879939630628, 0.0013021054910495877, 0.002844623988494277, 0.496982604265213]], [[0.4710555970668793, 0.027552129700779915, 0.017268052324652672, 0.013076421804726124, 0.47104784846305847], [0.10370083898305893, 0.27804189920425415, 0.42208191752433777, 0.09247591346502304, 0.1036994457244873], [0.29916805028915405, 0.05484556034207344, 0.17236344516277313, 0.1744549423456192, 0.29916808009147644], [0.20915457606315613, 0.007823671214282513, 0.016034869477152824, 0.5578322410583496, 0.2091546654701233], [0.47105517983436584, 0.027552422136068344, 0.01726818084716797, 0.013076544739305973, 0.47104766964912415]], [[0.4498398005962372, 0.04222094267606735, 0.03473106026649475, 0.023374756798148155, 0.449833482503891], [0.36557433009147644, 0.14594289660453796, 0.04237433522939682, 0.08053389936685562, 0.36557450890541077], [0.27793294191360474, 0.08510398864746094, 0.20759916305541992, 0.15143238008022308, 0.2779315114021301], [0.306990385055542, 0.04035225510597229, 0.06762367486953735, 0.2780449688434601, 0.3069886565208435], [0.44983944296836853, 0.04222111403942108, 0.034731172025203705, 0.02337495982646942, 0.44983333349227905]], [[0.4587034285068512, 0.03287816792726517, 0.022019952535629272, 0.027702031657099724, 0.4586964249610901], [0.11508491635322571, 0.14820659160614014, 0.26708754897117615, 0.3545377850532532, 0.11508315801620483], [0.20046164095401764, 0.227748304605484, 0.19565249979496002, 0.17567798495292664, 0.2004595398902893], [0.0904587134718895, 0.32389676570892334, 0.23250341415405273, 0.2626825273036957, 0.09045858681201935], [0.45870354771614075, 0.032878123223781586, 0.022019922733306885, 0.02770199254155159, 0.4586964249610901]], [[0.27668705582618713, 0.1234254464507103, 0.1265638768672943, 0.1966380923986435, 0.27668553590774536], [0.13295358419418335, 0.4424873888492584, 0.26941990852355957, 0.02218569442629814, 0.13295333087444305], [0.13521656394004822, 0.0428212508559227, 0.5521104335784912, 0.13463620841503143, 0.13521553575992584], [0.17040209472179413, 0.0071485452353954315, 0.03804003447294235, 0.6140069365501404, 0.1704023778438568], [0.2766878306865692, 0.12342648208141327, 0.1265634298324585, 0.19663597643375397, 0.27668631076812744]]], [[[0.2431693971157074, 0.2207258641719818, 0.14214250445365906, 0.15079328417778015, 0.24316896498203278], [0.3338344395160675, 0.045750394463539124, 0.07503077387809753, 0.21155236661434174, 0.3338319659233093], [0.31150609254837036, 0.21868927776813507, 0.03919888660311699, 0.11910246312618256, 0.31150326132774353], [0.3829919397830963, 0.03773926943540573, 0.013330532237887383, 0.18294917047023773, 0.38298916816711426], [0.24316951632499695, 0.22072598338127136, 0.1421421468257904, 0.15079329907894135, 0.24316908419132233]], [[0.4653320610523224, 0.013603842817246914, 0.020614925771951675, 0.03511406108736992, 0.4653351604938507], [0.04249252378940582, 0.39751118421554565, 0.19487009942531586, 0.322634220123291, 0.04249196499586105], [0.05969427525997162, 0.35544249415397644, 0.26033589243888855, 0.26483383774757385, 0.059693459421396255], [0.1709892898797989, 0.3138110935688019, 0.21828621625900269, 0.12592647969722748, 0.17098695039749146], [0.4653318226337433, 0.013603813014924526, 0.020614933222532272, 0.03511425480246544, 0.4653351604938507]], [[0.02632676437497139, 0.24816736578941345, 0.24436138570308685, 0.45481762290000916, 0.02632683515548706], [0.0881715714931488, 0.3515334129333496, 0.42088982462882996, 0.05123502388596535, 0.08817015588283539], [0.01593995839357376, 0.24327750504016876, 0.6087366342544556, 0.11610616743564606, 0.01593981496989727], [0.0020227618515491486, 0.01754109188914299, 0.013887926936149597, 0.9645254611968994, 0.002022739499807358], [0.026327012106776237, 0.24816714227199554, 0.24436138570308685, 0.4548173248767853, 0.02632707543671131]], [[0.17118674516677856, 0.1407853662967682, 0.1258692741394043, 0.39097318053245544, 0.17118534445762634], [0.36702069640159607, 0.08120176196098328, 0.11131954193115234, 0.07343918085098267, 0.36701884865760803], [0.35648807883262634, 0.03268606960773468, 0.1820538192987442, 0.07228586077690125, 0.3564862012863159], [0.37002477049827576, 0.011914169415831566, 0.024155599996447563, 0.22388321161270142, 0.3700222074985504], [0.17118723690509796, 0.1407840996980667, 0.12586845457553864, 0.39097437262535095, 0.17118583619594574]], [[0.32558274269104004, 0.11013711243867874, 0.11258591711521149, 0.12611284852027893, 0.325581431388855], [0.2520216405391693, 0.2419423758983612, 0.2334759682416916, 0.02053813636302948, 0.2520218789577484], [0.1730595827102661, 0.1632179617881775, 0.48511406779289246, 0.005548194516450167, 0.1730601191520691], [0.17110632359981537, 0.015016493387520313, 0.040263768285512924, 0.6025078296661377, 0.17110557854175568], [0.3255831003189087, 0.11013705283403397, 0.11258593201637268, 0.1261122077703476, 0.32558169960975647]], [[0.365114688873291, 0.14171099662780762, 0.08483272790908813, 0.04322920739650726, 0.3651123344898224], [0.24345748126506805, 0.06292268633842468, 0.24149790406227112, 0.20866556465625763, 0.24345634877681732], [0.20030830800533295, 0.10078523308038712, 0.18948934972286224, 0.3091077506542206, 0.20030942559242249], [0.03243851289153099, 0.05224013701081276, 0.48561063408851624, 0.39727210998535156, 0.03243856877088547], [0.3651151955127716, 0.14171043038368225, 0.08483211696147919, 0.04322938993573189, 0.36511290073394775]], [[0.49633556604385376, 0.003915973007678986, 0.001643052906729281, 0.0017744108336046338, 0.4963310658931732], [0.0617477111518383, 0.02824953757226467, 0.16586259007453918, 0.6823931336402893, 0.061747048050165176], [0.026304040104150772, 0.027380013838410378, 0.06695375591516495, 0.8530585765838623, 0.026303619146347046], [0.10470063984394073, 0.03559919446706772, 0.1074596419930458, 0.6475414633750916, 0.1046990305185318], [0.49633562564849854, 0.003915917593985796, 0.0016430311370640993, 0.001774396630935371, 0.496331125497818]], [[0.06181718781590462, 0.49096211791038513, 0.3126992881298065, 0.07270458340644836, 0.06181690841913223], [0.06580235809087753, 0.5483505129814148, 0.23113566637039185, 0.0889095664024353, 0.06580190360546112], [0.020882532000541687, 0.29287686944007874, 0.36043813824653625, 0.3049200475215912, 0.02088242955505848], [0.018180793151259422, 0.0077734654769301414, 0.0013040800113230944, 0.9545609354972839, 0.018180767074227333], [0.06181693077087402, 0.49096181988716125, 0.3127005398273468, 0.07270406931638718, 0.06181665137410164]], [[0.4957488477230072, 0.001612695399671793, 0.001600316958501935, 0.005286893807351589, 0.4957513213157654], [0.12387380748987198, 0.03192019462585449, 0.08695664256811142, 0.6333765983581543, 0.12387274205684662], [0.11833402514457703, 0.058795683085918427, 0.14102908968925476, 0.5635080933570862, 0.11833305656909943], [0.12240507453680038, 0.04386157914996147, 0.13692277669906616, 0.5744063258171082, 0.12240426242351532], [0.4957488477230072, 0.001612679217942059, 0.00160030007828027, 0.005286790430545807, 0.4957513213157654]], [[0.4880926311016083, 0.010881597176194191, 0.006801765412092209, 0.00613366486504674, 0.48809030652046204], [0.04204130545258522, 0.3378174901008606, 0.4707828164100647, 0.10731801390647888, 0.04204026982188225], [0.05221950262784958, 0.24388183653354645, 0.5306691527366638, 0.12101134657859802, 0.05221822112798691], [0.016058508306741714, 0.36740046739578247, 0.5275203585624695, 0.07296273112297058, 0.016057904809713364], [0.48809269070625305, 0.010881569236516953, 0.0068017300218343735, 0.006133627612143755, 0.4880903661251068]], [[0.37578022480010986, 0.0940610021352768, 0.11251789331436157, 0.041861385107040405, 0.3757794499397278], [0.4390966594219208, 0.04512602463364601, 0.07634027302265167, 0.0003430648648645729, 0.4390939474105835], [0.1198432520031929, 0.018310679122805595, 0.7419145107269287, 9.041052544489503e-05, 0.11984114348888397], [0.09158115833997726, 3.2187770557357e-05, 3.828587432508357e-05, 0.8167684674263, 0.09157988429069519], [0.37578001618385315, 0.09406142681837082, 0.11251815408468246, 0.04186128452420235, 0.3757791519165039]], [[0.08715175092220306, 0.2565235197544098, 0.38846516609191895, 0.18070852756500244, 0.08715100586414337], [0.4055399000644684, 0.11468283832073212, 0.06057359278202057, 0.01366441510617733, 0.40553927421569824], [0.3876444399356842, 0.09646293520927429, 0.11828012764453888, 0.009970497339963913, 0.3876419961452484], [0.43426331877708435, 0.035652726888656616, 0.021061154082417488, 0.07476243376731873, 0.43426036834716797], [0.08715198934078217, 0.2565227746963501, 0.38846510648727417, 0.18070891499519348, 0.08715123683214188]]], [[[0.3871358335018158, 0.08074275404214859, 0.0982009544968605, 0.04678463935852051, 0.3871358335018158], [0.1382356882095337, 0.036534179002046585, 0.08968294411897659, 0.5973124504089355, 0.13823474943637848], [0.1455637663602829, 0.04448186233639717, 0.060767706483602524, 0.6036238074302673, 0.14556285738945007], [0.35793423652648926, 0.05486004799604416, 0.09457958489656448, 0.134694442152977, 0.3579317331314087], [0.38713598251342773, 0.08074261993169785, 0.09820090234279633, 0.046784523874521255, 0.38713598251342773]], [[0.4950232207775116, 0.0024408234748989344, 0.0026898805517703295, 0.004825655370950699, 0.49502038955688477], [0.03650088608264923, 0.045663781464099884, 0.5229125022888184, 0.3584219813346863, 0.03650080785155296], [0.010871626436710358, 0.05565575510263443, 0.3205089867115021, 0.6020920872688293, 0.010871615260839462], [0.01879463903605938, 0.07512817531824112, 0.40726572275161743, 0.480016827583313, 0.018794577568769455], [0.4950232207775116, 0.00244081299751997, 0.002689862623810768, 0.004825623240321875, 0.4950205087661743]], [[0.23460529744625092, 0.13316559791564941, 0.200856551527977, 0.19676750898361206, 0.23460505902767181], [0.13875173032283783, 0.3600679934024811, 0.2452237755060196, 0.11720500886440277, 0.13875141739845276], [0.1580633670091629, 0.2717624008655548, 0.294734388589859, 0.1173764169216156, 0.1580633521080017], [0.15019004046916962, 0.16801956295967102, 0.27914169430732727, 0.2524581551551819, 0.1501905769109726], [0.23460616171360016, 0.13316509127616882, 0.20085592567920685, 0.1967669278383255, 0.23460592329502106]], [[0.49597597122192383, 0.004442839417606592, 0.0020255327690392733, 0.0015843781875446439, 0.4959712326526642], [0.36290082335472107, 0.0428246445953846, 0.15878309309482574, 0.07259160280227661, 0.36289986968040466], [0.26052385568618774, 0.045160189270973206, 0.15500850975513458, 0.27878624200820923, 0.26052120327949524], [0.05847161263227463, 0.013789813034236431, 0.03059985861182213, 0.838667631149292, 0.05847100168466568], [0.4959758520126343, 0.004442855715751648, 0.0020255332347005606, 0.0015843799337744713, 0.49597135186195374]], [[0.116797536611557, 0.1103675365447998, 0.17902833223342896, 0.47700971364974976, 0.11679689586162567], [0.4290395975112915, 0.031626082956790924, 0.054678015410900116, 0.05561905726790428, 0.42903730273246765], [0.43198803067207336, 0.023162661120295525, 0.034919921308755875, 0.07794423401355743, 0.43198519945144653], [0.4361257255077362, 0.035229574888944626, 0.041246626526117325, 0.05127502605319023, 0.4361230134963989], [0.11679743975400925, 0.11036721616983414, 0.17902810871601105, 0.47701048851013184, 0.11679679900407791]], [[0.4065249264240265, 0.0322457030415535, 0.07208862155675888, 0.08261865377426147, 0.4065220057964325], [0.2058335244655609, 0.0848330557346344, 0.3965284526348114, 0.10697321593761444, 0.20583178102970123], [0.10225370526313782, 0.10130731016397476, 0.5453317761421204, 0.14885428547859192, 0.10225287824869156], [0.05794677138328552, 0.1787347048521042, 0.4224098324775696, 0.2829623520374298, 0.05794635787606239], [0.40652525424957275, 0.032245658338069916, 0.07208842784166336, 0.08261837065219879, 0.40652230381965637]], [[0.046607453376054764, 0.2948465943336487, 0.35639193654060364, 0.2555469274520874, 0.04660714790225029], [0.4990055561065674, 7.661077688680962e-05, 0.00023901047825347632, 0.001673014834523201, 0.4990057945251465], [0.4995713233947754, 0.0001936060143634677, 6.901197775732726e-05, 0.0005940626142546535, 0.4995720386505127], [0.4997074007987976, 0.000241904504946433, 6.438595301005989e-05, 0.0002784388780128211, 0.4997078776359558], [0.04660724848508835, 0.2948472797870636, 0.35639238357543945, 0.2555461525917053, 0.04660692811012268]], [[0.49979183077812195, 0.00010156060307053849, 3.4005533962044865e-05, 0.00028146099066361785, 0.49979111552238464], [0.12386222928762436, 0.22273871302604675, 0.32787972688674927, 0.20165754854679108, 0.12386177480220795], [0.1096557080745697, 0.12418356537818909, 0.2912224233150482, 0.3652835488319397, 0.10965479910373688], [0.09111590683460236, 0.0640089213848114, 0.11149656027555466, 0.6422628164291382, 0.09111574292182922], [0.49979183077812195, 0.00010156031203223392, 3.400537389097735e-05, 0.0002814596227835864, 0.49979111552238464]], [[0.3624536991119385, 0.1370953470468521, 0.08635912090539932, 0.0516396202147007, 0.3624521791934967], [0.17812326550483704, 0.03280974552035332, 0.43308505415916443, 0.17785878479480743, 0.1781231015920639], [0.07715483754873276, 0.05777476355433464, 0.5876836776733398, 0.20023201406002045, 0.07715467363595963], [0.10276390612125397, 0.021705077961087227, 0.535855770111084, 0.23691138625144958, 0.10276392102241516], [0.3624535799026489, 0.13709574937820435, 0.08635912835597992, 0.051639556884765625, 0.36245205998420715]], [[0.4943197965621948, 0.003434767248108983, 0.0031476060394197702, 0.00478132301941514, 0.4943164885044098], [0.08182074874639511, 0.17341601848602295, 0.4665917754173279, 0.19635117053985596, 0.08182033151388168], [0.032541289925575256, 0.1121058464050293, 0.4159659743309021, 0.40684574842453003, 0.03254105523228645], [0.0385742112994194, 0.0661221593618393, 0.49200931191444397, 0.36472055315971375, 0.03857378661632538], [0.49431973695755005, 0.003434761893004179, 0.0031475997529923916, 0.004781327210366726, 0.49431654810905457]], [[0.2703591585159302, 0.2124943882226944, 0.12432364374399185, 0.12246575951576233, 0.27035704255104065], [0.09815420210361481, 0.2004934549331665, 0.4739946126937866, 0.1292043775320053, 0.09815338253974915], [0.15756376087665558, 0.1502688229084015, 0.389694482088089, 0.14491020143032074, 0.15756262838840485], [0.14573298394680023, 0.1482735276222229, 0.4059308171272278, 0.1543310135602951, 0.14573167264461517], [0.2703588306903839, 0.21249453723430634, 0.12432382255792618, 0.12246614694595337, 0.2703567147254944]], [[0.2080855518579483, 0.17281928658485413, 0.20487487316131592, 0.20613592863082886, 0.208084374666214], [0.3152410686016083, 0.06211375817656517, 0.09517773985862732, 0.21222814917564392, 0.3152393102645874], [0.27267980575561523, 0.06631080061197281, 0.11686210334300995, 0.2714688777923584, 0.2726784646511078], [0.194766566157341, 0.06632304936647415, 0.19146981835365295, 0.35267478227615356, 0.19476577639579773], [0.20808538794517517, 0.1728193461894989, 0.20487482845783234, 0.20613613724708557, 0.20808421075344086]]], [[[0.09973762184381485, 0.2702225148677826, 0.19540077447891235, 0.3349018692970276, 0.09973721206188202], [0.1178576722741127, 0.25771045684814453, 0.20978562533855438, 0.2967887818813324, 0.11785747855901718], [0.1586630493402481, 0.25436633825302124, 0.1839894950389862, 0.2443183958530426, 0.15866275131702423], [0.05105980113148689, 0.2520650029182434, 0.20665283501148224, 0.43916255235671997, 0.051059748977422714], [0.0997379869222641, 0.27022233605384827, 0.19540069997310638, 0.3349014222621918, 0.09973757714033127]], [[0.04839006066322327, 0.1024048700928688, 0.22351710498332977, 0.5772980451583862, 0.04838995635509491], [0.46237555146217346, 0.017642805352807045, 0.024417543783783913, 0.03318898752331734, 0.46237510442733765], [0.46288585662841797, 0.025310201570391655, 0.023886138573288918, 0.02503214403986931, 0.46288564801216125], [0.42792585492134094, 0.05797911435365677, 0.04120657965540886, 0.044963132590055466, 0.42792534828186035], [0.048390213400125504, 0.10240471363067627, 0.22351722419261932, 0.5772976875305176, 0.048390090465545654]], [[0.21554896235466003, 0.19117669761180878, 0.2163446992635727, 0.16138039529323578, 0.21554917097091675], [0.08823447674512863, 0.3607996702194214, 0.24688126146793365, 0.2158500850200653, 0.08823453634977341], [0.1469239592552185, 0.2821255326271057, 0.21025168895721436, 0.21377484500408173, 0.1469239443540573], [0.11872931569814682, 0.39256247878074646, 0.21574389934539795, 0.1542350947856903, 0.11872917413711548], [0.21554870903491974, 0.1911768615245819, 0.21634501218795776, 0.16138054430484772, 0.21554891765117645]], [[0.39599665999412537, 0.06814909726381302, 0.0517309308052063, 0.0881272628903389, 0.39599594473838806], [0.3878648579120636, 0.08900360018014908, 0.09446198493242264, 0.04080493375658989, 0.3878646790981293], [0.35992005467414856, 0.11716636270284653, 0.11328393220901489, 0.04970982298254967, 0.35991984605789185], [0.3635428845882416, 0.10914801806211472, 0.1070450022816658, 0.05672121047973633, 0.36354291439056396], [0.39599689841270447, 0.06814900040626526, 0.05173085257411003, 0.08812706172466278, 0.39599618315696716]], [[0.2655988335609436, 0.13103824853897095, 0.14141441881656647, 0.19635039567947388, 0.26559802889823914], [0.13378877937793732, 0.23610585927963257, 0.2572654187679291, 0.23905137181282043, 0.133788600564003], [0.16116352379322052, 0.22475580871105194, 0.249106302857399, 0.20381110906600952, 0.16116325557231903], [0.17454683780670166, 0.1790708601474762, 0.19587652385234833, 0.2759596109390259, 0.1745462268590927], [0.2655988335609436, 0.13103824853897095, 0.14141441881656647, 0.1963503658771515, 0.26559802889823914]], [[0.298153817653656, 0.17582936584949493, 0.16710923612117767, 0.0607551671564579, 0.29815247654914856], [0.15245674550533295, 0.29259759187698364, 0.29070958495140076, 0.11177914589643478, 0.15245695412158966], [0.062342751771211624, 0.346208781003952, 0.4027464985847473, 0.12635917961597443, 0.06234276294708252], [0.03777262195944786, 0.4040672481060028, 0.41415032744407654, 0.10623720288276672, 0.03777264803647995], [0.2981540560722351, 0.17582914233207703, 0.167108952999115, 0.060755085200071335, 0.29815277457237244]], [[0.040911220014095306, 0.13839779794216156, 0.3601778745651245, 0.41960200667381287, 0.04091111198067665], [0.030130064114928246, 0.3045351803302765, 0.35292840003967285, 0.28227636218070984, 0.030129998922348022], [0.06706054508686066, 0.2899547219276428, 0.3951887786388397, 0.18073561787605286, 0.06706038117408752], [0.12541387975215912, 0.2455875426530838, 0.26655852794647217, 0.2370264232158661, 0.12541362643241882], [0.040911197662353516, 0.13839779794216156, 0.3601780831813812, 0.41960179805755615, 0.04091109335422516]], [[0.2750934958457947, 0.0911068320274353, 0.2965455651283264, 0.062161240726709366, 0.27509286999702454], [0.2513631582260132, 0.08587400615215302, 0.19744372367858887, 0.21395643055438995, 0.251362681388855], [0.2320610135793686, 0.11019331216812134, 0.239192932844162, 0.1864919662475586, 0.2320607751607895], [0.17053933441638947, 0.23305588960647583, 0.2708626985549927, 0.1550028920173645, 0.17053915560245514], [0.27509352564811707, 0.09110675007104874, 0.2965455949306488, 0.06216127425432205, 0.27509286999702454]], [[0.04927831515669823, 0.23688657581806183, 0.2041250765323639, 0.4604318141937256, 0.049278195947408676], [0.04855746775865555, 0.2252120077610016, 0.15234223008155823, 0.5253309607505798, 0.04855736345052719], [0.060827113687992096, 0.16917826235294342, 0.10316870361566544, 0.6059989929199219, 0.06082687899470329], [0.09261418133974075, 0.17184603214263916, 0.15872395038604736, 0.48420199751853943, 0.0926138237118721], [0.049278322607278824, 0.23688672482967377, 0.20412510633468628, 0.46043166518211365, 0.04927820712327957]], [[0.18273894488811493, 0.23914743959903717, 0.3226308226585388, 0.0727439895272255, 0.18273882567882538], [0.31212127208709717, 0.09981619566679001, 0.15082980692386627, 0.12511193752288818, 0.3121207654476166], [0.25183114409446716, 0.12728413939476013, 0.21663321554660797, 0.15242090821266174, 0.2518306374549866], [0.21056142449378967, 0.11421580612659454, 0.22908298671245575, 0.2355787307024002, 0.21056100726127625], [0.18273915350437164, 0.23914755880832672, 0.3226306438446045, 0.0727437362074852, 0.1827390193939209]], [[0.09501391649246216, 0.26832157373428345, 0.20033034682273865, 0.3413202464580536, 0.09501391649246216], [0.15276160836219788, 0.140481635928154, 0.251766175031662, 0.30222904682159424, 0.15276160836219788], [0.07245560735464096, 0.10967995226383209, 0.24253030121326447, 0.5028784275054932, 0.07245562970638275], [0.10476388037204742, 0.1313188225030899, 0.29872703552246094, 0.3604263365268707, 0.10476386547088623], [0.09501394629478455, 0.26832154393196106, 0.20033033192157745, 0.341320276260376, 0.09501392394304276]], [[0.13019254803657532, 0.3251677453517914, 0.3404768407344818, 0.07397060841321945, 0.13019226491451263], [0.09115801751613617, 0.3701153099536896, 0.3008773922920227, 0.14669115841388702, 0.09115815162658691], [0.08317068964242935, 0.3448351323604584, 0.40731173753738403, 0.08151165395975113, 0.08317084610462189], [0.17480799555778503, 0.16190089285373688, 0.36883053183555603, 0.11965267360210419, 0.17480795085430145], [0.13019223511219025, 0.32516807317733765, 0.34047695994377136, 0.07397066056728363, 0.13019201159477234]]]], \"left_text\": [\"[CLS]\", \"i\", \"love\", \"transformers\", \"[SEP]\"], \"right_text\": [\"[CLS]\", \"i\", \"love\", \"transformers\", \"[SEP]\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-876c047e73e94c0d8c05b20cb7fc0e0f\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12} is a template marker that is replaced by actual params.\n        const config = {};\n\n        const MIN_X = 0;\n        const MIN_Y = 0;\n        const DIV_WIDTH = 970;\n        const THUMBNAIL_PADDING = 5;\n        const DETAIL_WIDTH = 300;\n        const DETAIL_ATTENTION_WIDTH = 140;\n        const DETAIL_BOX_WIDTH = 80;\n        const DETAIL_BOX_HEIGHT = 18;\n        const DETAIL_PADDING = 15;\n        const ATTN_PADDING = 0;\n        const DETAIL_HEADING_HEIGHT = 25;\n        const HEADING_TEXT_SIZE = 15;\n        const HEADING_PADDING = 5;\n        const TEXT_SIZE = 13;\n        const TEXT_PADDING = 5;\n        const LAYER_COLORS = d3.schemeCategory10;\n        const PALETTE = {\n            'light': {\n                'text': 'black',\n                'background': 'white',\n                'highlight': '#F5F5F5'\n            },\n            'dark': {\n                'text': '#ccc',\n                'background': 'black',\n                'highlight': '#222'\n            }\n        }\n\n        function render() {\n\n            // Set global state variables\n\n            var attData = config.attention[config.filter];\n            config.leftText = attData.left_text;\n            config.rightText = attData.right_text;\n            config.attn = attData.attn;\n            config.numLayers = config.attn.length;\n            config.numHeads = config.attn[0].length;\n            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n\n            const vis = $(`#${config.rootDivId} #vis`)\n            vis.empty();\n            vis.attr(\"height\", config.divHeight);\n            config.svg = d3.select(`#${config.rootDivId} #vis`)\n                .append('svg')\n                .attr(\"width\", DIV_WIDTH)\n                .attr(\"height\", config.divHeight)\n                .attr(\"fill\", getBackgroundColor());\n\n            renderAxisLabels();\n\n            var i;\n            var j;\n            for (i = 0; i < config.numLayers; i++) {\n                for (j = 0; j < config.numHeads; j++) {\n                    renderThumbnail(i, j);\n                }\n            }\n        }\n\n        function renderAxisLabels() {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            const tableWidth = config.thumbnailWidth * config.heads.length;\n            config.svg.append(\"text\")\n                .text(\"Heads\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", axisSize + tableWidth / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", 0)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numHeads; i++) {\n                config.svg.append(\"text\")\n                    .text(config.heads[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n                    .attr(\"text-anchor\", \"middle\")\n                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n                    .attr(\"dy\", TEXT_SIZE);\n            }\n            let x = 0;\n            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n            console.log(\"x\", x, y)\n            config.svg.append(\"text\")\n                .text(\"Layers\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", x)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numLayers; i++) {\n                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n                y = axisSize + (i + .5) * config.thumbnailHeight;\n                config.svg.append(\"text\")\n                    .text(config.layers[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", x)\n                    .attr(\"text-anchor\", \"end\")\n                    .attr(\"y\", y)\n                    .attr(\"dy\", TEXT_SIZE / 2);\n            }\n        }\n\n\n        function renderThumbnail(layerIndex, headIndex) {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n            const x = headIndex * config.thumbnailWidth + axisSize;\n            const y = layerIndex * config.thumbnailHeight + axisSize;\n            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n        }\n\n        function renderDetail(att, layerIndex, headIndex) {\n            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            var xOffset = .8 * config.thumbnailWidth;\n            var maxX = DIV_WIDTH;\n            var maxY = config.divHeight - 3;\n            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n            if (x < MIN_X) {\n                x = MIN_X;\n            } else if (x + DETAIL_WIDTH > maxX) {\n                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n            }\n            var posLeftText = x;\n            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            var yOffset = 20;\n            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n            if (y < MIN_Y) {\n                y = MIN_Y;\n            } else if (y + config.detailHeight > maxY) {\n                y = maxY - config.detailHeight;\n            }\n            renderDetailFrame(x, y, layerIndex);\n            y = y + DETAIL_PADDING;\n            renderDetailHeading(x, y, layerIndex, headIndex);\n            y = y + DETAIL_HEADING_HEIGHT;\n            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n        }\n\n        function renderDetailHeading(x, y, layerIndex, headIndex) {\n            var fillColor = getTextColor();\n            config.svg.append(\"text\")\n                .classed(\"detail\", true)\n                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .attr(\"font-weight\", \"bold\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x + DETAIL_WIDTH / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n                .attr(\"width\", DETAIL_WIDTH)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n        }\n\n        function renderDetailText(text, id, x, y, layerIndex) {\n            var tokenContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .selectAll(\"g\")\n                .data(text)\n                .enter()\n                .append(\"g\");\n\n            var fillColor = getTextColor();\n\n            tokenContainer.append(\"rect\")\n                .classed(\"highlight\", true)\n                .attr(\"fill\", fillColor)\n                .style(\"opacity\", 0.0)\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return y + i * DETAIL_BOX_HEIGHT;\n                });\n\n            var textContainer = tokenContainer.append(\"text\")\n                .classed(\"token\", true)\n                .text(function (d) {\n                    return d;\n                })\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return i * DETAIL_BOX_HEIGHT + y;\n                })\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"dy\", TEXT_SIZE);\n\n            if (id == \"leftText\") {\n                textContainer.style(\"text-anchor\", \"end\")\n                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n                tokenContainer.on(\"mouseover\", function (d, index) {\n                    highlightSelection(index);\n                });\n                tokenContainer.on(\"mouseleave\", function () {\n                    unhighlightSelection();\n                });\n            }\n        }\n\n        function highlightSelection(index) {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n        }\n\n        function unhighlightSelection() {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", 0.0);\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", 1);\n        }\n\n        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n\n            var attnContainer = config.svg.append(\"svg:g\");\n\n            var attnBackground = attnContainer.append(\"rect\")\n                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n                .classed(\"attn_background\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .attr(\"stroke-width\", 2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", 0)\n                .attr(\"fill\", getBackgroundColor());\n            var x1 = x + THUMBNAIL_PADDING;\n            var x2 = x1 + config.thumbnailWidth - 14;\n            var y1 = y + THUMBNAIL_PADDING;\n\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter() // When entering\n                .append(\"line\")\n                .attr(\"x1\", x1)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"x2\", x2)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n\n            var clickRegion = attnContainer.append(\"rect\")\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .style(\"opacity\", 0);\n\n            clickRegion.on(\"click\", function (d, index) {\n                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n\n                config.svg.selectAll(\".detail\").remove();\n                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n                    renderDetail(att, layerIndex, headIndex);\n                    config.detail_layer = layerIndex;\n                    config.detail_head = headIndex;\n                    attnBackground.attr(\"fill\", getHighlightColor());\n                    attnBackground.attr(\"stroke-opacity\", .8);\n                } else {\n                    config.detail_layer = null;\n                    config.detail_head = null;\n                    attnBackground.attr(\"fill\", getBackgroundColor());\n                    attnBackground.attr(\"stroke-opacity\", 0);\n                }\n            });\n\n            clickRegion.on(\"mouseover\", function (d) {\n                d3.select(this).style(\"cursor\", \"pointer\");\n            });\n        }\n\n        function renderDetailFrame(x, y, layerIndex) {\n            var detailFrame = config.svg.append(\"rect\")\n                .classed(\"detail\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.detailHeight)\n                .attr(\"width\", DETAIL_WIDTH)\n                .style(\"opacity\", 1)\n                .attr(\"stroke-width\", 1.5)\n                .attr(\"stroke-opacity\", 0.7)\n                .attr(\"stroke\", getLayerColor(layerIndex));\n        }\n\n        function renderDetailAttn(x, y, att, layerIndex) {\n            var attnContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .attr(\"pointer-events\", \"none\");\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .classed('attn-line-group', true)\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter()\n                .append(\"line\")\n                .attr(\"x1\", x + ATTN_PADDING)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n        }\n\n        function getLayerColor(layer) {\n          return LAYER_COLORS[config.layers[layer] % 10];\n        }\n\n        function getTextColor() {\n            return PALETTE[config.mode]['text']\n        }\n\n        function getBackgroundColor() {\n           return PALETTE[config.mode]['background']\n        }\n\n        function getHighlightColor() {\n           return PALETTE[config.mode]['highlight']\n        }\n\n        function initialize() {\n            config.attention = params['attention'];\n            config.filter = params['default_filter'];\n            config.mode = params['display_mode'];\n            config.layers = params['include_layers']\n            config.heads = params['include_heads']\n            config.totalHeads = params['total_heads']\n            config.rootDivId = params['root_div_id'];\n            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n                config.filter = e.currentTarget.value;\n                render();\n            });\n        }\n\n        initialize();\n        render();\n\n    });",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = \"microsoft/xtremedistil-l12-h384-uncased\"  # Find popular HuggingFace models here: https://huggingface.co/models\n",
    "input_text = \"I love transformers\"  \n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.W_q(query)\n",
    "        k = self.W_k(key)\n",
    "        v = self.W_v(value)\n",
    "\n",
    "        # Split heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        concat_attention = scaled_attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, num_heads = 300, 4\n",
    "mha = MultiHeadAttention(d_model, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention output shape: torch.Size([1, 3, 300])\n",
      "Multi-head attention weights shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Use our previous positional encoded output\n",
    "mha_output, mha_attention_weights = mha(positional_encoded, positional_encoded, positional_encoded)\n",
    "print(f\"Multi-head attention output shape: {mha_output.shape}\")\n",
    "print(f\"Multi-head attention weights shape: {mha_attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Position-wise Feed-Forward Networks\n",
    "\n",
    "After the attention mechanism, each sub-layer in the transformer contains a fully connected feed-forward network. This network is applied to each position separately and identically.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The position-wise feed-forward network consists of two linear transformations with a ReLU activation in between:\n",
    "\n",
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where $W_1$, $W_2$, $b_1$, and $b_2$ are learnable parameters.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"position-wise_feed-forward_network.png\" alt=\"Position-wise Feed-Forward Network\" style=\"width:40%; height: 40%;\">\n",
    "  <br>\n",
    "  <em>Figure 7: Illustration of a Position-wise Feed-Forward Network</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Example:\n",
    "Let's apply a feed-forward network to a single word embedding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# word embedding\n",
    "x = torch.tensor([0.5, -0.2, 0.1, 0.8])\n",
    "\n",
    "# First linear transformation\n",
    "W1 = torch.tensor([[0.1, 0.2],\n",
    "                   [-0.1, 0.1],\n",
    "                   [0.3, -0.2],\n",
    "                   [0.2, 0.1]])\n",
    "b1 = torch.tensor([0.01, 0.02])\n",
    "\n",
    "# Second linear transformation\n",
    "W2 = torch.tensor([[1.0, -0.5, 0.8, 0.2],\n",
    "                   [0.5, 0.3, -0.2, 0.4]])\n",
    "b2 = torch.tensor([0.03, -0.01, 0.02, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FFN\n",
    "hidden = F.relu(torch.matmul(x, W1) + b1)\n",
    "output = torch.matmul(hidden, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN input: tensor([ 0.5000, -0.2000,  0.1000,  0.8000])\n",
      "FFN output: tensor([ 0.3800, -0.0970,  0.2040,  0.1280])\n"
     ]
    }
   ],
   "source": [
    "print(\"FFN input:\", x)\n",
    "print(\"FFN output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation uses nn.Linear layers instead of explicit matrix multiplications.\n",
    "While it may not look exactly like the formula FFN(x) = max(0, xW_1 + b_1)W_2 + b_2,\n",
    "it is functionally equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 (Linear Layer) --> y = xW1 + b1\n",
    "# Relu --> max(0, x)\n",
    "# Layer 2 (Linear Layer) --> y = xW2 + b2\n",
    "\n",
    "# step 1: max(0, xW1 + b1)\n",
    "# step 2: max(0, xW1 + b)W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First linear transformation\n",
    "        x = self.fc1(x)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Second linear transformation\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-forward network output shape: torch.Size([1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "d_model, d_ff = 300, 64\n",
    "ff_network = PositionWiseFeedForward(d_model, d_ff)\n",
    "ff_output = ff_network(mha_output)\n",
    "print(f\"Feed-forward network output shape: {ff_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer Normalization\n",
    "\n",
    "Layer normalization is a crucial component in transformers, helping to stabilize the learning process and reduce training time.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "For a vector $x = (x_1, x_2, ..., x_H)$, layer normalization is defined as:\n",
    "\n",
    "$$LN(x) = \\alpha \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ is the mean of the elements in $x$\n",
    "- $\\sigma$ is the standard deviation of the elements in $x$\n",
    "- $\\alpha$ and $\\beta$ are learnable parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./layer_normalization.png\" alt=\"Layer Normalization\" style=\"width:auto; height: auto;\">\n",
    "  <br>\n",
    "  <em>Figure 8: Illustration of Layer Normalization</em>\n",
    "</p>\n",
    "\n",
    "### Example:\n",
    "Let's apply layer normalization to a simple feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0, -1.0, 3.0, 0.0])\n",
    "\n",
    "# Learnable parameters\n",
    "alpha = torch.tensor([1.0, 1.0, 1.0, 1.0])\n",
    "beta = torch.tensor([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "\n",
    "# Apply layer normalization\n",
    "epsilon = 1e-5\n",
    "normalized = alpha * (x - mean) / (std + epsilon) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector: tensor([ 2., -1.,  3.,  0.])\n",
      "Normalized vector: tensor([ 0.5477, -1.0954,  1.0954, -0.5477])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original vector:\", x)\n",
    "print(\"Normalized vector:\", normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute mean and standard deviation\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized output shape: torch.Size([1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(d_model)\n",
    "normalized_output = layer_norm(ff_output)\n",
    "print(f\"Normalized output shape: {normalized_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encoder Layer\n",
    "\n",
    "Now that we have all the components, let's put them together to create an encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout --> regularization, which prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.layernorm1 = LayerNorm(d_model)\n",
    "        self.layernorm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Add & Norm\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Add & Norm\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder layer output shape: torch.Size([1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "encoder_output = encoder_layer(positional_encoded)\n",
    "print(f\"Encoder layer output shape: {encoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERT(nn.Module):\n",
    "#     # TODO: add 12-24 encoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./bert.png\" alt=\"Bert Architecture\" style=\"width:auto; height: auto;\">\n",
    "  <br>\n",
    "  <em>Figure 9: BERT is and \"Encoder-only\" Transformer Architecture</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Decoder Layer\n",
    "\n",
    "The decoder layer is similar to the encoder layer but includes an additional multi-head attention layer that attends to the output of the encoder.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./gpt.png\" alt=\"GPT Architecture\" style=\"width:50%; height: 50%;\">\n",
    "  <br>\n",
    "  <em>Figure 10: GPT is and \"Decoder-only\" Transformer Architecture</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.layernorm1 = LayerNorm(d_model)\n",
    "        self.layernorm2 = LayerNorm(d_model)\n",
    "        self.layernorm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self attention\n",
    "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # Multi-head attention using encoder output as Key and Value\n",
    "        attn2, _ = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder layer output shape: torch.Size([1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
    "decoder_output = decoder_layer(positional_encoded, encoder_output)\n",
    "print(f\"Decoder layer output shape: {decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Full Encoder\n",
    "\n",
    "The full encoder consists of multiple encoder layers stacked on top of each other. It also includes the initial embedding layer and positional encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "                                             for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([64, 30, 300])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "num_layers = 12\n",
    "max_seq_length = 512\n",
    "\n",
    "encoder = Encoder(vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length)\n",
    "sample_input = torch.randint(0, vocab_size, (64, 30))  # Batch of 64, sequence length of 30\n",
    "encoder_output = encoder(sample_input)\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equavalent to a BERT model 🙂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Full Decoder\n",
    "\n",
    "The full decoder, like the encoder, consists of multiple decoder layers. It also includes embedding, positional encoding, and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "                                             for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through each decoder layer\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: torch.Size([64, 20, 300])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length)\n",
    "sample_target = torch.randint(0, vocab_size, (64, 20))  # Batch of 64, sequence length of 20\n",
    "decoder_output = decoder(sample_target, encoder_output)\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Transformer\n",
    "\n",
    "Now, let's put everything together to create the full Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout)\n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, tgt_mask, src_padding_mask)\n",
    "        return self.final_layer(dec_output)\n",
    "\n",
    "    def encode(self, src, src_mask=None):\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        return self.decoder(tgt, memory, tgt_mask, memory_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output shape: torch.Size([64, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length)\n",
    "\n",
    "src = torch.randint(0, src_vocab_size, (64, 30))  # Batch of 64, source sequence length of 30\n",
    "tgt = torch.randint(0, tgt_vocab_size, (64, 20))  # Batch of 64, target sequence length of 20\n",
    "\n",
    "output = transformer(src, tgt)\n",
    "print(f\"Transformer output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Understanding and Implementing Masks in Transformers\n",
    "\n",
    "Masks play a crucial role in the Transformer architecture. They serve two main purposes:\n",
    "\n",
    "1. Padding Mask: To handle variable-length sequences in a batch.\n",
    "2. Look-ahead Mask: To prevent the decoder from looking at future tokens during training (this helps with generation).\n",
    "\n",
    "Let's dive into each type of mask and then implement them.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./masks.png\" alt=\"Look Ahead Mask and Padding Mask\" style=\"width:80%; height: 80%; background-color:white;\">\n",
    "  <br>\n",
    "  <em>Figure 11: Masks in Transformers</em>\n",
    "</p>\n",
    "\n",
    "### 13.1 Padding Mask\n",
    "\n",
    "In natural language processing tasks, we often work with sequences of different lengths. To process these in batches, we pad shorter sequences to match the length of the longest sequence in the batch. However, we don't want our model to pay attention to these padding tokens.\n",
    "\n",
    "The padding mask is a binary mask where:\n",
    "- 1 indicates a real token\n",
    "- 0 indicates a padding token\n",
    "\n",
    "### 13.2 Look-ahead Mask\n",
    "\n",
    "In the decoder, we need to prevent it from looking at future tokens during training. This is because during inference, the model won't have access to future tokens. The look-ahead mask ensures that prediction for position i can depend only on the known outputs at positions less than i.\n",
    "\n",
    "The look-ahead mask is a triangular matrix where:\n",
    "- 1 indicates positions that can be attended to\n",
    "- 0 indicates positions that should be masked out\n",
    "\n",
    "### 13.3 Implementing the Mask Functions\n",
    "\n",
    "Now, let's implement the functions to create these masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Create a padding mask for the input sequence.\n",
    "\n",
    "    Args:\n",
    "    - seq: Input tensor of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "    - mask: Padding mask of shape (batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    # Create a mask for padding tokens (assuming 0 is the padding token)\n",
    "    mask = (seq == 0).float()\n",
    "\n",
    "    # Add extra dimensions to broadcast later\n",
    "    return mask.unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Create a look-ahead mask for the decoder.\n",
    "\n",
    "    Args:\n",
    "    - size: Size of the square matrix\n",
    "\n",
    "    Returns:\n",
    "    - mask: Look-ahead mask of shape (size, size)\n",
    "    \"\"\"\n",
    "    # Create a triangular matrix\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).float()\n",
    "\n",
    "    # Convert to binary mask\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, tgt):\n",
    "    \"\"\"\n",
    "    Create all necessary masks for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "    - src: Source sequence tensor of shape (batch_size, src_seq_len)\n",
    "    - tgt: Target sequence tensor of shape (batch_size, tgt_seq_len)\n",
    "\n",
    "    Returns:\n",
    "    - src_mask: Source padding mask\n",
    "    - tgt_mask: Combined target padding and look-ahead mask\n",
    "    \"\"\"\n",
    "    # Source padding mask\n",
    "    src_mask = create_padding_mask(src)\n",
    "\n",
    "    # Target padding mask\n",
    "    tgt_padding_mask = create_padding_mask(tgt)\n",
    "\n",
    "    # Target look-ahead mask\n",
    "    tgt_look_ahead_mask = create_look_ahead_mask(tgt.size(1))\n",
    "\n",
    "    # Combine padding and look-ahead masks for the target\n",
    "    tgt_mask = torch.max(tgt_padding_mask, tgt_look_ahead_mask.unsqueeze(0))\n",
    "\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source mask shape: torch.Size([2, 1, 1, 5])\n",
      "Target mask shape: torch.Size([2, 1, 5, 5])\n",
      "\n",
      "Source mask for first sequence:\n",
      "tensor([0., 0., 0., 1., 1.])\n",
      "\n",
      "Target mask for first sequence:\n",
      "tensor([[1., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]])  # Batch of 2, max length 5\n",
    "tgt = torch.tensor([[1, 2, 3, 4, 0], [5, 6, 0, 0, 0]])  # Batch of 2, max length 5\n",
    "\n",
    "src_mask, tgt_mask = create_masks(src, tgt)\n",
    "\n",
    "print(\"Source mask shape:\", src_mask.shape)\n",
    "print(\"Target mask shape:\", tgt_mask.shape)\n",
    "\n",
    "print(\"\\nSource mask for first sequence:\")\n",
    "print(src_mask[0].squeeze())\n",
    "\n",
    "print(\"\\nTarget mask for first sequence:\")\n",
    "print(tgt_mask[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Training the Transformer\n",
    "\n",
    "To train the Transformer, we need to define a loss function and an optimizer. For sequence-to-sequence tasks, we typically use cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is the padding index\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(src, tgt):\n",
    "    transformer.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Create masks (implement these functions based on your specific requirements)\n",
    "    src_mask, tgt_mask = create_masks(src, tgt)\n",
    "\n",
    "    # Forward pass\n",
    "    output = transformer(src, tgt[:, :-1], src_mask, tgt_mask)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a data loader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes our implementation of the Transformer model. We've covered all the major components, from the basic building blocks like multi-head attention and positional encoding, to the full encoder and decoder structures, and finally the complete Transformer architecture.\n",
    "\n",
    "Remember that this is a basic implementation and there are many optimizations and variations that can be applied in practice. Some areas for further exploration include:\n",
    "\n",
    "1. Implementing more sophisticated decoding strategies (e.g., beam search)\n",
    "2. Adding regularization techniques (e.g., label smoothing)\n",
    "3. Experimenting with different attention mechanisms\n",
    "4. Implementing transformer variants like BERT, GPT, or T5\n",
    "\n",
    "Happy transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Using GPT from Hugging Face\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) is a family of language models that use the decoder part of the transformer architecture. Let's use a GPT-2 model from Hugging Face to generate text.\n",
    "\n",
    "### 14.1 Setting Up GPT-2\n",
    "\n",
    "First, we need to install the transformers library and import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a pre-trained GPT-2 model and its associated tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Generating Text with GPT-2\n",
    "\n",
    "To generate text, we'll first tokenize an input prompt, then use the model to generate a sequence of tokens, and finally decode these tokens back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, \n",
    "                            max_length=max_length, \n",
    "                            num_return_sequences=1, \n",
    "                            no_repeat_ngram_size=2,\n",
    "                            top_k=50,\n",
    "                            top_p=0.95,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7)\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where AI has become commonplace, it seems like we're not really talking about AI at all. That's not to say that AI is impossible, or even remotely possible, but it's still a pretty big problem. AI isn't just a technical concept, of course. It's also a very real possibility. The problem is that human beings don't have the capacity to understand it. They don\"t know much about it, so they don`t really know what to do about\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a world where AI has become commonplace,\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you approach building a sentiment analysis model, and what NLP techniques or tools would you use to achieve accurate sentiment classification?\n",
    "\n",
    "1. Utilise a pre-trained deep learning model like BERT for sentiment analysis without any additional pre-processing.\n",
    "2. Tokenise the text using spaCy, remove common words, and use a basic machine learning model for sentiment analysis.\n",
    "3. Use regular expressions to match positive and negative keywords in the text and classify based on the keyword count.\n",
    "4. Manually read and label each message to assign sentiment labels, then train a model on this labelled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3 --> done in Monday's lecture (not very good at all)\n",
    "* 4 --> heck no\n",
    "* 1 --> got a great model, but text fed into it still needs to be processed\n",
    "    * for example: input is \"I love transformers\", then you still need to convert it to a vector for sentiment analysis\n",
    "* 2 --> most correct. Even more correct: Utitlize pre-trained deep learning model to both tokenize and classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Using BERT from Hugging Face\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that uses the encoder part of the transformer architecture. Let's use a BERT model from Hugging Face for a text classification task.\n",
    "\n",
    "### 15.1 Setting Up BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a pre-trained BERT model and its associated tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using `BertForSequenceClassification`, which is BERT with an additional classification layer on top. We set `num_labels=2` for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 Text Classification with BERT\n",
    "\n",
    "Let's create a function to classify text sentiment using our BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Get predicted class (0 for negative, 1 for positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    return \"Positive\" if predicted_class == 1 else \"Negative\", probs[0][predicted_class].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive, Confidence: 0.55\n"
     ]
    }
   ],
   "source": [
    "text = \"I love how this transformer model works! It's amazing!\"\n",
    "sentiment, confidence = classify_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
